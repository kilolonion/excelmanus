"""AgentEngine ÂçïÂÖÉÊµãËØïÔºöË¶ÜÁõñ Tool Calling Âæ™ÁéØÊ†∏ÂøÉÈÄªËæë„ÄÇ"""

from __future__ import annotations

import asyncio
import json
from pathlib import Path
import time
from types import SimpleNamespace
from unittest.mock import AsyncMock, MagicMock, patch

import httpx
import pytest

from excelmanus.config import ExcelManusConfig, ModelProfile
from excelmanus.engine import AgentEngine, ChatResult, DelegateSubagentOutcome, ToolCallResult
from excelmanus.events import EventType
from excelmanus.hooks import HookAgentAction, HookDecision, HookEvent, HookResult
from excelmanus.mcp.manager import add_tool_prefix
from excelmanus.memory import TokenCounter
from excelmanus.plan_mode import PendingPlanState, PlanDraft
from excelmanus.skillpacks import SkillMatchResult, Skillpack
from excelmanus.subagent import SubagentConfig, SubagentResult
from excelmanus.task_list import TaskStatus
from excelmanus.tools import ToolRegistry, task_tools
from excelmanus.tools.registry import ToolDef
from excelmanus.window_perception import AdvisorContext, PerceptionBudget, WindowState, WindowType


# ‚îÄ‚îÄ ËæÖÂä©Â∑•ÂéÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def _make_config(**overrides) -> ExcelManusConfig:
    """ÂàõÂª∫ÊµãËØïÁî®ÈÖçÁΩÆ„ÄÇ"""
    defaults = {
        "api_key": "test-key",
        "base_url": "https://test.example.com/v1",
        "model": "test-model",
        "max_iterations": 20,
        "max_consecutive_failures": 3,
        "workspace_root": ".",
    }
    defaults.update(overrides)
    return ExcelManusConfig(**defaults)


def _make_registry_with_tools() -> ToolRegistry:
    """ÂàõÂª∫ÂåÖÂê´ÁÆÄÂçïÊµãËØïÂ∑•ÂÖ∑ÁöÑ ToolRegistry„ÄÇ"""
    registry = ToolRegistry()

    def add_numbers(a: int, b: int) -> int:
        return a + b

    def fail_tool() -> str:
        raise RuntimeError("Â∑•ÂÖ∑ÊâßË°åÂ§±Ë¥•")

    tools = [
        ToolDef(
            name="add_numbers",
            description="‰∏§Êï∞Áõ∏Âä†",
            input_schema={
                "type": "object",
                "properties": {
                    "a": {"type": "integer"},
                    "b": {"type": "integer"},
                },
                "required": ["a", "b"],
            },
            func=add_numbers,
        ),
        ToolDef(
            name="fail_tool",
            description="ÊÄªÊòØÂ§±Ë¥•ÁöÑÂ∑•ÂÖ∑",
            input_schema={"type": "object", "properties": {}},
            func=fail_tool,
        ),
    ]
    registry.register_tools(tools)
    return registry


def _make_text_response(content: str) -> MagicMock:
    """ÊûÑÈÄ†‰∏Ä‰∏™Á∫ØÊñáÊú¨ LLM ÂìçÂ∫îÔºàÊó† tool_callsÔºâ„ÄÇ"""
    message = SimpleNamespace(content=content, tool_calls=None)
    choice = SimpleNamespace(message=message)
    response = SimpleNamespace(choices=[choice])
    return response


def _make_tool_call_response(
    tool_calls: list[tuple[str, str, str]],
    content: str | None = None,
) -> MagicMock:
    """ÊûÑÈÄ†‰∏Ä‰∏™ÂåÖÂê´ tool_calls ÁöÑ LLM ÂìçÂ∫î„ÄÇ

    Args:
        tool_calls: [(tool_call_id, tool_name, arguments_json), ...]
        content: ÂèØÈÄâÁöÑÊñáÊú¨ÂÜÖÂÆπ
    """
    tc_objects = []
    for call_id, name, args in tool_calls:
        tc = SimpleNamespace(
            id=call_id,
            function=SimpleNamespace(name=name, arguments=args),
        )
        tc_objects.append(tc)

    message = SimpleNamespace(content=content, tool_calls=tc_objects)
    choice = SimpleNamespace(message=message)
    response = SimpleNamespace(choices=[choice])
    return response


# ‚îÄ‚îÄ ÊµãËØïÁî®‰æã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


class TestAgentEngineInit:
    """AgentEngine ÂàùÂßãÂåñÊµãËØï„ÄÇ"""

    def test_creates_async_client(self) -> None:
        """È™åËØÅÂàùÂßãÂåñÊó∂ÂàõÂª∫ AsyncOpenAI ÂÆ¢Êà∑Á´Ø„ÄÇ"""
        config = _make_config()
        registry = ToolRegistry()
        engine = AgentEngine(config, registry)
        assert engine._client is not None
        assert engine._config is config
        assert engine._registry is registry


class TestControlCommandFullAccess:
    """‰ºöËØùÁ∫ß /fullAccess ÊéßÂà∂ÂëΩ‰ª§ÊµãËØï„ÄÇ"""

    @pytest.mark.asyncio
    async def test_status_defaults_to_restricted(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        result = await engine.chat("/fullAccess status")
        assert isinstance(result, ChatResult)
        assert "restricted" in result
        assert engine.full_access_enabled is False
        assert engine.last_route_result.route_mode == "control_command"

    @pytest.mark.asyncio
    async def test_on_then_off(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        on_result = await engine.chat("/fullAccess")
        assert "full_access" in on_result
        assert engine.full_access_enabled is True
        assert engine.last_route_result.route_mode == "control_command"

        off_result = await engine.chat("/fullAccess off")
        assert "restricted" in off_result
        assert engine.full_access_enabled is False
        assert engine.last_route_result.route_mode == "control_command"

    @pytest.mark.asyncio
    async def test_command_does_not_invoke_llm_or_write_memory(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        mocked_create = AsyncMock(return_value=_make_text_response("‰∏çÂ∫îË¢´Ë∞ÉÁî®"))
        engine._client.chat.completions.create = mocked_create
        before_count = len(engine.memory.get_messages())

        result = await engine.chat("/full_access status")
        assert "restricted" in result
        mocked_create.assert_not_called()
        after_count = len(engine.memory.get_messages())
        assert before_count == after_count == 1

    @pytest.mark.asyncio
    async def test_route_blocked_skillpacks_switch_with_full_access(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        route_result = SkillMatchResult(
            skills_used=[],
            tool_scope=[],
            route_mode="llm_confirm",
            system_contexts=[],
        )
        mock_router = MagicMock()
        mock_router.route = AsyncMock(return_value=route_result)
        engine._skill_router = mock_router

        engine._client.chat.completions.create = AsyncMock(
            return_value=_make_text_response("ok")
        )
        await engine.chat("ÊôÆÈÄöËØ∑Ê±Ç")
        _, kwargs_default = mock_router.route.call_args
        assert kwargs_default["blocked_skillpacks"] == {"excel_code_runner"}

        await engine.chat("/fullAccess on")
        mock_router.route.reset_mock()
        engine._client.chat.completions.create = AsyncMock(
            return_value=_make_text_response("ok2")
        )
        await engine.chat("ÊôÆÈÄöËØ∑Ê±Ç2")
        _, kwargs_unlocked = mock_router.route.call_args
        assert kwargs_unlocked["blocked_skillpacks"] is None


class TestControlCommandSubagent:
    """‰ºöËØùÁ∫ß /subagent ÊéßÂà∂ÂëΩ‰ª§ÊµãËØï„ÄÇ"""

    @pytest.mark.asyncio
    async def test_status_defaults_to_enabled(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        result = await engine.chat("/subagent status")
        assert "enabled" in result
        assert engine.subagent_enabled is True
        assert engine.last_route_result.route_mode == "control_command"

    @pytest.mark.asyncio
    async def test_off_then_on(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        off_result = await engine.chat("/subagent off")
        assert "Â∑≤ÂÖ≥Èó≠" in off_result
        assert engine.subagent_enabled is False
        assert engine.last_route_result.route_mode == "control_command"

        on_result = await engine.chat("/subagent on")
        assert "Â∑≤ÂºÄÂêØ" in on_result
        assert engine.subagent_enabled is True
        assert engine.last_route_result.route_mode == "control_command"

    @pytest.mark.asyncio
    async def test_no_args_defaults_to_status(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        result = await engine.chat("/subagent")
        assert "ÂΩìÂâç subagent Áä∂ÊÄÅ" in result
        assert engine.subagent_enabled is True

    @pytest.mark.asyncio
    async def test_alias_sub_agent_supported(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        result = await engine.chat("/sub_agent off")
        assert "Â∑≤ÂÖ≥Èó≠" in result
        assert engine.subagent_enabled is False

    @pytest.mark.asyncio
    async def test_command_does_not_invoke_llm_or_write_memory(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        mocked_create = AsyncMock(return_value=_make_text_response("‰∏çÂ∫îË¢´Ë∞ÉÁî®"))
        engine._client.chat.completions.create = mocked_create
        before_count = len(engine.memory.get_messages())

        result = await engine.chat("/subagent off")
        assert "Â∑≤ÂÖ≥Èó≠" in result
        mocked_create.assert_not_called()
        after_count = len(engine.memory.get_messages())
        assert before_count == after_count == 1

    @pytest.mark.asyncio
    async def test_list_command_returns_catalog(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        result = await engine.chat("/subagent list")
        assert "explorer" in result
        assert "analyst" in result

    @pytest.mark.asyncio
    async def test_run_command_with_agent_routes_to_delegate(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine._delegate_to_subagent = AsyncMock(
            return_value=DelegateSubagentOutcome(
                reply="ÊâßË°åÂÆåÊàê",
                success=True,
                picked_agent="explorer",
                task_text="ÂàÜÊûêËøô‰∏™Êñá‰ª∂",
                normalized_paths=[],
                subagent_result=None,
            )
        )

        result = await engine.chat("/subagent run explorer -- ÂàÜÊûêËøô‰∏™Êñá‰ª∂")
        assert result == "ÊâßË°åÂÆåÊàê"
        engine._delegate_to_subagent.assert_awaited_once_with(
            task="ÂàÜÊûêËøô‰∏™Êñá‰ª∂",
            agent_name="explorer",
            on_event=None,
        )

    @pytest.mark.asyncio
    async def test_run_command_without_agent_routes_to_delegate(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine._delegate_to_subagent = AsyncMock(
            return_value=DelegateSubagentOutcome(
                reply="ÊâßË°åÂÆåÊàê",
                success=True,
                picked_agent="explorer",
                task_text="ÂàÜÊûêËøô‰∏™Êñá‰ª∂",
                normalized_paths=[],
                subagent_result=None,
            )
        )

        result = await engine.chat("/subagent run -- ÂàÜÊûêËøô‰∏™Êñá‰ª∂")
        assert result == "ÊâßË°åÂÆåÊàê"
        engine._delegate_to_subagent.assert_awaited_once_with(
            task="ÂàÜÊûêËøô‰∏™Êñá‰ª∂",
            agent_name=None,
            on_event=None,
        )


class TestModelSwitchConsistency:
    """Ê®°ÂûãÂàáÊç¢‰∏éË∑ØÁî±Ê®°Âûã‰∏ÄËá¥ÊÄßÊµãËØï„ÄÇ"""

    def test_switch_model_syncs_router_when_router_model_not_configured(self) -> None:
        config = _make_config(
            model="main-a",
            models=(
                ModelProfile(
                    name="alt",
                    model="main-b",
                    api_key="alt-key",
                    base_url="https://alt.example.com/v1",
                    description="Â§áÈÄâÊ®°Âûã",
                ),
            ),
        )
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        assert engine._router_follow_active_model is True
        assert engine._router_model == "main-a"
        assert engine._router_client is engine._client

        msg = engine.switch_model("alt")
        assert "Â∑≤ÂàáÊç¢Âà∞Ê®°Âûã" in msg
        assert engine._active_model == "main-b"
        assert engine._router_model == "main-b"
        assert engine._router_client is engine._client

    def test_switch_model_keeps_router_when_router_model_configured(self) -> None:
        config = _make_config(
            model="main-a",
            router_model="router-fixed",
            router_api_key="router-key",
            router_base_url="https://router.example.com/v1",
            models=(
                ModelProfile(
                    name="alt",
                    model="main-b",
                    api_key="alt-key",
                    base_url="https://alt.example.com/v1",
                    description="Â§áÈÄâÊ®°Âûã",
                ),
            ),
        )
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        old_router_client = engine._router_client

        assert engine._router_follow_active_model is False
        assert engine._router_model == "router-fixed"

        engine.switch_model("alt")
        assert engine._active_model == "main-b"
        assert engine._router_model == "router-fixed"
        assert engine._router_client is old_router_client
        assert engine._router_client is not engine._client

    @pytest.mark.asyncio
    async def test_window_perception_advisor_follows_router_model_after_switch(self) -> None:
        config = _make_config(
            model="main-a",
            models=(
                ModelProfile(
                    name="alt",
                    model="main-b",
                    api_key="alt-key",
                    base_url="https://alt.example.com/v1",
                    description="Â§áÈÄâÊ®°Âûã",
                ),
            ),
            window_perception_advisor_mode="hybrid",
        )
        engine = AgentEngine(config, _make_registry_with_tools())
        engine.switch_model("alt")
        engine._router_client.chat.completions.create = AsyncMock(
            return_value=_make_text_response('{"task_type":"GENERAL_BROWSE","advices":[]}')
        )

        _ = await engine._run_window_perception_advisor_async(
            windows=[WindowState(id="w1", type=WindowType.SHEET, title="A")],
            active_window_id="w1",
            budget=PerceptionBudget(),
            context=AdvisorContext(turn_number=1, task_type="GENERAL_BROWSE"),
        )

        _, kwargs = engine._router_client.chat.completions.create.call_args
        assert kwargs["model"] == "main-b"

    @pytest.mark.asyncio
    async def test_window_perception_advisor_retries_once_on_transient_error(
        self,
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        class _TransientError(Exception):
            def __init__(self) -> None:
                super().__init__("rate limit")
                self.status_code = 429
                self.response = SimpleNamespace(headers={"Retry-After": "0.1"})

        config = _make_config(
            window_perception_advisor_mode="hybrid",
            window_perception_advisor_timeout_ms=20_000,
        )
        engine = AgentEngine(config, _make_registry_with_tools())
        mocked_create = AsyncMock(
            side_effect=[
                _TransientError(),
                _make_text_response('{"task_type":"GENERAL_BROWSE","advices":[]}'),
            ]
        )
        engine._router_client.chat.completions.create = mocked_create
        mocked_sleep = AsyncMock(return_value=None)
        monkeypatch.setattr("excelmanus.engine.asyncio.sleep", mocked_sleep)

        plan = await engine._run_window_perception_advisor_async(
            windows=[WindowState(id="w1", type=WindowType.SHEET, title="A")],
            active_window_id="w1",
            budget=PerceptionBudget(),
            context=AdvisorContext(turn_number=1, task_type="GENERAL_BROWSE"),
        )

        assert plan is not None
        assert mocked_create.await_count == 2
        mocked_sleep.assert_awaited_once()

    def test_is_transient_window_advisor_exception_detects_nested_connect_error(self) -> None:
        wrapped = RuntimeError("Gemini API ËØ∑Ê±ÇÂ§±Ë¥•: ")
        wrapped.__cause__ = httpx.ConnectError("")
        assert AgentEngine._is_transient_window_advisor_exception(wrapped) is True

    def test_extract_retry_after_seconds_from_nested_exception(self) -> None:
        class _RateLimitError(Exception):
            def __init__(self) -> None:
                super().__init__("rate limited")
                self.response = SimpleNamespace(headers={"Retry-After": "0.6"})

        wrapped = RuntimeError("Gemini API ËØ∑Ê±ÇÂ§±Ë¥•: ")
        wrapped.__cause__ = _RateLimitError()

        retry_after = AgentEngine._extract_retry_after_seconds(wrapped)
        assert retry_after == pytest.approx(0.6)

    @pytest.mark.asyncio
    async def test_window_perception_advisor_retries_on_wrapped_connect_error(
        self,
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        wrapped = RuntimeError("Gemini API ËØ∑Ê±ÇÂ§±Ë¥•: ")
        wrapped.__cause__ = httpx.ConnectError("")

        config = _make_config(
            window_perception_advisor_mode="hybrid",
            window_perception_advisor_timeout_ms=20_000,
        )
        engine = AgentEngine(config, _make_registry_with_tools())
        mocked_create = AsyncMock(
            side_effect=[
                wrapped,
                _make_text_response('{"task_type":"GENERAL_BROWSE","advices":[]}'),
            ]
        )
        engine._router_client.chat.completions.create = mocked_create
        mocked_sleep = AsyncMock(return_value=None)
        monkeypatch.setattr("excelmanus.engine.asyncio.sleep", mocked_sleep)

        plan = await engine._run_window_perception_advisor_async(
            windows=[WindowState(id="w1", type=WindowType.SHEET, title="A")],
            active_window_id="w1",
            budget=PerceptionBudget(),
            context=AdvisorContext(turn_number=1, task_type="GENERAL_BROWSE"),
        )

        assert plan is not None
        assert mocked_create.await_count == 2
        mocked_sleep.assert_awaited_once()

    @pytest.mark.asyncio
    async def test_window_perception_advisor_timeout_does_not_retry(
        self,
        monkeypatch: pytest.MonkeyPatch,
    ) -> None:
        async def _slow_response(**_kwargs):
            await asyncio.sleep(0.2)
            return _make_text_response('{"task_type":"GENERAL_BROWSE","advices":[]}')

        config = _make_config(
            window_perception_advisor_mode="hybrid",
            window_perception_advisor_timeout_ms=20,
        )
        engine = AgentEngine(config, _make_registry_with_tools())
        mocked_create = AsyncMock(side_effect=_slow_response)
        engine._router_client.chat.completions.create = mocked_create

        def _unexpected_retry_delay(_exc: Exception) -> float:
            raise AssertionError("‰∏çÂ∫îËøõÂÖ•ÈáçËØïÂàÜÊîØ")

        monkeypatch.setattr(
            AgentEngine,
            "_window_advisor_retry_delay_seconds",
            staticmethod(_unexpected_retry_delay),
        )

        plan = await engine._run_window_perception_advisor_async(
            windows=[WindowState(id="w1", type=WindowType.SHEET, title="A")],
            active_window_id="w1",
            budget=PerceptionBudget(),
            context=AdvisorContext(turn_number=1, task_type="GENERAL_BROWSE"),
        )

        assert plan is None
        assert mocked_create.await_count == 1


class TestSystemMessageMode:
    """system_message_mode Ë°å‰∏∫ÊµãËØï„ÄÇ"""

    def test_auto_mode_defaults_to_replace(self) -> None:
        config = _make_config(system_message_mode="auto")
        engine = AgentEngine(config, _make_registry_with_tools())
        assert engine._effective_system_mode() == "replace"

    def test_build_system_prompts_replace_mode_splits_system_messages(self) -> None:
        config = _make_config(system_message_mode="replace")
        engine = AgentEngine(config, _make_registry_with_tools())
        prompts = engine._build_system_prompts(["[Skillpack] data_basic\nÊèèËø∞ÔºöÊµãËØï"])
        assert len(prompts) == 2
        assert "[Skillpack] data_basic" in prompts[1]

    def test_build_system_prompts_merge_mode_merges_into_single_message(self) -> None:
        config = _make_config(system_message_mode="merge")
        engine = AgentEngine(config, _make_registry_with_tools())
        prompts = engine._build_system_prompts(["[Skillpack] data_basic\nÊèèËø∞ÔºöÊµãËØï"])
        assert len(prompts) == 1
        assert "[Skillpack] data_basic" in prompts[0]

    @pytest.mark.asyncio
    async def test_auto_mode_fallback_merges_messages_after_provider_compat_error(self) -> None:
        config = _make_config(system_message_mode="auto")
        engine = AgentEngine(config, _make_registry_with_tools())
        mocked_create = AsyncMock(
            side_effect=[
                RuntimeError("at most one system message is supported"),
                _make_text_response("ok"),
            ]
        )
        engine._client.chat.completions.create = mocked_create

        response = await engine._create_chat_completion_with_system_fallback(
            {
                "model": config.model,
                "messages": [
                    {"role": "system", "content": "S1"},
                    {"role": "system", "content": "S2"},
                    {"role": "user", "content": "hello"},
                ],
            }
        )

        assert response.choices[0].message.content == "ok"
        assert mocked_create.call_count == 2
        retry_messages = mocked_create.call_args_list[1].kwargs["messages"]
        assert retry_messages[0]["role"] == "system"
        assert "S1" in retry_messages[0]["content"]
        assert "S2" in retry_messages[0]["content"]
        assert sum(1 for msg in retry_messages if msg.get("role") == "system") == 1
        assert engine._system_mode_fallback == "merge"


class TestContextBudgetAndHardCap:
    """‰∏ä‰∏ãÊñáÈ¢ÑÁÆó‰∏éÂ∑•ÂÖ∑ÁªìÊûúÂÖ®Â±ÄÁ°¨Êà™Êñ≠ÊµãËØï„ÄÇ"""

    @pytest.mark.asyncio
    async def test_tool_loop_messages_fit_max_context_budget(self) -> None:
        config = _make_config(max_context_tokens=3000)
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine.memory.add_user_message("ÊµãËØï‰∏ä‰∏ãÊñáÈ¢ÑÁÆó")

        route_result = SkillMatchResult(
            skills_used=[],
            tool_scope=["add_numbers"],
            route_mode="fallback",
            system_contexts=["X" * 6000],
        )
        mocked_create = AsyncMock(return_value=_make_text_response("ok"))
        engine._client.chat.completions.create = mocked_create

        result = await engine._tool_calling_loop(route_result, on_event=None)
        assert result.reply == "ok"
        assert mocked_create.call_count == 1

        _, kwargs = mocked_create.call_args
        sent_messages = kwargs["messages"]
        total_tokens = sum(TokenCounter.count_message(m) for m in sent_messages)
        assert total_tokens <= int(config.max_context_tokens * 0.9)

    @pytest.mark.asyncio
    async def test_tool_loop_returns_actionable_error_when_system_prompt_itself_over_budget(self) -> None:
        config = _make_config(max_context_tokens=20)
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine.memory.add_user_message("ÊµãËØïÊûÅÂ∞è‰∏ä‰∏ãÊñá")
        mocked_create = AsyncMock(return_value=_make_text_response("‰∏çÂ∫îË∞ÉÁî®"))
        engine._client.chat.completions.create = mocked_create

        route_result = SkillMatchResult(
            skills_used=[],
            tool_scope=[],
            route_mode="fallback",
            system_contexts=[],
        )
        result = await engine._tool_calling_loop(route_result, on_event=None)

        assert "Á≥ªÁªü‰∏ä‰∏ãÊñáËøáÈïø" in result.reply
        mocked_create.assert_not_called()

    @pytest.mark.asyncio
    async def test_execute_tool_call_applies_global_hard_cap(self) -> None:
        def long_tool() -> str:
            return "A" * 500

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="long_tool",
                description="ÈïøÊñáÊú¨Â∑•ÂÖ∑",
                input_schema={"type": "object", "properties": {}},
                func=long_tool,
                max_result_chars=0,
            ),
        ])

        config = _make_config(tool_result_hard_cap_chars=80)
        engine = AgentEngine(config, registry)
        tc = SimpleNamespace(
            id="call_long",
            function=SimpleNamespace(name="long_tool", arguments="{}"),
        )

        result = await engine._execute_tool_call(
            tc,
            tool_scope=["long_tool"],
            on_event=None,
            iteration=1,
            route_result=None,
        )

        assert result.success is True
        assert "ÁªìÊûúÂ∑≤ÂÖ®Â±ÄÊà™Êñ≠" in result.result
        assert "‰∏äÈôê: 80 Â≠óÁ¨¶" in result.result

    @pytest.mark.asyncio
    async def test_window_perception_enriches_json_tool_result(self) -> None:
        def read_excel() -> str:
            return json.dumps(
                {
                    "file": "sales.xlsx",
                    "shape": {"rows": 20, "columns": 5},
                    "preview": [{"‰∫ßÂìÅ": "A", "ÈáëÈ¢ù": 100}],
                },
                ensure_ascii=False,
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={"type": "object", "properties": {}},
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        config = _make_config()
        engine = AgentEngine(config, registry)
        tc = SimpleNamespace(
            id="call_read",
            function=SimpleNamespace(name="read_excel", arguments="{}"),
        )
        result = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=1,
            route_result=None,
        )

        assert "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ÁéØÂ¢ÉÊÑüÁü• ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ" in result.result
        assert "üìä Êñá‰ª∂: sales.xlsx" in result.result
        assert "_environment_perception" not in result.result
        json_part, _sep, _tail = result.result.partition("\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ÁéØÂ¢ÉÊÑüÁü• ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        payload = json.loads(json_part)
        assert payload["file"] == "sales.xlsx"

    @pytest.mark.asyncio
    async def test_window_perception_block_contains_scroll_status_and_style_details(self) -> None:
        def read_excel() -> str:
            return json.dumps(
                {
                    "file": "sales.xlsx",
                    "sheet": "Q1",
                    "shape": {"rows": 5000, "columns": 30},
                    "preview": [
                        {"‰∫ßÂìÅ": "A", "ÈîÄÂîÆÈ¢ù": 12500, "ËææÊàêÁéá": "106.6%"},
                        {"‰∫ßÂìÅ": "B", "ÈîÄÂîÆÈ¢ù": 8300, "ËææÊàêÁéá": "90.4%"},
                    ],
                    "styles": {
                        "style_classes": {"s0": {"font": {"bold": True}}},
                        "merged_ranges": ["F1:H1"],
                    },
                    "conditional_formatting": [
                        {"range": "D2:D7", "type": "cellIs", "operator": "greaterThan"},
                    ],
                    "column_widths": {"A": 12.0, "B": 15.0},
                    "row_heights": {"1": 24.0, "2": 18.0},
                },
                ensure_ascii=False,
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={"type": "object", "properties": {}},
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        engine = AgentEngine(_make_config(), registry)
        tc = SimpleNamespace(
            id="call_read",
            function=SimpleNamespace(name="read_excel", arguments="{}"),
        )
        result = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=1,
            route_result=None,
        )

        assert "ÊªöÂä®Êù°‰ΩçÁΩÆ:" in result.result
        assert "Áä∂ÊÄÅÊ†è: SUM=" in result.result
        assert "ÂàóÂÆΩ: A=12, B=15" in result.result
        assert "Ë°åÈ´ò: 1=24, 2=18" in result.result
        assert "ÂêàÂπ∂ÂçïÂÖÉÊ†º: F1:H1" in result.result
        assert "Êù°‰ª∂Ê†ºÂºèÊïàÊûú: D2:D7: Êù°‰ª∂ÁùÄËâ≤ÔºàcellIs/greaterThanÔºâ" in result.result

    @pytest.mark.asyncio
    async def test_window_perception_can_be_disabled(self) -> None:
        def read_excel() -> str:
            return json.dumps(
                {"file": "sales.xlsx", "shape": {"rows": 20, "columns": 5}},
                ensure_ascii=False,
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={"type": "object", "properties": {}},
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        config = _make_config(window_perception_enabled=False)
        engine = AgentEngine(config, registry)
        tc = SimpleNamespace(
            id="call_read",
            function=SimpleNamespace(name="read_excel", arguments="{}"),
        )
        result = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=1,
            route_result=None,
        )

        payload = json.loads(result.result)
        assert payload["file"] == "sales.xlsx"
        assert "ÁéØÂ¢ÉÊÑüÁü•" not in result.result
        assert engine._effective_window_return_mode() == "enriched"

    @pytest.mark.asyncio
    async def test_window_perception_notice_is_injected_into_system_prompts(self) -> None:
        def read_excel(file_path: str, sheet_name: str) -> str:
            return json.dumps(
                {
                    "file": file_path,
                    "sheet": sheet_name,
                    "shape": {"rows": 200, "columns": 12},
                    "preview": [{"‰∫ßÂìÅ": "A", "ÈáëÈ¢ù": 100}],
                },
                ensure_ascii=False,
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string"},
                        "sheet_name": {"type": "string"},
                    },
                    "required": ["file_path", "sheet_name"],
                },
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        config = _make_config(system_message_mode="replace")
        engine = AgentEngine(config, registry)

        tc = SimpleNamespace(
            id="call_read",
            function=SimpleNamespace(
                name="read_excel",
                arguments=json.dumps({"file_path": "sales.xlsx", "sheet_name": "Q1"}),
            ),
        )
        await engine._execute_tool_call(
            tc=tc,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=1,
            route_result=None,
        )

        prompts, error = engine._prepare_system_prompts_for_request([])
        assert error is None
        merged_prompt = "\n\n".join(prompts)
        assert "## Á™óÂè£ÊÑüÁü•‰∏ä‰∏ãÊñá" in merged_prompt
        assert "sales.xlsx" in merged_prompt
        assert "Q1" in merged_prompt

    @pytest.mark.asyncio
    async def test_window_perception_anchored_returns_confirmation(self) -> None:
        def read_excel() -> str:
            return json.dumps(
                {
                    "file": "sales.xlsx",
                    "sheet": "Q1",
                    "shape": {"rows": 20, "columns": 5},
                    "columns": ["Êó•Êúü", "‰∫ßÂìÅ", "Êï∞Èáè", "Âçï‰ª∑", "ÈáëÈ¢ù"],
                    "preview": [{"Êó•Êúü": "2024-01-01", "‰∫ßÂìÅ": "A", "Êï∞Èáè": 1, "Âçï‰ª∑": 100, "ÈáëÈ¢ù": 100}],
                },
                ensure_ascii=False,
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={"type": "object", "properties": {}},
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        config = _make_config(window_return_mode="anchored")
        engine = AgentEngine(config, registry)
        tc = SimpleNamespace(
            id="call_read",
            function=SimpleNamespace(name="read_excel", arguments="{}"),
        )
        result = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=1,
            route_result=None,
        )

        assert result.result.startswith("‚úÖ [")
        assert "read_excel: A1:J25" in result.result
        assert "ÊÑèÂõæ: aggregate" in result.result
        assert "Êï∞ÊçÆÂ∑≤ËûçÂÖ•Á™óÂè£ÔºåËØ∑‰ºòÂÖàÂºïÁî®Á™óÂè£ÂÜÖÂÆπ„ÄÇ" in result.result
        assert "ÁéØÂ¢ÉÊÑüÁü•" not in result.result

    @pytest.mark.asyncio
    async def test_window_perception_unified_returns_compact_confirmation(self) -> None:
        def read_excel(file_path: str, sheet_name: str, range: str) -> str:
            return json.dumps(
                {
                    "file": file_path,
                    "sheet": sheet_name,
                    "shape": {"rows": 20, "columns": 5},
                    "columns": ["Êó•Êúü", "‰∫ßÂìÅ", "Êï∞Èáè", "Âçï‰ª∑", "ÈáëÈ¢ù"],
                    "preview": [{"Êó•Êúü": "2024-01-01", "‰∫ßÂìÅ": "A", "Êï∞Èáè": 1, "Âçï‰ª∑": 100, "ÈáëÈ¢ù": 100}],
                },
                ensure_ascii=False,
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string"},
                        "sheet_name": {"type": "string"},
                        "range": {"type": "string"},
                    },
                    "required": ["file_path", "sheet_name", "range"],
                },
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        config = _make_config(window_return_mode="unified")
        engine = AgentEngine(config, registry)
        tc = SimpleNamespace(
            id="call_read",
            function=SimpleNamespace(
                name="read_excel",
                arguments=json.dumps(
                    {"file_path": "sales.xlsx", "sheet_name": "Q1", "range": "A1:E10"},
                    ensure_ascii=False,
                ),
            ),
        )
        result = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=1,
            route_result=None,
        )

        assert result.result.startswith("‚úÖ [")
        assert "read_excel: A1:E10" in result.result
        assert "| ÊÑèÂõæ=aggregate" in result.result
        assert "È¶ñË°åÈ¢ÑËßà" not in result.result
        assert "ÁéØÂ¢ÉÊÑüÁü•" not in result.result

    @pytest.mark.asyncio
    async def test_window_perception_adaptive_gpt_defaults_to_unified(self) -> None:
        def read_excel(file_path: str, sheet_name: str, range: str) -> str:
            return json.dumps(
                {
                    "file": file_path,
                    "sheet": sheet_name,
                    "shape": {"rows": 20, "columns": 5},
                    "columns": ["Êó•Êúü", "‰∫ßÂìÅ", "Êï∞Èáè", "Âçï‰ª∑", "ÈáëÈ¢ù"],
                    "preview": [{"Êó•Êúü": "2024-01-01", "‰∫ßÂìÅ": "A", "Êï∞Èáè": 1, "Âçï‰ª∑": 100, "ÈáëÈ¢ù": 100}],
                },
                ensure_ascii=False,
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string"},
                        "sheet_name": {"type": "string"},
                        "range": {"type": "string"},
                    },
                    "required": ["file_path", "sheet_name", "range"],
                },
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        config = _make_config(
            model="gpt-5.3",
            window_return_mode="adaptive",
        )
        engine = AgentEngine(config, registry)
        tc = SimpleNamespace(
            id="call_read",
            function=SimpleNamespace(
                name="read_excel",
                arguments=json.dumps(
                    {"file_path": "sales.xlsx", "sheet_name": "Q1", "range": "A1:E10"},
                    ensure_ascii=False,
                ),
            ),
        )
        result = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=1,
            route_result=None,
        )

        assert result.result.startswith("‚úÖ [")
        assert "read_excel: A1:E10" in result.result
        assert "| ÊÑèÂõæ=aggregate" in result.result
        assert "È¶ñË°åÈ¢ÑËßà" not in result.result
        assert "ÁéØÂ¢ÉÊÑüÁü•" not in result.result
        assert engine._effective_window_return_mode() == "unified"

    @pytest.mark.asyncio
    async def test_window_perception_adaptive_repeat_tripwire_downgrades_to_anchored(self) -> None:
        def read_excel(file_path: str, sheet_name: str, range: str) -> str:
            return json.dumps(
                {
                    "file": file_path,
                    "sheet": sheet_name,
                    "shape": {"rows": 20, "columns": 5},
                    "columns": ["Êó•Êúü", "‰∫ßÂìÅ", "Êï∞Èáè", "Âçï‰ª∑", "ÈáëÈ¢ù"],
                    "preview": [{"Êó•Êúü": "2024-01-01", "‰∫ßÂìÅ": "A", "Êï∞Èáè": 1, "Âçï‰ª∑": 100, "ÈáëÈ¢ù": 100}],
                },
                ensure_ascii=False,
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string"},
                        "sheet_name": {"type": "string"},
                        "range": {"type": "string"},
                    },
                    "required": ["file_path", "sheet_name", "range"],
                },
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        config = _make_config(
            model="gpt-5.3",
            window_return_mode="adaptive",
        )
        engine = AgentEngine(config, registry)
        tc = SimpleNamespace(
            id="call_read",
            function=SimpleNamespace(
                name="read_excel",
                arguments=json.dumps(
                    {"file_path": "sales.xlsx", "sheet_name": "Q1", "range": "A1:E10"},
                    ensure_ascii=False,
                ),
            ),
        )

        first = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=1,
            route_result=None,
        )
        second = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=2,
            route_result=None,
        )
        third = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=3,
            route_result=None,
        )

        assert "È¶ñË°åÈ¢ÑËßà" not in first.result
        assert "ÊèêÁ§∫=ÂΩìÂâçÊÑèÂõæ[aggregate]‰∏ãÊ≠§Êï∞ÊçÆÂ∑≤Âú®Á™óÂè£" in second.result
        assert "ÊÑèÂõæ: aggregate" in third.result
        assert "ÊèêÁ§∫: ÂΩìÂâçÊÑèÂõæ[aggregate]‰∏ãÊ≠§Êï∞ÊçÆÂ∑≤Âú®Á™óÂè£" in third.result
        assert "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ÁéØÂ¢ÉÊÑüÁü• ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ" not in third.result
        assert engine._effective_window_return_mode() == "anchored"

    @pytest.mark.asyncio
    async def test_window_perception_adaptive_model_switch_keeps_downgraded_state(self) -> None:
        def read_excel(file_path: str, sheet_name: str, range: str) -> str:
            return json.dumps(
                {
                    "file": file_path,
                    "sheet": sheet_name,
                    "shape": {"rows": 20, "columns": 5},
                    "columns": ["Êó•Êúü", "‰∫ßÂìÅ", "Êï∞Èáè", "Âçï‰ª∑", "ÈáëÈ¢ù"],
                    "preview": [{"Êó•Êúü": "2024-01-01", "‰∫ßÂìÅ": "A", "Êï∞Èáè": 1, "Âçï‰ª∑": 100, "ÈáëÈ¢ù": 100}],
                },
                ensure_ascii=False,
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string"},
                        "sheet_name": {"type": "string"},
                        "range": {"type": "string"},
                    },
                    "required": ["file_path", "sheet_name", "range"],
                },
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        config = _make_config(
            model="gpt-5.3",
            window_return_mode="adaptive",
            models=(
                ModelProfile(
                    name="deepseek",
                    model="deepseek-chat",
                    api_key="test-key-2",
                    base_url="https://deepseek.example.com/v1",
                    description="ÂàáÊç¢Ê®°Âûã",
                ),
            ),
        )
        engine = AgentEngine(config, registry)
        tc = SimpleNamespace(
            id="call_read",
            function=SimpleNamespace(
                name="read_excel",
                arguments=json.dumps(
                    {"file_path": "sales.xlsx", "sheet_name": "Q1", "range": "A1:E10"},
                    ensure_ascii=False,
                ),
            ),
        )
        await engine._execute_tool_call(
            tc=tc,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=1,
            route_result=None,
        )
        await engine._execute_tool_call(
            tc=tc,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=2,
            route_result=None,
        )
        await engine._execute_tool_call(
            tc=tc,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=3,
            route_result=None,
        )
        assert engine._effective_window_return_mode() == "anchored"

        switch_message = engine.switch_model("deepseek")
        assert "Â∑≤ÂàáÊç¢Âà∞Ê®°Âûã" in switch_message
        assert engine._effective_window_return_mode() == "anchored"

    @pytest.mark.asyncio
    async def test_window_perception_adaptive_ingest_failures_trigger_downgrade(self) -> None:
        def read_excel(file_path: str, sheet_name: str, range: str) -> str:
            return json.dumps(
                {
                    "file": file_path,
                    "sheet": sheet_name,
                    "shape": {"rows": 20, "columns": 5},
                    "columns": ["Êó•Êúü", "‰∫ßÂìÅ", "Êï∞Èáè", "Âçï‰ª∑", "ÈáëÈ¢ù"],
                    "preview": [{"Êó•Êúü": "2024-01-01", "‰∫ßÂìÅ": "A", "Êï∞Èáè": 1, "Âçï‰ª∑": 100, "ÈáëÈ¢ù": 100}],
                },
                ensure_ascii=False,
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string"},
                        "sheet_name": {"type": "string"},
                        "range": {"type": "string"},
                    },
                    "required": ["file_path", "sheet_name", "range"],
                },
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        config = _make_config(model="gpt-5.3", window_return_mode="adaptive")
        engine = AgentEngine(config, registry)
        tc = SimpleNamespace(
            id="call_read",
            function=SimpleNamespace(
                name="read_excel",
                arguments=json.dumps(
                    {"file_path": "sales.xlsx", "sheet_name": "Q1", "range": "A1:E10"},
                    ensure_ascii=False,
                ),
            ),
        )

        original_apply = engine._window_perception._apply_ingest

        def _raise_ingest(*_args, **_kwargs):
            raise RuntimeError("ingest boom")

        engine._window_perception._apply_ingest = _raise_ingest  # type: ignore[assignment]
        try:
            await engine._execute_tool_call(
                tc=tc,
                tool_scope=["read_excel"],
                on_event=None,
                iteration=1,
                route_result=None,
            )
            await engine._execute_tool_call(
                tc=tc,
                tool_scope=["read_excel"],
                on_event=None,
                iteration=2,
                route_result=None,
            )
        finally:
            engine._window_perception._apply_ingest = original_apply  # type: ignore[assignment]

        assert engine._effective_window_return_mode() == "anchored"

    @pytest.mark.asyncio
    async def test_window_perception_unified_repeat_and_fallback_to_enriched(self) -> None:
        def read_excel(file_path: str, sheet_name: str, range: str) -> str:
            return json.dumps(
                {
                    "file": file_path,
                    "sheet": sheet_name,
                    "shape": {"rows": 20, "columns": 5},
                    "columns": ["Êó•Êúü", "‰∫ßÂìÅ", "Êï∞Èáè", "Âçï‰ª∑", "ÈáëÈ¢ù"],
                    "preview": [{"Êó•Êúü": "2024-01-01", "‰∫ßÂìÅ": "A", "Êï∞Èáè": 1, "Âçï‰ª∑": 100, "ÈáëÈ¢ù": 100}],
                },
                ensure_ascii=False,
            )

        def write_cells() -> str:
            return json.dumps({"status": "success"}, ensure_ascii=False)

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string"},
                        "sheet_name": {"type": "string"},
                        "range": {"type": "string"},
                    },
                    "required": ["file_path", "sheet_name", "range"],
                },
                func=read_excel,
                max_result_chars=0,
            ),
            ToolDef(
                name="write_cells",
                description="ÂÜôÂÖ•",
                input_schema={"type": "object", "properties": {}},
                func=write_cells,
                max_result_chars=0,
            ),
        ])
        config = _make_config(window_return_mode="unified")
        engine = AgentEngine(config, registry)

        read_tc = SimpleNamespace(
            id="call_read",
            function=SimpleNamespace(
                name="read_excel",
                arguments=json.dumps(
                    {"file_path": "sales.xlsx", "sheet_name": "Q1", "range": "A1:E10"},
                    ensure_ascii=False,
                ),
            ),
        )
        first = await engine._execute_tool_call(
            tc=read_tc,
            tool_scope=["read_excel", "write_cells"],
            on_event=None,
            iteration=1,
            route_result=None,
        )
        second = await engine._execute_tool_call(
            tc=read_tc,
            tool_scope=["read_excel", "write_cells"],
            on_event=None,
            iteration=2,
            route_result=None,
        )
        third = await engine._execute_tool_call(
            tc=read_tc,
            tool_scope=["read_excel", "write_cells"],
            on_event=None,
            iteration=3,
            route_result=None,
        )

        assert "‚ö†Ô∏è Ê≠§Êï∞ÊçÆÂ∑≤Âú®Á™óÂè£" not in first.result
        assert "ÊèêÁ§∫=ÂΩìÂâçÊÑèÂõæ[aggregate]‰∏ãÊ≠§Êï∞ÊçÆÂ∑≤Âú®Á™óÂè£" in second.result
        assert "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ÁéØÂ¢ÉÊÑüÁü• ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ" in third.result

        write_tc = SimpleNamespace(
            id="call_write",
            function=SimpleNamespace(name="write_cells", arguments="{}"),
        )
        _ = await engine._execute_tool_call(
            tc=write_tc,
            tool_scope=["read_excel", "write_cells"],
            on_event=None,
            iteration=4,
            route_result=None,
        )
        after_write = await engine._execute_tool_call(
            tc=read_tc,
            tool_scope=["read_excel", "write_cells"],
            on_event=None,
            iteration=5,
            route_result=None,
        )
        assert "‚ö†Ô∏è Ê≠§Êï∞ÊçÆÂ∑≤Âú®Á™óÂè£" not in after_write.result
        assert "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ÁéØÂ¢ÉÊÑüÁü• ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ" not in after_write.result

    @pytest.mark.asyncio
    async def test_enriched_mode_hides_focus_window_tool(self) -> None:
        config = _make_config(window_return_mode="enriched")
        registry = _make_registry_with_tools()
        registry.register_tool(
            ToolDef(
                name="focus_window",
                description="Á™óÂè£ËÅöÁÑ¶",
                input_schema={"type": "object", "properties": {}},
                func=lambda: "ok",
            )
        )
        engine = AgentEngine(config, registry)
        route_result = SkillMatchResult(
            skills_used=[],
            tool_scope=["focus_window", "add_numbers"],
            route_mode="fallback",
            system_contexts=[],
        )
        scope = engine._get_current_tool_scope(route_result=route_result)
        assert "focus_window" not in scope

        anchored_engine = AgentEngine(_make_config(window_return_mode="anchored"), registry)
        anchored_scope = anchored_engine._get_current_tool_scope(route_result=route_result)
        assert "focus_window" in anchored_scope

        adaptive_enriched_engine = AgentEngine(
            _make_config(window_return_mode="adaptive", model="deepseek-chat"),
            registry,
        )
        adaptive_enriched_scope = adaptive_enriched_engine._get_current_tool_scope(
            route_result=route_result
        )
        assert "focus_window" not in adaptive_enriched_scope

        adaptive_unified_engine = AgentEngine(
            _make_config(window_return_mode="adaptive", model="gpt-5.2"),
            registry,
        )
        adaptive_unified_scope = adaptive_unified_engine._get_current_tool_scope(
            route_result=route_result
        )
        assert "focus_window" in adaptive_unified_scope

    @pytest.mark.asyncio
    async def test_window_perception_anchored_notice_is_data_window_and_tail(self) -> None:
        def read_excel() -> str:
            return json.dumps(
                {
                    "file": "sales.xlsx",
                    "sheet": "Q1",
                    "shape": {"rows": 20, "columns": 5},
                    "preview": [{"‰∫ßÂìÅ": "A", "ÈáëÈ¢ù": 100}],
                },
                ensure_ascii=False,
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={"type": "object", "properties": {}},
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        config = _make_config(window_return_mode="anchored", system_message_mode="replace")
        engine = AgentEngine(config, registry)
        tc = SimpleNamespace(
            id="call_read",
            function=SimpleNamespace(name="read_excel", arguments="{}"),
        )
        await engine._execute_tool_call(
            tc=tc,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=1,
            route_result=None,
        )

        prompts, error = engine._prepare_system_prompts_for_request(["## SkillCtx\nÂÜÖÂÆπ"])
        assert error is None
        assert prompts[-1].startswith("## Êï∞ÊçÆÁ™óÂè£")

    @pytest.mark.asyncio
    async def test_window_perception_notice_respects_budget_and_window_limit(self) -> None:
        def read_excel(file_path: str, sheet_name: str) -> str:
            preview = [
                {
                    "‰∫ßÂìÅ": f"‰∫ßÂìÅ{i}",
                    "Â§áÊ≥®": "Ë∂ÖÈïøÂÜÖÂÆπ" * 30,
                    "ËØ¥Êòé": "X" * 240,
                }
                for i in range(25)
            ]
            return json.dumps(
                {
                    "file": file_path,
                    "sheet": sheet_name,
                    "shape": {"rows": 5000, "columns": 30},
                    "preview": preview,
                },
                ensure_ascii=False,
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string"},
                        "sheet_name": {"type": "string"},
                    },
                    "required": ["file_path", "sheet_name"],
                },
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        config = _make_config(
            window_perception_system_budget_tokens=400,
            window_perception_max_windows=2,
            window_perception_minimized_tokens=40,
        )
        engine = AgentEngine(config, registry)

        for idx, file_name in enumerate(("q1.xlsx", "q2.xlsx", "q3.xlsx"), start=1):
            tc = SimpleNamespace(
                id=f"call_read_{idx}",
                function=SimpleNamespace(
                    name="read_excel",
                    arguments=json.dumps({"file_path": file_name, "sheet_name": "Q1"}),
                ),
            )
            await engine._execute_tool_call(
                tc=tc,
                tool_scope=["read_excel"],
                on_event=None,
                iteration=idx,
                route_result=None,
            )

        notice = engine._build_window_perception_notice()
        tokens = TokenCounter.count_message({"role": "system", "content": notice})
        assert tokens <= config.window_perception_system_budget_tokens
        assert "## Á™óÂè£ÊÑüÁü•‰∏ä‰∏ãÊñá" in notice
        assert "q3.xlsx" in notice
        assert "q1.xlsx" not in notice

    @pytest.mark.asyncio
    async def test_window_perception_lifecycle_ages_to_background_and_suspended(self) -> None:
        def read_excel(file_path: str, sheet_name: str) -> str:
            return json.dumps(
                {
                    "file": file_path,
                    "sheet": sheet_name,
                    "shape": {"rows": 2004, "columns": 12},
                    "preview": [{"ËÆ¢ÂçïÁºñÂè∑": "ORD-1", "Êó•Êúü": "2025-01-01", "ÈáëÈ¢ù": 100}],
                },
                ensure_ascii=False,
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string"},
                        "sheet_name": {"type": "string"},
                    },
                    "required": ["file_path", "sheet_name"],
                },
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        config = _make_config(
            window_perception_background_after_idle=1,
            window_perception_suspend_after_idle=3,
            window_perception_terminate_after_idle=5,
        )
        engine = AgentEngine(config, registry)

        async def _read(file_path: str, iteration: int) -> None:
            tc = SimpleNamespace(
                id=f"call_read_{iteration}",
                function=SimpleNamespace(
                    name="read_excel",
                    arguments=json.dumps({"file_path": file_path, "sheet_name": "Q1"}),
                ),
            )
            await engine._execute_tool_call(
                tc=tc,
                tool_scope=["read_excel"],
                on_event=None,
                iteration=iteration,
                route_result=None,
            )

        await _read("sales.xlsx", 1)
        notice1 = engine._build_window_perception_notice()
        assert "„ÄêÁ™óÂè£ ¬∑ sales.xlsx / Q1„Äë" in notice1

        await _read("catalog.xlsx", 2)
        notice2 = engine._build_window_perception_notice()
        assert "„ÄêÁ™óÂè£ ¬∑ catalog.xlsx / Q1„Äë" in notice2
        assert "„ÄêÂêéÂè∞ ¬∑ sales.xlsx / Q1„Äë" in notice2

        notice3 = engine._build_window_perception_notice()
        assert "„ÄêÂêéÂè∞ ¬∑ sales.xlsx / Q1„Äë" in notice3
        assert "„ÄêÂêéÂè∞ ¬∑ catalog.xlsx / Q1„Äë" in notice3

        notice4 = engine._build_window_perception_notice()
        assert "„ÄêÊåÇËµ∑ ¬∑ sales.xlsx / Q1" in notice4
        assert "„ÄêÂêéÂè∞ ¬∑ catalog.xlsx / Q1„Äë" in notice4

    @pytest.mark.asyncio
    async def test_window_perception_terminated_window_can_reactivate(self) -> None:
        def read_excel(file_path: str, sheet_name: str) -> str:
            return json.dumps(
                {
                    "file": file_path,
                    "sheet": sheet_name,
                    "shape": {"rows": 500, "columns": 8},
                    "preview": [{"ÂàóA": 1, "ÂàóB": 2}],
                },
                ensure_ascii=False,
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string"},
                        "sheet_name": {"type": "string"},
                    },
                    "required": ["file_path", "sheet_name"],
                },
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        config = _make_config(
            window_perception_background_after_idle=1,
            window_perception_suspend_after_idle=2,
            window_perception_terminate_after_idle=3,
        )
        engine = AgentEngine(config, registry)

        tc = SimpleNamespace(
            id="call_read_init",
            function=SimpleNamespace(
                name="read_excel",
                arguments=json.dumps({"file_path": "reactivate.xlsx", "sheet_name": "Q1"}),
            ),
        )
        await engine._execute_tool_call(
            tc=tc,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=1,
            route_result=None,
        )

        notice1 = engine._build_window_perception_notice()
        assert "reactivate.xlsx" in notice1

        _ = engine._build_window_perception_notice()  # idle=1
        _ = engine._build_window_perception_notice()  # idle=2
        notice4 = engine._build_window_perception_notice()  # idle=3 -> terminated
        assert "reactivate.xlsx" not in notice4

        tc2 = SimpleNamespace(
            id="call_read_reopen",
            function=SimpleNamespace(
                name="read_excel",
                arguments=json.dumps({"file_path": "reactivate.xlsx", "sheet_name": "Q1"}),
            ),
        )
        await engine._execute_tool_call(
            tc=tc2,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=2,
            route_result=None,
        )
        notice5 = engine._build_window_perception_notice()
        assert "„ÄêÁ™óÂè£ ¬∑ reactivate.xlsx / Q1„Äë" in notice5

    @pytest.mark.asyncio
    async def test_window_perception_hybrid_advisor_is_non_blocking(self) -> None:
        def read_excel(file_path: str, sheet_name: str) -> str:
            return json.dumps(
                {
                    "file": file_path,
                    "sheet": sheet_name,
                    "shape": {"rows": 200, "columns": 8},
                    "preview": [{"ÂàóA": 1, "ÂàóB": 2}],
                },
                ensure_ascii=False,
            )

        async def _slow_response(**_kwargs):
            await asyncio.sleep(0.2)
            return _make_text_response(
                '{"task_type":"GENERAL_BROWSE","advices":[]}'
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string"},
                        "sheet_name": {"type": "string"},
                    },
                    "required": ["file_path", "sheet_name"],
                },
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        config = _make_config(
            window_perception_advisor_mode="hybrid",
            window_perception_advisor_trigger_window_count=1,
            window_perception_advisor_trigger_turn=1,
            window_perception_advisor_timeout_ms=1000,
        )
        engine = AgentEngine(config, registry)
        engine._router_client.chat.completions.create = AsyncMock(side_effect=_slow_response)

        tc = SimpleNamespace(
            id="call_read_init",
            function=SimpleNamespace(
                name="read_excel",
                arguments=json.dumps({"file_path": "sales.xlsx", "sheet_name": "Q1"}),
            ),
        )
        await engine._execute_tool_call(
            tc=tc,
            tool_scope=["read_excel"],
            on_event=None,
            iteration=1,
            route_result=None,
        )

        started = time.monotonic()
        notice = engine._build_window_perception_notice()
        elapsed = time.monotonic() - started
        assert "sales.xlsx" in notice
        assert elapsed < 0.15

    @pytest.mark.asyncio
    async def test_window_perception_hybrid_advisor_applies_cached_plan_next_turn(self) -> None:
        def read_excel(file_path: str, sheet_name: str) -> str:
            return json.dumps(
                {
                    "file": file_path,
                    "sheet": sheet_name,
                    "shape": {"rows": 2004, "columns": 12},
                    "preview": [{"ËÆ¢ÂçïÁºñÂè∑": "ORD-1", "Êó•Êúü": "2025-01-01", "ÈáëÈ¢ù": 100}],
                },
                ensure_ascii=False,
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string"},
                        "sheet_name": {"type": "string"},
                    },
                    "required": ["file_path", "sheet_name"],
                },
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        config = _make_config(
            window_perception_advisor_mode="hybrid",
            window_perception_advisor_trigger_window_count=1,
            window_perception_advisor_trigger_turn=1,
            window_perception_advisor_plan_ttl_turns=2,
        )
        engine = AgentEngine(config, registry)
        engine._router_client.chat.completions.create = AsyncMock(
            return_value=_make_text_response('{"task_type":"GENERAL_BROWSE","advices":[]}')
        )

        async def _read(file_path: str, iteration: int) -> None:
            tc = SimpleNamespace(
                id=f"call_read_{iteration}",
                function=SimpleNamespace(
                    name="read_excel",
                    arguments=json.dumps({"file_path": file_path, "sheet_name": "Q1"}),
                ),
            )
            await engine._execute_tool_call(
                tc=tc,
                tool_scope=["read_excel"],
                on_event=None,
                iteration=iteration,
                route_result=None,
            )

        await _read("sales.xlsx", 1)
        await _read("catalog.xlsx", 2)

        first_notice = engine._build_window_perception_notice()
        assert "sales.xlsx / Q1" in first_notice
        assert "catalog.xlsx / Q1" in first_notice

        plan_text = '{"task_type":"GENERAL_BROWSE","advices":[{"window_id":"sheet_1","tier":"suspended","reason":"done"}]}'
        engine._router_client.chat.completions.create = AsyncMock(
            return_value=_make_text_response(plan_text)
        )

        _ = engine._build_window_perception_notice()
        await asyncio.sleep(0)
        await asyncio.sleep(0)

        second_notice = engine._build_window_perception_notice()
        assert "„ÄêÊåÇËµ∑ ¬∑ sales.xlsx / Q1" in second_notice

    @pytest.mark.asyncio
    async def test_window_perception_hybrid_advisor_falls_back_when_router_fails(self) -> None:
        def read_excel(file_path: str, sheet_name: str) -> str:
            return json.dumps(
                {
                    "file": file_path,
                    "sheet": sheet_name,
                    "shape": {"rows": 2004, "columns": 12},
                    "preview": [{"ËÆ¢ÂçïÁºñÂè∑": "ORD-1", "Êó•Êúü": "2025-01-01", "ÈáëÈ¢ù": 100}],
                },
                ensure_ascii=False,
            )

        registry = ToolRegistry()
        registry.register_tools([
            ToolDef(
                name="read_excel",
                description="ËØªÂèñ",
                input_schema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string"},
                        "sheet_name": {"type": "string"},
                    },
                    "required": ["file_path", "sheet_name"],
                },
                func=read_excel,
                max_result_chars=0,
            ),
        ])
        config = _make_config(
            window_perception_advisor_mode="hybrid",
            window_perception_advisor_trigger_window_count=1,
            window_perception_advisor_trigger_turn=1,
        )
        engine = AgentEngine(config, registry)
        engine._router_client.chat.completions.create = AsyncMock(
            side_effect=RuntimeError("router failed")
        )

        async def _read(file_path: str, iteration: int) -> None:
            tc = SimpleNamespace(
                id=f"call_read_{iteration}",
                function=SimpleNamespace(
                    name="read_excel",
                    arguments=json.dumps({"file_path": file_path, "sheet_name": "Q1"}),
                ),
            )
            await engine._execute_tool_call(
                tc=tc,
                tool_scope=["read_excel"],
                on_event=None,
                iteration=iteration,
                route_result=None,
            )

        await _read("sales.xlsx", 1)
        await _read("catalog.xlsx", 2)
        _ = engine._build_window_perception_notice()
        await asyncio.sleep(0)
        fallback_notice = engine._build_window_perception_notice()
        assert "„ÄêÂêéÂè∞ ¬∑ sales.xlsx / Q1„Äë" in fallback_notice


class TestTaskUpdateFailureSemantics:
    """task_update Â§±Ë¥•ËØ≠‰πâ‰∏é‰∫ã‰ª∂‰∏ÄËá¥ÊÄßÊµãËØï„ÄÇ"""

    @pytest.mark.asyncio
    async def test_invalid_transition_returns_failure_and_no_task_item_updated_event(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        registry.register_tools(task_tools.get_tools())
        engine = AgentEngine(config, registry)
        engine._task_store.create("ÊµãËØï‰ªªÂä°", ["Â≠ê‰ªªÂä°A"])

        tc = SimpleNamespace(
            id="call_task_update_1",
            function=SimpleNamespace(
                name="task_update",
                arguments=json.dumps({"task_index": 0, "status": "completed"}),
            ),
        )

        events: list = []
        result = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["task_update"],
            on_event=events.append,
            iteration=1,
            route_result=None,
        )

        assert result.success is False
        assert "ÈùûÊ≥ïÁä∂ÊÄÅËΩ¨Êç¢" in result.result
        assert all(
            event.event_type != EventType.TASK_ITEM_UPDATED
            for event in events
        )
        assert engine._task_store.current is not None
        assert engine._task_store.current.items[0].status == TaskStatus.PENDING


class TestPlanModeControl:
    """plan mode ÊéßÂà∂ÂëΩ‰ª§‰∏éÊâßË°åÊµÅÊµãËØï„ÄÇ"""

    @pytest.mark.asyncio
    async def test_plan_status_on_off(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        status = await engine.chat("/plan status")
        assert "disabled" in status.reply

        turn_on = await engine.chat("/plan on")
        assert "Â∑≤ÂºÄÂêØ" in turn_on.reply
        assert engine.plan_mode_enabled is True

        turn_off = await engine.chat("/plan off")
        assert "Â∑≤ÂÖ≥Èó≠" in turn_off.reply
        assert engine.plan_mode_enabled is False

    @pytest.mark.asyncio
    async def test_planmode_alias_returns_tombstone_message(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        result = await engine.chat("/planmode on")
        assert "ÂëΩ‰ª§Â∑≤ÁßªÈô§ÔºåËØ∑‰ΩøÁî® /plan ..." in result.reply
        assert engine.plan_mode_enabled is False

    @pytest.mark.asyncio
    async def test_plan_mode_alias_returns_tombstone_message(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        result = await engine.chat("/plan_mode status")
        assert "ÂëΩ‰ª§Â∑≤ÁßªÈô§ÔºåËØ∑‰ΩøÁî® /plan ..." in result.reply
        assert engine.plan_mode_enabled is False

    @pytest.mark.asyncio
    async def test_plan_mode_message_generates_pending_plan_only(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine._plan_mode_enabled = True
        engine._route_skills = AsyncMock()

        draft = PlanDraft(
            plan_id="pln_test_001",
            markdown="# ËÆ°Âàí\n\n## ‰ªªÂä°Ê∏ÖÂçï\n- [ ] A",
            title="ÊµãËØïËÆ°Âàí",
            subtasks=["A"],
            file_path=".excelmanus/plans/plan_test.md",
            source="plan_mode",
            objective="ËØ∑ËßÑÂàíÊµãËØï‰ªªÂä°",
            created_at_utc="2026-02-13T00:00:00Z",
        )

        async def _fake_create_pending(**kwargs):
            engine._pending_plan = PendingPlanState(draft=draft)
            return draft, None

        engine._create_pending_plan_draft = AsyncMock(side_effect=_fake_create_pending)
        result = await engine.chat("ËØ∑ËßÑÂàíÊµãËØï‰ªªÂä°")
        assert "ÂæÖ‰Ω†ÂÆ°Êâπ" in result.reply
        assert engine._route_skills.await_count == 0
        assert engine._pending_plan is not None

    @pytest.mark.asyncio
    async def test_plan_approve_from_plan_mode_creates_tasklist_and_executes(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine._plan_mode_enabled = True

        draft = PlanDraft(
            plan_id="pln_test_approve_1",
            markdown="# Ëá™Âä®ÂåñËÆ°Âàí\n\n## ‰ªªÂä°Ê∏ÖÂçï\n- [ ] Á¨¨‰∏ÄÊ≠•\n- [ ] Á¨¨‰∫åÊ≠•",
            title="Ëá™Âä®ÂåñËÆ°Âàí",
            subtasks=["Á¨¨‰∏ÄÊ≠•", "Á¨¨‰∫åÊ≠•"],
            file_path=".excelmanus/plans/plan_test.md",
            source="plan_mode",
            objective="ÊâßË°åËá™Âä®Âåñ‰ªªÂä°",
            created_at_utc="2026-02-13T00:00:00Z",
        )
        engine._pending_plan = PendingPlanState(draft=draft)
        engine._route_skills = AsyncMock(
            return_value=SkillMatchResult(
                skills_used=[],
                tool_scope=["add_numbers"],
                route_mode="fallback",
                system_contexts=[],
            )
        )
        engine._client.chat.completions.create = AsyncMock(
            return_value=_make_text_response("ÊâßË°åÂÆåÊàê")
        )

        result = await engine.chat("/plan approve pln_test_approve_1")
        assert "ÊâßË°åÂÆåÊàê" in result.reply
        assert engine.plan_mode_enabled is False
        assert engine._task_store.current is not None
        assert engine._task_store.current.title == "Ëá™Âä®ÂåñËÆ°Âàí"
        assert "Êù•Ê∫ê: .excelmanus/plans/plan_test.md" in (engine._approved_plan_context or "")

    @pytest.mark.asyncio
    async def test_task_create_hook_enters_pending_plan(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        draft = PlanDraft(
            plan_id="pln_task_create_hook",
            markdown="# ËÆ°Âàí\n\n## ‰ªªÂä°Ê∏ÖÂçï\n- [ ] A",
            title="‰ªªÂä°Ê∏ÖÂçï",
            subtasks=["A"],
            file_path=".excelmanus/plans/plan_hook.md",
            source="task_create_hook",
            objective="ËçâÁ®ø‰ªªÂä°",
            created_at_utc="2026-02-13T00:00:00Z",
        )

        async def _fake_create_pending(**kwargs):
            engine._pending_plan = PendingPlanState(
                draft=draft,
                tool_call_id="call_tc",
                route_to_resume=kwargs.get("route_to_resume"),
            )
            return draft, None

        engine._create_pending_plan_draft = AsyncMock(side_effect=_fake_create_pending)
        tc = SimpleNamespace(
            id="call_tc",
            function=SimpleNamespace(
                name="task_create",
                arguments=json.dumps({"title": "‰ªªÂä°Ê∏ÖÂçï", "subtasks": ["A"]}),
            ),
        )
        route_result = SkillMatchResult(
            skills_used=[],
            tool_scope=["task_create"],
            route_mode="fallback",
            system_contexts=[],
        )
        result = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["task_create"],
            on_event=None,
            iteration=1,
            route_result=route_result,
        )
        assert result.success is True
        assert result.pending_plan is True
        assert result.defer_tool_result is True
        assert engine._task_store.current is None

    @pytest.mark.asyncio
    async def test_pending_plan_blocks_and_reject_unblocks(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        draft = PlanDraft(
            plan_id="pln_block_1",
            markdown="# ËÆ°Âàí\n\n## ‰ªªÂä°Ê∏ÖÂçï\n- [ ] A",
            title="ÈòªÂ°ûËÆ°Âàí",
            subtasks=["A"],
            file_path=".excelmanus/plans/plan_block.md",
            source="plan_mode",
            objective="ÈòªÂ°ûÁõÆÊ†á",
            created_at_utc="2026-02-13T00:00:00Z",
        )
        engine._pending_plan = PendingPlanState(draft=draft)

        blocked = await engine.chat("ÁªßÁª≠ÊâßË°å")
        assert "ÂæÖ‰Ω†ÂÆ°Êâπ" in blocked.reply

        rejected = await engine.chat("/plan reject pln_block_1")
        assert "Â∑≤ÊãíÁªùËÆ°Âàí" in rejected.reply
        assert engine._pending_plan is None

    def test_build_system_prompts_includes_approved_plan_context(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine._approved_plan_context = "Êù•Ê∫ê: .excelmanus/plans/plan_x.md\n# ËÆ°Âàí"

        prompts = engine._build_system_prompts([])
        assert len(prompts) == 1
        assert "## Â∑≤ÊâπÂáÜËÆ°Âàí‰∏ä‰∏ãÊñá" in prompts[0]
        assert "plan_x.md" in prompts[0]


class TestManualSkillSlashCommand:
    """ÊâãÂä® Skill ÊñúÊù†ÂëΩ‰ª§Ëß£Êûê‰∏éË∑ØÁî±„ÄÇ"""

    @pytest.mark.asyncio
    async def test_route_mode_is_all_tools_when_skill_router_missing(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine._skill_router = None
        engine._client.chat.completions.create = AsyncMock(
            return_value=_make_text_response("ok")
        )

        result = await engine.chat("ËØ∑ËØªÂèñÊï∞ÊçÆ")
        assert result == "ok"
        assert engine.last_route_result.route_mode == "all_tools"

    @pytest.mark.asyncio
    async def test_slash_skill_command_maps_to_slash_route_args(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        route_result = SkillMatchResult(
            skills_used=["data_basic"],
            tool_scope=[],
            route_mode="hint_direct",
            system_contexts=[],
        )
        mock_loader = MagicMock()
        mock_loader.get_skillpacks.return_value = {"data_basic": MagicMock()}
        mock_router = MagicMock()
        mock_router._loader = mock_loader
        mock_router.route = AsyncMock(return_value=route_result)
        engine._skill_router = mock_router
        engine._client.chat.completions.create = AsyncMock(
            return_value=_make_text_response("ok")
        )

        result = await engine.chat("/data_basic ËØ∑ÂàÜÊûêËøô‰∏™Êñá‰ª∂")
        assert result == "ok"

        _, kwargs = mock_router.route.call_args
        assert kwargs["slash_command"] == "data_basic"
        assert kwargs["raw_args"] == "ËØ∑ÂàÜÊûêËøô‰∏™Êñá‰ª∂"

    @pytest.mark.asyncio
    async def test_explicit_slash_command_arguments_pass_through(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        route_result = SkillMatchResult(
            skills_used=["data_basic"],
            tool_scope=[],
            route_mode="slash_direct",
            system_contexts=[],
            parameterized=True,
        )
        mock_loader = MagicMock()
        mock_loader.get_skillpacks.return_value = {"data_basic": MagicMock()}
        mock_router = MagicMock()
        mock_router._loader = mock_loader
        mock_router.route = AsyncMock(return_value=route_result)
        engine._skill_router = mock_router
        engine._client.chat.completions.create = AsyncMock(
            return_value=_make_text_response("ok")
        )

        await engine.chat(
            "ÊâßË°åÊäÄËÉΩ",
            slash_command="data_basic",
            raw_args='"sales data.xlsx" bar',
        )
        _, kwargs = mock_router.route.call_args
        assert kwargs["slash_command"] == "data_basic"
        assert kwargs["raw_args"] == '"sales data.xlsx" bar'

    def test_resolve_skill_command_normalizes_dash_and_underscore(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        mock_loader = MagicMock()
        mock_loader.get_skillpacks.return_value = {"data_basic": MagicMock()}
        mock_router = MagicMock()
        mock_router._loader = mock_loader
        engine._skill_router = mock_router

        assert engine.resolve_skill_command("/data_basic") == "data_basic"
        assert engine.resolve_skill_command("/data-basic ÂèÇÊï∞") == "data_basic"
        assert engine.resolve_skill_command("/DATA_BASIC") == "data_basic"

    def test_resolve_skill_command_ignores_path_like_input(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        mock_loader = MagicMock()
        mock_loader.get_skillpacks.return_value = {"data_basic": MagicMock()}
        mock_router = MagicMock()
        mock_router._loader = mock_loader
        engine._skill_router = mock_router

        assert engine.resolve_skill_command("/Users/test/file.xlsx") is None
        assert engine.resolve_skill_command("/tmp/data.xlsx") is None

    def test_resolve_skill_command_supports_namespace(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        mock_loader = MagicMock()
        mock_loader.get_skillpacks.return_value = {
            "team/data-cleaner": MagicMock(),
        }
        mock_router = MagicMock()
        mock_router._loader = mock_loader
        engine._skill_router = mock_router

        assert (
            engine.resolve_skill_command("/team/data-cleaner --mode fast")
            == "team/data-cleaner"
        )
        assert engine.resolve_skill_command("/team/data-cleaner.xlsx") is None

    def test_resolve_skill_command_respects_user_invocable(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        mock_loader = MagicMock()
        mock_loader.get_skillpacks.return_value = {
            "private_skill": Skillpack(
                name="private_skill",
                description="private",
                allowed_tools=["add_numbers"],
                triggers=[],
                instructions="",
                source="project",
                root_dir="/tmp/private",
                user_invocable=False,
            )
        }
        mock_router = MagicMock()
        mock_router._loader = mock_loader
        engine._skill_router = mock_router

        assert engine.resolve_skill_command("/private_skill run") is None

    @pytest.mark.asyncio
    async def test_chat_rejects_slash_for_not_user_invocable_skill(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        engine._route_skills = AsyncMock(
            return_value=SkillMatchResult(
                skills_used=[],
                tool_scope=[],
                route_mode="slash_not_user_invocable",
                system_contexts=[],
            )
        )
        result = await engine.chat(
            "/private_skill do",
            slash_command="private_skill",
            raw_args="do",
        )
        assert isinstance(result, ChatResult)
        assert "‰∏çÂÖÅËÆ∏ÊâãÂä®Ë∞ÉÁî®" in result.reply


class TestForkPathRemoved:
    """fork ÈìæË∑ØÂ∑≤Á°¨ÁßªÈô§Ôºå‰ªÖ‰øùÁïôÊòæÂºè delegate_to_subagent„ÄÇ"""

    @pytest.mark.asyncio
    async def test_chat_with_active_skill_no_longer_auto_delegates(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        engine._active_skill = Skillpack(
            name="excel_code_runner",
            description="‰ª£Á†ÅÂ§ÑÁêÜ",
            allowed_tools=[],
            triggers=[],
            instructions="",
            source="project",
            root_dir="/tmp/skill",
        )
        route_result = SkillMatchResult(
            skills_used=["excel_code_runner"],
            tool_scope=["add_numbers"],
            route_mode="fallback",
            system_contexts=["[Skillpack] excel_code_runner"],
        )
        engine._route_skills = AsyncMock(return_value=route_result)
        engine._delegate_to_subagent = AsyncMock(
            return_value=DelegateSubagentOutcome(
                reply="‰∏çÂ∫îË¢´Ë∞ÉÁî®",
                success=True,
            )
        )
        engine._client.chat.completions.create = AsyncMock(
            return_value=_make_text_response("‰∏ª‰ª£ÁêÜÊâßË°åÂÆåÊàê„ÄÇ")
        )

        result = await engine.chat("ËØ∑Â§ÑÁêÜËøô‰∏™Â§ßÊñá‰ª∂")
        assert result.reply == "‰∏ª‰ª£ÁêÜÊâßË°åÂÆåÊàê„ÄÇ"
        engine._delegate_to_subagent.assert_not_awaited()
        engine._client.chat.completions.create.assert_awaited_once()

    @pytest.mark.asyncio
    async def test_select_skill_success_no_longer_triggers_auto_delegate(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine._memory.add_user_message("ËØ∑ÂàÜÊûêÈîÄÂîÆË∂ãÂäø")
        engine._delegate_to_subagent = AsyncMock()

        async def _fake_execute_tool_call(*args, **kwargs) -> ToolCallResult:
            engine._active_skill = Skillpack(
                name="team/analyst",
                description="ÊôÆÈÄöÊäÄËÉΩ",
                allowed_tools=["add_numbers"],
                triggers=[],
                instructions="",
                source="project",
                root_dir="/tmp/skill",
            )
            return ToolCallResult(
                tool_name="select_skill",
                arguments={"skill_name": "team/analyst"},
                result="OK",
                success=True,
            )

        engine._execute_tool_call = AsyncMock(side_effect=_fake_execute_tool_call)
        engine._client.chat.completions.create = AsyncMock(
            side_effect=[
                _make_tool_call_response(
                    [("call_1", "select_skill", json.dumps({"skill_name": "team/analyst"}))]
                ),
                _make_text_response("‰∏ª‰ª£ÁêÜÁªßÁª≠ÊâßË°å„ÄÇ"),
            ]
        )

        route_result = SkillMatchResult(
            skills_used=[],
            tool_scope=["select_skill"],
            route_mode="fallback",
            system_contexts=[],
        )
        result = await engine._tool_calling_loop(route_result, on_event=None)

        assert result.reply == "‰∏ª‰ª£ÁêÜÁªßÁª≠ÊâßË°å„ÄÇ"
        engine._delegate_to_subagent.assert_not_awaited()
        assert engine._client.chat.completions.create.call_count == 2

    def test_engine_has_no_run_fork_skill_entrypoint(self) -> None:
        engine = AgentEngine(_make_config(), _make_registry_with_tools())
        assert not hasattr(engine, "_run_fork_skill")


class TestDelegateSubagent:
    """delegate_to_subagent ÂÖÉÂ∑•ÂÖ∑ÊµãËØï„ÄÇ"""

    @pytest.mark.asyncio
    async def test_delegate_tool_call_runs_subagent_and_returns_summary(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine._delegate_to_subagent = AsyncMock(
            return_value=DelegateSubagentOutcome(
                reply="Â≠ê‰ª£ÁêÜÊëòË¶Å",
                success=True,
                picked_agent="explorer",
                task_text="Êé¢Êü•ÈîÄÈáèÂºÇÂ∏∏",
                normalized_paths=["sales.xlsx"],
                subagent_result=None,
            )
        )

        tc = SimpleNamespace(
            id="call_1",
            function=SimpleNamespace(
                name="delegate_to_subagent",
                arguments=json.dumps(
                    {"task": "Êé¢Êü•ÈîÄÈáèÂºÇÂ∏∏", "file_paths": ["sales.xlsx"]},
                ),
            ),
        )

        result = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["delegate_to_subagent"],
            on_event=None,
            iteration=1,
        )

        assert result.success is True
        assert result.result == "Â≠ê‰ª£ÁêÜÊëòË¶Å"

    @pytest.mark.asyncio
    async def test_delegate_updates_window_perception_context_from_subagent(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        engine.run_subagent = AsyncMock(
            return_value=SubagentResult(
                success=True,
                summary="Â≠ê‰ª£ÁêÜÊëòË¶Å",
                subagent_name="explorer",
                permission_mode="readOnly",
                conversation_id="conv_1",
                observed_files=["./examples/bench/stress_test_comprehensive.xlsx"],
            )
        )

        result = await engine._handle_delegate_to_subagent(
            task="Êü•ÊâæÂåÖÂê´ÈîÄÂîÆÊòéÁªÜÂ∑•‰ΩúË°®ÁöÑÊñá‰ª∂",
            agent_name="explorer",
            file_paths=None,
        )
        assert result == "Â≠ê‰ª£ÁêÜÊëòË¶Å"

        notice = engine._build_window_perception_notice()
        assert "examples/bench/stress_test_comprehensive.xlsx" in notice

        prompts = engine._build_system_prompts([])
        assert len(prompts) == 1
        assert "examples/bench/stress_test_comprehensive.xlsx" in prompts[0]

    @pytest.mark.asyncio
    async def test_run_subagent_passes_window_context_and_enricher(self) -> None:
        config = _make_config(window_perception_enabled=True)
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        engine._subagent_registry = MagicMock()
        engine._subagent_registry.get.return_value = SubagentConfig(
            name="explorer",
            description="ÊµãËØï",
            allowed_tools=["read_excel"],
            permission_mode="readOnly",
        )
        engine._subagent_executor.run = AsyncMock(
            return_value=SubagentResult(
                success=True,
                summary="ok",
                subagent_name="explorer",
                permission_mode="readOnly",
                conversation_id="conv_x",
            )
        )

        engine._window_perception.observe_subagent_context(
            candidate_paths=["./examples/bench/stress_test_comprehensive.xlsx"],
            subagent_name="explorer",
            task="È¢ÑÁÉ≠Á™óÂè£",
        )

        result = await engine.run_subagent(agent_name="explorer", prompt="ËØ∑ÂàÜÊûê")

        assert result.success is True
        kwargs = engine._subagent_executor.run.await_args.kwargs
        assert "Á™óÂè£ÊÑüÁü•‰∏ä‰∏ãÊñá" in kwargs["parent_context"]
        assert callable(kwargs["tool_result_enricher"])

    @pytest.mark.asyncio
    async def test_delegate_pending_approval_asks_user_and_supports_fullaccess_retry(
        self,
        tmp_path: Path,
    ) -> None:
        config = _make_config(workspace_root=str(tmp_path))
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        pending = engine._approval.create_pending(
            tool_name="run_code",
            arguments={"script": "print('ok')"},
            tool_scope=["run_code"],
        )

        engine.run_subagent = AsyncMock(
            side_effect=[
                SubagentResult(
                    success=False,
                    summary="Â≠ê‰ª£ÁêÜÂëΩ‰∏≠È´òÈ£éÈô©Êìç‰Ωú",
                    subagent_name="analyst",
                    permission_mode="default",
                    conversation_id="conv_1",
                    pending_approval_id=pending.approval_id,
                ),
                SubagentResult(
                    success=True,
                    summary="ÈáçËØïÂÆåÊàê",
                    subagent_name="analyst",
                    permission_mode="default",
                    conversation_id="conv_2",
                ),
            ]
        )

        tc = SimpleNamespace(
            id="call_1",
            function=SimpleNamespace(
                name="delegate_to_subagent",
                arguments=json.dumps(
                    {
                        "task": "ÁªüËÆ°ÂüéÂ∏ÇÈîÄÂîÆÈ¢ù",
                        "agent_name": "analyst",
                        "file_paths": ["examples/bench/stress_test_comprehensive.xlsx"],
                    },
                    ensure_ascii=False,
                ),
            ),
        )

        first = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["delegate_to_subagent"],
            on_event=None,
            iteration=1,
        )
        assert first.success is True
        assert first.pending_question is True
        assert engine.has_pending_question() is True
        prompt = engine._question_flow.format_prompt()
        assert "fullAccess" in prompt
        assert pending.approval_id in prompt

        resumed = await engine.chat("2")
        assert "Â∑≤ÂºÄÂêØ fullAccess" in resumed.reply
        assert "Â∑≤ÊãíÁªùÂæÖÁ°ÆËÆ§Êìç‰Ωú" in resumed.reply
        assert "ÈáçËØïÂÆåÊàê" in resumed.reply
        assert engine.full_access_enabled is True
        assert engine._approval.pending is None
        assert engine.run_subagent.await_count == 2

    @pytest.mark.asyncio
    async def test_delegate_rejects_invalid_file_paths_type(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        tc = SimpleNamespace(
            id="call_1",
            function=SimpleNamespace(
                name="delegate_to_subagent",
                arguments=json.dumps(
                    {"task": "Êé¢Êü•ÈîÄÈáèÂºÇÂ∏∏", "file_paths": "sales.xlsx"}
                ),
            ),
        )

        result = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["delegate_to_subagent"],
            on_event=None,
            iteration=1,
        )

        assert result.success is False
        assert "file_paths ÂøÖÈ°ª‰∏∫Â≠óÁ¨¶‰∏≤Êï∞ÁªÑ" in result.result

    @pytest.mark.asyncio
    async def test_delegate_rejects_invalid_agent_name_type(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        tc = SimpleNamespace(
            id="call_1",
            function=SimpleNamespace(
                name="delegate_to_subagent",
                arguments=json.dumps(
                    {"task": "Êé¢Êü•ÈîÄÈáèÂºÇÂ∏∏", "agent_name": 123},
                ),
            ),
        )

        result = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["delegate_to_subagent"],
            on_event=None,
            iteration=1,
        )
        assert result.success is False
        assert "agent_name ÂøÖÈ°ª‰∏∫Â≠óÁ¨¶‰∏≤" in result.result

    @pytest.mark.asyncio
    async def test_auto_select_subagent_uses_description_catalog(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine._subagent_registry = MagicMock()
        engine._subagent_registry.list_all.return_value = [
            SubagentConfig(
                name="explorer",
                description="ÁõÆÂΩï‰∏é Excel ÁªìÊûÑÊé¢Êü•",
            ),
            SubagentConfig(
                name="analyst",
                description="ÁªüËÆ°ÂàÜÊûê‰∏éÂºÇÂ∏∏ÂÆö‰Ωç",
            ),
        ]
        engine._subagent_registry.build_catalog.return_value = (
            "ÂèØÁî®Â≠ê‰ª£ÁêÜÔºö\n- explorerÔºöÁõÆÂΩï‰∏é Excel ÁªìÊûÑÊé¢Êü•\n- analystÔºöÁªüËÆ°ÂàÜÊûê‰∏éÂºÇÂ∏∏ÂÆö‰Ωç",
            ["explorer", "analyst"],
        )
        engine._router_client.chat.completions.create = AsyncMock(
            return_value=_make_text_response('{"agent_name":"explorer"}')
        )

        picked = await engine._auto_select_subagent(
            task="ËØ∑ÂÖàÊÄªÁªìËøô‰∏™Êñá‰ª∂Â§πÁªìÊûÑ",
            file_paths=["data"],
        )

        assert picked == "explorer"
        _, kwargs = engine._router_client.chat.completions.create.call_args
        messages = kwargs["messages"]
        assert "ÂÄôÈÄâÂ≠ê‰ª£ÁêÜÔºö" in messages[1]["content"]
        assert "explorer" in messages[1]["content"]
        assert "analyst" in messages[1]["content"]
        assert "Áõ∏ÂÖ≥Êñá‰ª∂Ôºödata" in messages[1]["content"]

    @pytest.mark.asyncio
    async def test_auto_select_subagent_fallbacks_to_explorer_on_invalid_choice(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine._subagent_registry = MagicMock()
        engine._subagent_registry.list_all.return_value = [
            SubagentConfig(name="explorer", description="ÁõÆÂΩïÊé¢Êü•"),
            SubagentConfig(name="writer", description="ÂÜôÂÖ•ÊîπÈÄ†"),
        ]
        engine._subagent_registry.build_catalog.return_value = (
            "ÂèØÁî®Â≠ê‰ª£ÁêÜÔºö\n- explorerÔºöÁõÆÂΩïÊé¢Êü•\n- writerÔºöÂÜôÂÖ•ÊîπÈÄ†",
            ["explorer", "writer"],
        )
        engine._router_client.chat.completions.create = AsyncMock(
            return_value=_make_text_response('{"agent_name":"unknown"}')
        )

        picked = await engine._auto_select_subagent(
            task="ËØ∑ÂàÜÊûê‰∏Ä‰∏ã",
            file_paths=[],
        )

        assert picked == "explorer"


class TestAskUserFlow:
    """ask_user ÊåÇËµ∑ÊÅ¢Â§ç‰∏éÈòüÂàóË°å‰∏∫ÊµãËØï„ÄÇ"""

    @staticmethod
    def _ask_question_payload(
        *,
        header: str = "ÂÆûÁé∞ÊñπÊ°à",
        text: str = "ËØ∑ÈÄâÊã©ÂÆûÁé∞ÊñπÊ°à",
        multi_select: bool = False,
    ) -> dict:
        return {
            "question": {
                "header": header,
                "text": text,
                "options": [
                    {"label": "ÊñπÊ°àA", "description": "Âø´ÈÄüÂÆûÁé∞"},
                    {"label": "ÊñπÊ°àB", "description": "Á®≥ÂÅ•ÂÆûÁé∞"},
                ],
                "multiSelect": multi_select,
            }
        }

    @pytest.mark.asyncio
    async def test_ask_user_suspends_and_resumes_without_reroute(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        route_result = SkillMatchResult(
            skills_used=[],
            tool_scope=["ask_user", "add_numbers"],
            route_mode="llm_confirm",
            system_contexts=[],
        )
        engine._route_skills = AsyncMock(return_value=route_result)

        ask_response = _make_tool_call_response(
            [
                (
                    "call_q1",
                    "ask_user",
                    json.dumps(self._ask_question_payload(), ensure_ascii=False),
                )
            ]
        )
        do_work_response = _make_tool_call_response(
            [("call_add", "add_numbers", json.dumps({"a": 1, "b": 2}))]
        )
        final_response = _make_text_response("Â∑≤Êåâ‰Ω†ÁöÑÈÄâÊã©ÂÆåÊàêÔºåÁªìÊûúÊòØ 3„ÄÇ")
        engine._client.chat.completions.create = AsyncMock(
            side_effect=[ask_response, do_work_response, final_response]
        )

        first = await engine.chat("ËØ∑ÂÆåÊàê‰ªªÂä°")
        assert "ËØ∑ÂÖàÂõûÁ≠îËøô‰∏™ÈóÆÈ¢òÂêéÂÜçÁªßÁª≠" in first.reply
        assert engine.has_pending_question() is True
        assert engine._route_skills.await_count == 1

        resumed = await engine.chat("1")
        assert resumed.reply == "Â∑≤Êåâ‰Ω†ÁöÑÈÄâÊã©ÂÆåÊàêÔºåÁªìÊûúÊòØ 3„ÄÇ"
        assert engine.has_pending_question() is False
        # ÂõûÁ≠îÈóÆÈ¢òÂêéÁõ¥Êé•ÊÅ¢Â§çÊâßË°åÔºå‰∏çÂ∫îÈáçÊñ∞Ë∑ØÁî±
        assert engine._route_skills.await_count == 1

        tool_msgs = [m for m in engine.memory.get_messages() if m.get("role") == "tool"]
        ask_msg = next(m for m in tool_msgs if m.get("tool_call_id") == "call_q1")
        ask_payload = json.loads(ask_msg["content"])
        assert ask_payload["question_id"].startswith("qst_")
        assert ask_payload["multi_select"] is False
        assert ask_payload["selected_options"][0]["label"] == "ÊñπÊ°àA"

    @pytest.mark.asyncio
    async def test_fifo_multiple_questions_and_skip_non_ask_user(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        route_result = SkillMatchResult(
            skills_used=[],
            tool_scope=["ask_user", "add_numbers"],
            route_mode="llm_confirm",
            system_contexts=[],
        )
        engine._route_skills = AsyncMock(return_value=route_result)

        first_round = _make_tool_call_response(
            [
                (
                    "call_q1",
                    "ask_user",
                    json.dumps(
                        self._ask_question_payload(
                            header="ËØ≠Ë®Ä",
                            text="ÈÄâÊã©ÂºÄÂèëËØ≠Ë®Ä",
                            multi_select=False,
                        ),
                        ensure_ascii=False,
                    ),
                ),
                ("call_skip", "add_numbers", json.dumps({"a": 10, "b": 20})),
                (
                    "call_q2",
                    "ask_user",
                    json.dumps(
                        self._ask_question_payload(
                            header="Á∫¶Êùü",
                            text="ÈÄâÊã©Á∫¶ÊùüÁ≠ñÁï•",
                            multi_select=True,
                        ),
                        ensure_ascii=False,
                    ),
                ),
            ]
        )
        final_response = _make_text_response("‰∏§‰∏™ÈóÆÈ¢òÈÉΩÁ°ÆËÆ§ÂÆåÊØï„ÄÇ")
        engine._client.chat.completions.create = AsyncMock(
            side_effect=[first_round, final_response]
        )

        asked = await engine.chat("ÂºÄÂßãÊâßË°å")
        assert "ÈÄâÊã©ÂºÄÂèëËØ≠Ë®Ä" in asked.reply
        assert engine.has_pending_question() is True
        assert len(asked.tool_calls) == 3
        skipped = next(r for r in asked.tool_calls if r.tool_name == "add_numbers")
        assert skipped.success is True
        assert "Â∑≤Ë∑≥Ëøá" in skipped.result

        second_prompt = await engine.chat("1")
        assert "ÈÄâÊã©Á∫¶ÊùüÁ≠ñÁï•" in second_prompt.reply
        assert engine.has_pending_question() is True

        done = await engine.chat("1\nËá™ÂÆö‰πâÁ≠ñÁï•")
        assert done.reply == "‰∏§‰∏™ÈóÆÈ¢òÈÉΩÁ°ÆËÆ§ÂÆåÊØï„ÄÇ"
        assert engine.has_pending_question() is False
        assert engine._route_skills.await_count == 1

        tool_msgs = [m for m in engine.memory.get_messages() if m.get("role") == "tool"]
        assert any(m.get("tool_call_id") == "call_q1" for m in tool_msgs)
        q2_msg = next(m for m in tool_msgs if m.get("tool_call_id") == "call_q2")
        q2_payload = json.loads(q2_msg["content"])
        assert q2_payload["multi_select"] is True
        assert any(item["label"] == "ÊñπÊ°àA" for item in q2_payload["selected_options"])
        assert q2_payload["other_text"] == "Ëá™ÂÆö‰πâÁ≠ñÁï•"

    @pytest.mark.asyncio
    async def test_pending_question_blocks_slash_command(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        route_result = SkillMatchResult(
            skills_used=[],
            tool_scope=["ask_user"],
            route_mode="llm_confirm",
            system_contexts=[],
        )
        engine._route_skills = AsyncMock(return_value=route_result)

        ask_response = _make_tool_call_response(
            [
                (
                    "call_q1",
                    "ask_user",
                    json.dumps(self._ask_question_payload(), ensure_ascii=False),
                )
            ]
        )
        final_response = _make_text_response("Â∑≤ÊÅ¢Â§çÊâßË°å„ÄÇ")
        engine._client.chat.completions.create = AsyncMock(
            side_effect=[ask_response, final_response]
        )

        first = await engine.chat("ÂèëËµ∑ÊèêÈóÆ")
        assert engine.has_pending_question() is True
        assert "ËØ∑ÂÖàÂõûÁ≠îËøô‰∏™ÈóÆÈ¢òÂêéÂÜçÁªßÁª≠" in first.reply

        blocked = await engine.chat("/help")
        assert "ËØ∑ÂÖàÂõûÁ≠îÂêéÂÜç‰ΩøÁî®ÂëΩ‰ª§" in blocked.reply
        assert engine.has_pending_question() is True
        # ÂæÖÂõûÁ≠îÁä∂ÊÄÅ‰∏çËß¶ÂèëÈáçË∑ØÁî±
        assert engine._route_skills.await_count == 1

        resumed = await engine.chat("1")
        assert resumed.reply == "Â∑≤ÊÅ¢Â§çÊâßË°å„ÄÇ"
        assert engine.has_pending_question() is False

class TestMetaToolDefinitions:
    """ÂÖÉÂ∑•ÂÖ∑ÂÆö‰πâÁªìÊûÑ‰∏éÂä®ÊÄÅÊõ¥Êñ∞ÊµãËØïÔºàtask6.4Ôºâ„ÄÇ"""

    def test_build_meta_tools_schema_structure(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        mock_router = MagicMock()
        mock_router.build_skill_catalog.return_value = (
            "ÂèØÁî®ÊäÄËÉΩÔºö\n- data_basicÔºöÊï∞ÊçÆÂ§ÑÁêÜ\n- chart_basicÔºöÂõæË°®ÁîüÊàê",
            ["data_basic", "chart_basic"],
        )
        engine._skill_router = mock_router
        engine._subagent_registry = MagicMock()
        engine._subagent_registry.build_catalog.return_value = (
            "ÂèØÁî®Â≠ê‰ª£ÁêÜÔºö\n- folder_summarizerÔºöÁõÆÂΩïÊÄªÁªì",
            ["folder_summarizer"],
        )

        meta_tools = engine._build_meta_tools()
        assert len(meta_tools) == 4
        by_name = {tool["function"]["name"]: tool for tool in meta_tools}
        assert "select_skill" in by_name
        assert "delegate_to_subagent" in by_name
        assert "list_subagents" in by_name
        assert "ask_user" in by_name

        select_tool = by_name["select_skill"]["function"]
        select_params = select_tool["parameters"]
        assert "Skill_Catalog" in select_tool["description"]
        assert select_params["required"] == ["skill_name"]
        assert select_params["properties"]["skill_name"]["enum"] == [
            "data_basic",
            "chart_basic",
        ]
        assert "reason" in select_params["properties"]

        delegate_tool = by_name["delegate_to_subagent"]["function"]
        delegate_params = delegate_tool["parameters"]
        assert delegate_params["required"] == ["task"]
        assert delegate_params["properties"]["file_paths"]["type"] == "array"
        assert "agent_name" in delegate_params["properties"]
        assert delegate_params["properties"]["agent_name"]["enum"] == ["folder_summarizer"]
        assert "Subagent_Catalog" in delegate_tool["description"]
        assert "folder_summarizer" in delegate_tool["description"]

        ask_user_tool = by_name["ask_user"]["function"]
        ask_user_params = ask_user_tool["parameters"]
        assert ask_user_params["required"] == ["question"]
        question_schema = ask_user_params["properties"]["question"]
        assert question_schema["required"] == ["text", "header", "options"]
        assert question_schema["properties"]["options"]["minItems"] == 2
        assert question_schema["properties"]["options"]["maxItems"] == 4

    def test_build_meta_tools_reflects_updated_catalog(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        mock_router = MagicMock()
        mock_router.build_skill_catalog.side_effect = [
            ("ÂèØÁî®ÊäÄËÉΩÔºö\n- data_basicÔºöÊï∞ÊçÆÂ§ÑÁêÜ", ["data_basic"]),
            (
                "ÂèØÁî®ÊäÄËÉΩÔºö\n- data_basicÔºöÊï∞ÊçÆÂ§ÑÁêÜ\n- chart_basicÔºöÂõæË°®ÁîüÊàê",
                ["data_basic", "chart_basic"],
            ),
        ]
        engine._skill_router = mock_router

        first = engine._build_meta_tools()
        second = engine._build_meta_tools()

        first_enum = first[0]["function"]["parameters"]["properties"]["skill_name"]["enum"]
        second_enum = second[0]["function"]["parameters"]["properties"]["skill_name"]["enum"]
        assert first_enum == ["data_basic"]
        assert second_enum == ["data_basic", "chart_basic"]


class TestMetaToolScopeUpdate:
    """ÂÖÉÂ∑•ÂÖ∑Ë∞ÉÁî®ÂêéÂêåËΩÆÊõ¥Êñ∞Â∑•ÂÖ∑ËåÉÂõ¥Ôºàtask6.1Ôºâ„ÄÇ"""

    @pytest.mark.asyncio
    async def test_select_skill_updates_scope_within_same_iteration(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        data_skill = Skillpack(
            name="data_basic",
            description="Êï∞ÊçÆÂ§ÑÁêÜÊäÄËÉΩ",
            allowed_tools=["add_numbers"],
            triggers=["Êï∞ÊçÆ"],
            instructions="‰ΩøÁî® add_numbers ËøõË°åÊµãËØï„ÄÇ",
            source="system",
            root_dir="/tmp/data_basic",
        )

        mock_loader = MagicMock()
        mock_loader.get_skillpacks.return_value = {"data_basic": data_skill}
        mock_loader.get_skillpack.return_value = data_skill

        mock_router = MagicMock()
        mock_router._loader = mock_loader
        mock_router._find_skill_by_name = MagicMock(return_value=data_skill)
        mock_router.build_skill_catalog.return_value = (
            "ÂèØÁî®ÊäÄËÉΩÔºö\n- data_basicÔºöÊï∞ÊçÆÂ§ÑÁêÜÊäÄËÉΩ",
            ["data_basic"],
        )
        engine._skill_router = mock_router

        route_result = SkillMatchResult(
            skills_used=[],
            tool_scope=["select_skill"],
            route_mode="slash_direct",
            system_contexts=[],
        )
        engine._route_skills = AsyncMock(return_value=route_result)

        first_resp = _make_tool_call_response(
            [
                ("call_1", "select_skill", json.dumps({"skill_name": "data_basic"})),
                ("call_2", "add_numbers", json.dumps({"a": 1, "b": 2})),
            ]
        )
        second_resp = _make_text_response("done")
        engine._client.chat.completions.create = AsyncMock(
            side_effect=[first_resp, second_resp]
        )

        result = await engine.chat("ÊµãËØïÂêåËΩÆÂàáÊç¢")
        assert result == "done"

        tool_msgs = [m for m in engine.memory.get_messages() if m.get("role") == "tool"]
        add_numbers_msg = next(m for m in tool_msgs if m.get("tool_call_id") == "call_2")
        assert "3" in add_numbers_msg.get("content", "")


class TestFallbackScopeGuard:
    """fallback/slash_not_found Âú∫ÊôØ‰∏ãÂ∑•ÂÖ∑ÊùÉÈôêÊî∂Êïõ„ÄÇ"""

    def test_get_current_tool_scope_for_fallback_adds_only_meta_tools(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        route_result = SkillMatchResult(
            skills_used=[],
            tool_scope=["list_skills"],
            route_mode="fallback",
            system_contexts=[],
        )
        scope = engine._get_current_tool_scope(route_result=route_result)
        assert "list_skills" in scope
        assert "select_skill" in scope
        assert "delegate_to_subagent" in scope
        assert "list_subagents" in scope
        assert "ask_user" in scope
        assert "add_numbers" not in scope

    @pytest.mark.asyncio
    async def test_fallback_blocks_tool_until_select_skill_then_allows(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        data_skill = Skillpack(
            name="data_basic",
            description="Êï∞ÊçÆÂ§ÑÁêÜÊäÄËÉΩ",
            allowed_tools=["add_numbers"],
            triggers=["Êï∞ÊçÆ"],
            instructions="‰ΩøÁî® add_numbers ËøõË°åÊµãËØï„ÄÇ",
            source="system",
            root_dir="/tmp/data_basic",
        )
        mock_loader = MagicMock()
        mock_loader.get_skillpacks.return_value = {"data_basic": data_skill}
        mock_loader.get_skillpack.return_value = data_skill
        mock_router = MagicMock()
        mock_router._loader = mock_loader
        mock_router._find_skill_by_name = MagicMock(return_value=data_skill)
        mock_router.build_skill_catalog.return_value = (
            "ÂèØÁî®ÊäÄËÉΩÔºö\n- data_basicÔºöÊï∞ÊçÆÂ§ÑÁêÜÊäÄËÉΩ",
            ["data_basic"],
        )
        engine._skill_router = mock_router

        route_result = SkillMatchResult(
            skills_used=[],
            tool_scope=["list_skills"],
            route_mode="fallback",
            system_contexts=[],
        )
        initial_scope = engine._get_current_tool_scope(route_result=route_result)
        forbidden_call = SimpleNamespace(
            id="call_1",
            function=SimpleNamespace(
                name="add_numbers",
                arguments=json.dumps({"a": 1, "b": 2}),
            ),
        )
        forbidden_result = await engine._execute_tool_call(
            tc=forbidden_call,
            tool_scope=initial_scope,
            on_event=None,
            iteration=1,
        )
        assert forbidden_result.success is False
        error_payload = json.loads(forbidden_result.result)
        assert error_payload["error_code"] == "TOOL_NOT_ALLOWED"
        assert error_payload["tool"] == "add_numbers"

        await engine._handle_select_skill("data_basic")
        upgraded_scope = engine._get_current_tool_scope(route_result=route_result)
        assert "add_numbers" in upgraded_scope

        allowed_call = SimpleNamespace(
            id="call_2",
            function=SimpleNamespace(
                name="add_numbers",
                arguments=json.dumps({"a": 1, "b": 2}),
            ),
        )
        allowed_result = await engine._execute_tool_call(
            tc=allowed_call,
            tool_scope=upgraded_scope,
            on_event=None,
            iteration=1,
        )
        assert allowed_result.success is True
        assert allowed_result.result == "3"

    @pytest.mark.asyncio
    async def test_chat_last_route_scope_matches_effective_scope(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        route_result = SkillMatchResult(
            skills_used=[],
            tool_scope=["list_skills"],
            route_mode="fallback",
            system_contexts=[],
        )
        engine._route_skills = AsyncMock(return_value=route_result)
        engine._client.chat.completions.create = AsyncMock(
            return_value=_make_text_response("ok")
        )

        result = await engine.chat("ËØ∑ÂàÜÊûêËøô‰∏™Êñá‰ª∂")
        assert result == "ok"
        scope = engine.last_route_result.tool_scope
        assert "list_skills" in scope
        assert "select_skill" in scope
        assert "delegate_to_subagent" in scope
        assert "list_subagents" in scope
        assert "ask_user" in scope
        assert "add_numbers" not in scope


class TestMCPScopeSelector:
    """MCP Â∑•ÂÖ∑ÊéàÊùÉÈÄâÊã©Âô®Â±ïÂºÄ„ÄÇ"""

    @staticmethod
    def _register_mcp_test_tool(
        registry: ToolRegistry,
        *,
        server: str = "context7",
        tool: str = "query_docs",
        result: str = "mcp-ok",
    ) -> str:
        mcp_tool = add_tool_prefix(server, tool)
        registry.register_tool(
            ToolDef(
                name=mcp_tool,
                description="mcp-test-tool",
                input_schema={"type": "object", "properties": {}},
                func=lambda: result,
            )
        )
        return mcp_tool

    @pytest.mark.parametrize(
        ("route_mode", "tool_scope"),
        [
            ("fallback", ["list_skills"]),
            ("slash_not_found", ["list_skills"]),
            ("no_skillpack", ["list_skills"]),
            ("slash_direct", ["add_numbers"]),
        ],
    )
    def test_get_current_tool_scope_includes_mcp_for_all_route_modes(
        self,
        route_mode: str,
        tool_scope: list[str],
    ) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        mcp_tool = self._register_mcp_test_tool(registry)
        engine = AgentEngine(config, registry)

        route_result = SkillMatchResult(
            skills_used=[],
            tool_scope=tool_scope,
            route_mode=route_mode,
            system_contexts=[],
        )
        scope = engine._get_current_tool_scope(route_result=route_result)
        assert mcp_tool in scope

    def test_get_current_tool_scope_includes_mcp_when_active_skill(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        mcp_tool = self._register_mcp_test_tool(registry)
        engine = AgentEngine(config, registry)
        engine._active_skill = Skillpack(
            name="data_basic",
            description="active skill",
            allowed_tools=["add_numbers"],
            triggers=[],
            instructions="test",
            source="project",
            root_dir="/tmp/active_skill",
        )

        scope = engine._get_current_tool_scope(
            route_result=SkillMatchResult(
                skills_used=["data_basic"],
                tool_scope=["add_numbers"],
                route_mode="fallback",
                system_contexts=[],
            )
        )
        assert "add_numbers" in scope
        assert mcp_tool in scope

    def test_scope_expands_mcp_all_selector(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        mcp_tool_a = add_tool_prefix("context7", "query_docs")
        mcp_tool_b = add_tool_prefix("filesystem", "read_file")
        registry.register_tools(
            [
                ToolDef(
                    name=mcp_tool_a,
                    description="mcp-a",
                    input_schema={"type": "object", "properties": {}},
                    func=lambda: "ok-a",
                ),
                ToolDef(
                    name=mcp_tool_b,
                    description="mcp-b",
                    input_schema={"type": "object", "properties": {}},
                    func=lambda: "ok-b",
                ),
            ]
        )
        engine = AgentEngine(config, registry)
        engine._full_access_enabled = True

        route_result = SkillMatchResult(
            skills_used=["mcp_skill"],
            tool_scope=["mcp:*"],
            route_mode="slash_direct",
            system_contexts=[],
        )
        scope = engine._get_current_tool_scope(route_result=route_result)
        assert mcp_tool_a in scope
        assert mcp_tool_b in scope
        assert "select_skill" in scope

    def test_scope_expands_server_level_mcp_selector(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        context_tool_a = add_tool_prefix("context7", "query_docs")
        context_tool_b = add_tool_prefix("context7", "resolve_library_id")
        fs_tool = add_tool_prefix("filesystem", "read_file")
        registry.register_tools(
            [
                ToolDef(
                    name=context_tool_a,
                    description="context-a",
                    input_schema={"type": "object", "properties": {}},
                    func=lambda: "ok-a",
                ),
                ToolDef(
                    name=context_tool_b,
                    description="context-b",
                    input_schema={"type": "object", "properties": {}},
                    func=lambda: "ok-b",
                ),
                ToolDef(
                    name=fs_tool,
                    description="fs",
                    input_schema={"type": "object", "properties": {}},
                    func=lambda: "ok-c",
                ),
            ]
        )
        engine = AgentEngine(config, registry)

        route_result = SkillMatchResult(
            skills_used=["mcp_skill"],
            tool_scope=["mcp:context7:*"],
            route_mode="slash_direct",
            system_contexts=[],
        )
        scope = engine._get_current_tool_scope(route_result=route_result)
        assert context_tool_a in scope
        assert context_tool_b in scope
        # Á†¥ÂùèÊÄßÈáçÊûÑÂêéÔºåMCP Â∑•ÂÖ∑ÂÖ®Âú∫ÊôØÂÖ®ÈáèÊ≥®ÂÖ• scope„ÄÇ
        assert fs_tool in scope

    @pytest.mark.asyncio
    async def test_execute_tool_call_accepts_expanded_mcp_selector(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        context_tool = add_tool_prefix("context7", "query_docs")
        registry.register_tool(
            ToolDef(
                name=context_tool,
                description="context-tool",
                input_schema={"type": "object", "properties": {}},
                func=lambda: "mcp-ok",
            )
        )
        engine = AgentEngine(config, registry)
        engine._full_access_enabled = True

        route_result = SkillMatchResult(
            skills_used=["mcp_skill"],
            tool_scope=["mcp:context7:query_docs"],
            route_mode="slash_direct",
            system_contexts=[],
        )
        scope = engine._get_current_tool_scope(route_result=route_result)

        call = SimpleNamespace(
            id="call_mcp",
            function=SimpleNamespace(name=context_tool, arguments=json.dumps({})),
        )
        result = await engine._execute_tool_call(
            tc=call,
            tool_scope=scope,
            on_event=None,
            iteration=1,
        )
        assert result.success is True
        assert result.result == "mcp-ok"

    @pytest.mark.asyncio
    async def test_non_whitelist_mcp_still_requires_pending_approval(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        mcp_tool = self._register_mcp_test_tool(registry)
        engine = AgentEngine(config, registry)

        scope = engine._get_current_tool_scope(
            route_result=SkillMatchResult(
                skills_used=[],
                tool_scope=["list_skills"],
                route_mode="fallback",
                system_contexts=[],
            )
        )
        call = SimpleNamespace(
            id="call_mcp_pending",
            function=SimpleNamespace(name=mcp_tool, arguments=json.dumps({})),
        )
        result = await engine._execute_tool_call(
            tc=call,
            tool_scope=scope,
            on_event=None,
            iteration=1,
        )

        assert result.success is True
        assert result.pending_approval is True
        assert engine._approval.pending is not None

    @pytest.mark.asyncio
    async def test_whitelist_mcp_executes_without_fullaccess(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        mcp_tool = self._register_mcp_test_tool(registry)
        engine = AgentEngine(config, registry)
        engine._approval.register_mcp_auto_approve([mcp_tool])

        scope = engine._get_current_tool_scope(
            route_result=SkillMatchResult(
                skills_used=[],
                tool_scope=["list_skills"],
                route_mode="fallback",
                system_contexts=[],
            )
        )
        call = SimpleNamespace(
            id="call_mcp_auto",
            function=SimpleNamespace(name=mcp_tool, arguments=json.dumps({})),
        )
        result = await engine._execute_tool_call(
            tc=call,
            tool_scope=scope,
            on_event=None,
            iteration=1,
        )

        assert result.success is True
        assert result.pending_approval is False
        assert result.result == "mcp-ok"
        assert engine._approval.pending is None


class TestSkillMCPRequirements:
    """Skill ÁöÑ MCP ‰æùËµñÊ†°È™å„ÄÇ"""

    @pytest.mark.asyncio
    async def test_select_skill_rejects_when_required_mcp_server_missing(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        skill = Skillpack(
            name="need_mcp",
            description="‰æùËµñÂ§ñÈÉ® MCP",
            allowed_tools=["mcp:context7:*"],
            triggers=[],
            instructions="Ë∞ÉÁî® context7",
            source="project",
            root_dir="/tmp/need_mcp",
            required_mcp_servers=["context7"],
        )
        mock_loader = MagicMock()
        mock_loader.get_skillpacks.return_value = {"need_mcp": skill}
        mock_router = MagicMock()
        mock_router._loader = mock_loader
        mock_router._find_skill_by_name = MagicMock(return_value=skill)
        engine._skill_router = mock_router

        result = await engine._handle_select_skill("need_mcp")

        assert "MCP ‰æùËµñÊú™Êª°Ë∂≥" in result
        assert engine._active_skill is None

    @pytest.mark.asyncio
    async def test_select_skill_accepts_when_required_mcp_server_and_tool_ready(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        mcp_tool = add_tool_prefix("context7", "query_docs")
        registry.register_tool(
            ToolDef(
                name=mcp_tool,
                description="ÊñáÊ°£Êü•ËØ¢",
                input_schema={"type": "object", "properties": {}},
                func=lambda: "ok",
            )
        )
        engine = AgentEngine(config, registry)
        engine._mcp_manager._clients["context7"] = MagicMock()

        skill = Skillpack(
            name="need_mcp",
            description="‰æùËµñÂ§ñÈÉ® MCP",
            allowed_tools=["mcp:context7:*"],
            triggers=[],
            instructions="Ë∞ÉÁî® context7",
            source="project",
            root_dir="/tmp/need_mcp",
            required_mcp_servers=["context7"],
            required_mcp_tools=["context7:query_docs"],
        )
        mock_loader = MagicMock()
        mock_loader.get_skillpacks.return_value = {"need_mcp": skill}
        mock_router = MagicMock()
        mock_router._loader = mock_loader
        mock_router._find_skill_by_name = MagicMock(return_value=skill)
        engine._skill_router = mock_router

        result = await engine._handle_select_skill("need_mcp")

        assert result.startswith("OK")
        assert engine._active_skill is not None
        assert engine._active_skill.name == "need_mcp"


class TestCommandDispatchAndHooks:
    @pytest.mark.asyncio
    async def test_command_dispatch_maps_plain_args(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        registry.register_tool(
            ToolDef(
                name="echo_tool",
                description="ÂõûÊòæ",
                input_schema={
                    "type": "object",
                    "properties": {"input": {"type": "string"}},
                    "required": ["input"],
                },
                func=lambda input: input,
            )
        )
        engine = AgentEngine(config, registry)
        engine._full_access_enabled = True
        skill = Skillpack(
            name="echo",
            description="ÂëΩ‰ª§ÂàÜÂèë",
            allowed_tools=["echo_tool"],
            triggers=[],
            instructions="ÂõûÊòæËæìÂÖ•",
            source="project",
            root_dir="/tmp/echo",
            command_dispatch="tool",
            command_tool="echo_tool",
        )
        route_result = SkillMatchResult(
            skills_used=["echo"],
            tool_scope=["echo_tool"],
            route_mode="slash_direct",
            system_contexts=[],
        )

        result = await engine._run_command_dispatch_skill(
            skill=skill,
            raw_args="hello-dispatch",
            route_result=route_result,
            on_event=None,
        )
        assert result.reply == "hello-dispatch"
        assert result.tool_calls
        assert result.tool_calls[0].success is True

    @pytest.mark.asyncio
    async def test_pre_tool_hook_deny_blocks_tool(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine._active_skill = Skillpack(
            name="hook/deny",
            description="deny hook",
            allowed_tools=["add_numbers"],
            triggers=[],
            instructions="",
            source="project",
            root_dir="/tmp/hook",
            hooks={
                "PreToolUse": [
                    {
                        "matcher": "add_numbers",
                        "hooks": [{"type": "prompt", "decision": "deny", "reason": "blocked"}],
                    }
                ]
            },
        )

        tc = SimpleNamespace(
            id="call_hook_deny",
            function=SimpleNamespace(name="add_numbers", arguments=json.dumps({"a": 1, "b": 2})),
        )
        result = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["add_numbers"],
            on_event=None,
            iteration=1,
        )
        assert result.success is False
        assert "blocked" in result.result

    @pytest.mark.asyncio
    async def test_pre_tool_hook_ask_creates_pending_approval(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine._active_skill = Skillpack(
            name="hook/ask",
            description="ask hook",
            allowed_tools=["add_numbers"],
            triggers=[],
            instructions="",
            source="project",
            root_dir="/tmp/hook",
            hooks={
                "PreToolUse": [
                    {
                        "matcher": "add_numbers",
                        "hooks": [{"type": "prompt", "decision": "ask"}],
                    }
                ]
            },
        )

        tc = SimpleNamespace(
            id="call_hook_ask",
            function=SimpleNamespace(name="add_numbers", arguments=json.dumps({"a": 1, "b": 2})),
        )
        result = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["add_numbers"],
            on_event=None,
            iteration=1,
        )
        assert result.success is True
        assert result.pending_approval is True
        assert isinstance(result.approval_id, str) and result.approval_id

    @pytest.mark.asyncio
    async def test_pre_tool_hook_updated_input_is_applied(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine._active_skill = Skillpack(
            name="hook/update",
            description="update input hook",
            allowed_tools=["add_numbers"],
            triggers=[],
            instructions="",
            source="project",
            root_dir="/tmp/hook",
            hooks={
                "PreToolUse": [
                    {
                        "matcher": "add_numbers",
                        "hooks": [
                            {
                                "type": "prompt",
                                "decision": "allow",
                                "updated_input": {"a": 7, "b": 4},
                            }
                        ],
                    }
                ]
            },
        )

        tc = SimpleNamespace(
            id="call_hook_update",
            function=SimpleNamespace(name="add_numbers", arguments=json.dumps({"a": 1, "b": 2})),
        )
        result = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["add_numbers"],
            on_event=None,
            iteration=1,
        )
        assert result.success is True
        assert result.result == "11"

    @pytest.mark.asyncio
    async def test_pre_tool_hook_allow_skips_pending_approval_for_high_risk(
        self,
        tmp_path: Path,
    ) -> None:
        config = _make_config(workspace_root=str(tmp_path))
        registry = _make_registry_with_tools()

        def write_text_file(file_path: str, content: str) -> str:
            Path(file_path).write_text(content, encoding="utf-8")
            return "ok"

        registry.register_tool(
            ToolDef(
                name="write_text_file",
                description="ÂÜôÂÖ•ÊñáÊú¨",
                input_schema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string"},
                        "content": {"type": "string"},
                    },
                    "required": ["file_path", "content"],
                },
                func=write_text_file,
            )
        )
        engine = AgentEngine(config, registry)
        engine._active_skill = Skillpack(
            name="hook/allow",
            description="allow hook",
            allowed_tools=["write_text_file"],
            triggers=[],
            instructions="",
            source="project",
            root_dir="/tmp/hook",
            hooks={
                "PreToolUse": [
                    {
                        "matcher": "write_text_file",
                        "hooks": [{"type": "prompt", "decision": "allow"}],
                    }
                ]
            },
        )

        output = tmp_path / "hook_allow.txt"
        tc = SimpleNamespace(
            id="call_hook_allow",
            function=SimpleNamespace(
                name="write_text_file",
                arguments=json.dumps({"file_path": str(output), "content": "ok"}),
            ),
        )
        result = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["write_text_file"],
            on_event=None,
            iteration=1,
        )
        assert result.success is True
        assert result.pending_approval is False
        assert output.exists()

    def test_non_pre_tool_ask_downgrades_to_continue(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        skill = Skillpack(
            name="hook/ask_scope",
            description="ask scope",
            allowed_tools=["add_numbers"],
            triggers=[],
            instructions="",
            source="project",
            root_dir="/tmp/hook",
            hooks={
                "UserPromptSubmit": {
                    "type": "prompt",
                    "decision": "ask",
                    "reason": "ÈúÄË¶ÅÁ°ÆËÆ§",
                }
            },
        )

        result = engine._run_skill_hook(
            skill=skill,
            event=HookEvent.USER_PROMPT_SUBMIT,
            payload={"user_message": "ÊµãËØï"},
        )
        assert result is not None
        assert result.decision == HookDecision.CONTINUE
        assert "‰∏çÊîØÊåÅ ASK" in result.reason

    @pytest.mark.asyncio
    async def test_pre_tool_agent_hook_runs_subagent_and_injects_context(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine.run_subagent = AsyncMock(
            return_value=SubagentResult(
                success=True,
                summary="Â≠ê‰ª£ÁêÜÊëòË¶Å",
                subagent_name="explorer",
                permission_mode="default",
                conversation_id="sub_1",
            )
        )
        engine._active_skill = Skillpack(
            name="hook/agent",
            description="agent hook",
            allowed_tools=["add_numbers"],
            triggers=[],
            instructions="",
            source="project",
            root_dir="/tmp/hook",
            hooks={
                "PreToolUse": [
                    {
                        "matcher": "add_numbers",
                        "hooks": [
                            {
                                "type": "agent",
                                "agent_name": "explorer",
                                "task": "ËØ∑Ê£ÄÊü•Ë∞ÉÁî®ÂèÇÊï∞",
                                "inject_summary_as_context": True,
                            }
                        ],
                    }
                ]
            },
        )

        tc = SimpleNamespace(
            id="call_hook_agent",
            function=SimpleNamespace(
                name="add_numbers",
                arguments=json.dumps({"a": 1, "b": 2}),
            ),
        )
        result = await engine._execute_tool_call(
            tc=tc,
            tool_scope=["add_numbers"],
            on_event=None,
            iteration=1,
        )
        assert result.success is True
        assert result.result == "3"
        engine.run_subagent.assert_awaited_once()
        assert any("Â≠ê‰ª£ÁêÜÊëòË¶Å" in item for item in engine._transient_hook_contexts)

    @pytest.mark.asyncio
    async def test_agent_hook_recursion_guard_respects_on_failure_deny(self) -> None:
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)
        engine._hook_agent_action_depth = 1

        resolved = await engine._resolve_hook_result(
            event=HookEvent.PRE_TOOL_USE,
            hook_result=HookResult(
                decision=HookDecision.CONTINUE,
                agent_action=HookAgentAction(task="ÈÄíÂΩíÊµãËØï", on_failure="deny"),
            ),
            on_event=None,
        )
        assert resolved is not None
        assert resolved.decision == HookDecision.DENY
        assert "ÈÄíÂΩíËß¶Âèë" in resolved.reason


class TestChatPureText:
    """Á∫ØÊñáÊú¨ÂõûÂ§çÂú∫ÊôØÔºàRequirement 1.3Ôºâ„ÄÇ"""

    @pytest.mark.asyncio
    async def test_returns_text_when_no_tool_calls(self) -> None:
        """LLM ËøîÂõûÁ∫ØÊñáÊú¨Êó∂ÔºåÁõ¥Êé•ËøîÂõûËØ•ÊñáÊú¨„ÄÇ"""
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        mock_response = _make_text_response("‰Ω†Â•ΩÔºåËøôÊòØÂõûÂ§ç„ÄÇ")
        engine._client.chat.completions.create = AsyncMock(
            return_value=mock_response
        )

        result = await engine.chat("‰Ω†Â•Ω")
        assert isinstance(result, ChatResult)
        assert result == "‰Ω†Â•ΩÔºåËøôÊòØÂõûÂ§ç„ÄÇ"
        assert result.reply == "‰Ω†Â•ΩÔºåËøôÊòØÂõûÂ§ç„ÄÇ"
        assert result.iterations == 1
        assert result.truncated is False
        assert result.tool_calls == []

    @pytest.mark.asyncio
    async def test_empty_content_returns_empty_string(self) -> None:
        """LLM ËøîÂõû content=None Êó∂ÔºåËøîÂõûÁ©∫Â≠óÁ¨¶‰∏≤„ÄÇ"""
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        mock_response = _make_text_response("")
        # Ê®°Êãü content ‰∏∫ None
        mock_response.choices[0].message.content = None
        engine._client.chat.completions.create = AsyncMock(
            return_value=mock_response
        )

        result = await engine.chat("ÊµãËØï")
        assert result == ""

    @pytest.mark.asyncio
    async def test_string_response_is_treated_as_text_reply(self) -> None:
        """ÂÖºÂÆπÊüê‰∫õÁΩëÂÖ≥Áõ¥Êé•ËøîÂõûÁ∫ØÂ≠óÁ¨¶‰∏≤„ÄÇ"""
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        engine._client.chat.completions.create = AsyncMock(return_value="‰Ω†Â•ΩÔºåÂ≠óÁ¨¶‰∏≤ÂìçÂ∫î„ÄÇ")

        result = await engine.chat("‰Ω†Â•Ω")
        assert isinstance(result, ChatResult)
        assert result.reply == "‰Ω†Â•ΩÔºåÂ≠óÁ¨¶‰∏≤ÂìçÂ∫î„ÄÇ"
        assert result.tool_calls == []
        assert result.iterations == 1
        assert result.truncated is False

    @pytest.mark.asyncio
    async def test_html_document_response_returns_endpoint_hint(self) -> None:
        """ÂΩì‰∏äÊ∏∏ËøîÂõû HTML È°µÈù¢Êó∂ÔºåËøîÂõûÂèØÊìç‰ΩúÁöÑÈÖçÁΩÆÊèêÁ§∫„ÄÇ"""
        config = _make_config(base_url="https://example.invalid/")
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        engine._client.chat.completions.create = AsyncMock(
            return_value="<!doctype html><html><head><meta charset='utf-8'></head><body>oops</body></html>"
        )

        result = await engine.chat("‰Ω†ÊòØË∞Å")
        assert "EXCELMANUS_BASE_URL" in result.reply
        assert "/v1" in result.reply
        assert "<!doctype html>" not in result.reply.lower()


class TestChatToolCalling:
    """Tool Calling Âæ™ÁéØÂú∫ÊôØÔºàRequirements 1.1, 1.2, 1.9Ôºâ„ÄÇ"""

    @pytest.mark.asyncio
    async def test_single_tool_call_then_text(self) -> None:
        """Âçï‰∏™ tool_call ÊâßË°åÂêéÔºåLLM ËøîÂõûÊñáÊú¨ÁªìÊùüÂæ™ÁéØ„ÄÇ"""
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        # Á¨¨‰∏ÄËΩÆÔºöLLM ËøîÂõû tool_call
        tool_response = _make_tool_call_response(
            [("call_1", "add_numbers", json.dumps({"a": 3, "b": 5}))]
        )
        # Á¨¨‰∫åËΩÆÔºöLLM ËøîÂõûÁ∫ØÊñáÊú¨
        text_response = _make_text_response("3 + 5 = 8")

        engine._client.chat.completions.create = AsyncMock(
            side_effect=[tool_response, text_response]
        )

        result = await engine.chat("ËÆ°ÁÆó 3 + 5")
        assert isinstance(result, ChatResult)
        assert result == "3 + 5 = 8"
        assert result.reply == "3 + 5 = 8"
        assert result.iterations == 2
        assert result.truncated is False
        assert len(result.tool_calls) == 1
        assert result.tool_calls[0].tool_name == "add_numbers"
        assert result.tool_calls[0].success is True

    @pytest.mark.asyncio
    async def test_multiple_tool_calls_in_single_response(self) -> None:
        """ÂçïËΩÆÂìçÂ∫îÂåÖÂê´Â§ö‰∏™ tool_callsÔºàRequirement 1.9Ôºâ„ÄÇ"""
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        # Á¨¨‰∏ÄËΩÆÔºöLLM ËøîÂõû‰∏§‰∏™ tool_calls
        tool_response = _make_tool_call_response([
            ("call_1", "add_numbers", json.dumps({"a": 1, "b": 2})),
            ("call_2", "add_numbers", json.dumps({"a": 3, "b": 4})),
        ])
        # Á¨¨‰∫åËΩÆÔºöLLM ËøîÂõûÁ∫ØÊñáÊú¨
        text_response = _make_text_response("ÁªìÊûúÂàÜÂà´ÊòØ 3 Âíå 7")

        engine._client.chat.completions.create = AsyncMock(
            side_effect=[tool_response, text_response]
        )

        result = await engine.chat("ÂàÜÂà´ËÆ°ÁÆó 1+2 Âíå 3+4")
        assert result == "ÁªìÊûúÂàÜÂà´ÊòØ 3 Âíå 7"

    @pytest.mark.asyncio
    async def test_tool_result_fed_back_to_memory(self) -> None:
        """Â∑•ÂÖ∑ÊâßË°åÁªìÊûúË¢´Ê≠£Á°ÆÂõûÂ°´Âà∞ÂØπËØùËÆ∞ÂøÜ„ÄÇ"""
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        tool_response = _make_tool_call_response(
            [("call_1", "add_numbers", json.dumps({"a": 10, "b": 20}))]
        )
        text_response = _make_text_response("ÁªìÊûúÊòØ 30")

        engine._client.chat.completions.create = AsyncMock(
            side_effect=[tool_response, text_response]
        )

        await engine.chat("ËÆ°ÁÆó 10 + 20")

        # Ê£ÄÊü•ËÆ∞ÂøÜ‰∏≠ÂåÖÂê´ tool result Ê∂àÊÅØ
        messages = engine.memory.get_messages()
        tool_msgs = [m for m in messages if m.get("role") == "tool"]
        assert len(tool_msgs) == 1
        assert "30" in tool_msgs[0]["content"]

    @pytest.mark.asyncio
    async def test_preserves_assistant_extra_fields_for_tool_message(self) -> None:
        """assistant tool Ê∂àÊÅØÂ∫î‰øùÁïôÊâ©Â±ïÂ≠óÊÆµÔºà‰æõÂ∫îÂïÜÂÖºÂÆπÔºâ„ÄÇ"""
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        message = SimpleNamespace(
            content=None,
            reasoning_content="internal-thought",
            tool_calls=[
                SimpleNamespace(
                    id="call_1",
                    function=SimpleNamespace(
                        name="add_numbers",
                        arguments=json.dumps({"a": 1, "b": 2}),
                    ),
                )
            ],
        )
        tool_response = SimpleNamespace(choices=[SimpleNamespace(message=message)])
        text_response = _make_text_response("done")

        engine._client.chat.completions.create = AsyncMock(
            side_effect=[tool_response, text_response]
        )

        result = await engine.chat("ËÆ°ÁÆó")
        assert result == "done"

        msgs = engine.memory.get_messages()
        assistant_with_tool = [m for m in msgs if m.get("tool_calls")]
        assert len(assistant_with_tool) == 1
        assert assistant_with_tool[0].get("reasoning_content") == "internal-thought"


class TestChatToolError:
    """Â∑•ÂÖ∑ÂºÇÂ∏∏Â§ÑÁêÜÂú∫ÊôØÔºàRequirement 1.5Ôºâ„ÄÇ"""

    @pytest.mark.asyncio
    async def test_tool_error_fed_back_as_tool_message(self) -> None:
        """Â∑•ÂÖ∑ÊâßË°åÂºÇÂ∏∏Ë¢´ÊçïËé∑Âπ∂‰Ωú‰∏∫ tool message ÂèçÈ¶àÁªô LLM„ÄÇ"""
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        # Á¨¨‰∏ÄËΩÆÔºöË∞ÉÁî®‰ºöÂ§±Ë¥•ÁöÑÂ∑•ÂÖ∑
        tool_response = _make_tool_call_response(
            [("call_1", "fail_tool", "{}")]
        )
        # Á¨¨‰∫åËΩÆÔºöLLM Êî∂Âà∞ÈîôËØØÂêéËøîÂõûÊñáÊú¨
        text_response = _make_text_response("Â∑•ÂÖ∑ÊâßË°åÂá∫Èîô‰∫ÜÔºåËØ∑Ê£ÄÊü•„ÄÇ")

        engine._client.chat.completions.create = AsyncMock(
            side_effect=[tool_response, text_response]
        )

        result = await engine.chat("ÊâßË°åÂ§±Ë¥•Â∑•ÂÖ∑")
        assert "Â∑•ÂÖ∑ÊâßË°åÂá∫Èîô" in result or "Ê£ÄÊü•" in result

        # È™åËØÅÈîôËØØ‰ø°ÊÅØË¢´ÂõûÂ°´Âà∞ËÆ∞ÂøÜ
        messages = engine.memory.get_messages()
        tool_msgs = [m for m in messages if m.get("role") == "tool"]
        assert len(tool_msgs) == 1
        assert "ÈîôËØØ" in tool_msgs[0]["content"]

    @pytest.mark.asyncio
    async def test_malformed_arguments_should_not_execute_tool(self) -> None:
        """ÂèÇÊï∞ JSON ÈùûÊ≥ïÊó∂‰∏çÂ∫îÊâßË°åÂ∑•ÂÖ∑ÂáΩÊï∞„ÄÇ"""
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        bad_args_response = _make_tool_call_response(
            [("call_1", "add_numbers", '{"a": 1')]
        )
        text_response = _make_text_response("Â∑≤Â§ÑÁêÜ")
        engine._client.chat.completions.create = AsyncMock(
            side_effect=[bad_args_response, text_response]
        )

        with patch(
            "excelmanus.engine.asyncio.to_thread", new_callable=AsyncMock
        ) as mock_to_thread:
            result = await engine.chat("ÂùèÂèÇÊï∞ÊµãËØï")
            assert result == "Â∑≤Â§ÑÁêÜ"
            # ÂèÇÊï∞Ëß£ÊûêÂ§±Ë¥•Âêé‰∏çÂ∫îÊâßË°åÂ∑•ÂÖ∑
            mock_to_thread.assert_not_called()

        msgs = engine.memory.get_messages()
        tool_msgs = [m for m in msgs if m.get("role") == "tool"]
        assert len(tool_msgs) == 1
        assert "ÂèÇÊï∞Ëß£ÊûêÈîôËØØ" in tool_msgs[0]["content"]


class TestConsecutiveFailureCircuitBreaker:
    """ËøûÁª≠Â§±Ë¥•ÁÜîÊñ≠Âú∫ÊôØÔºàRequirement 1.6Ôºâ„ÄÇ"""

    @pytest.mark.asyncio
    async def test_circuit_breaker_after_consecutive_failures(self) -> None:
        """ËøûÁª≠ 3 Ê¨°Â∑•ÂÖ∑Â§±Ë¥•ÂêéÔºåÁÜîÊñ≠ÁªàÊ≠¢Âπ∂ËøîÂõûÈîôËØØÊëòË¶Å„ÄÇ"""
        config = _make_config(max_consecutive_failures=3)
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        # ÊûÑÈÄ† 3 ËΩÆËøûÁª≠Â§±Ë¥•ÁöÑ tool_call ÂìçÂ∫î
        fail_responses = [
            _make_tool_call_response([
                (f"call_{i}", "fail_tool", "{}")
            ])
            for i in range(1, 4)
        ]

        engine._client.chat.completions.create = AsyncMock(
            side_effect=fail_responses
        )

        result = await engine.chat("ËøûÁª≠Â§±Ë¥•ÊµãËØï")
        assert "ËøûÁª≠" in result
        assert "Â§±Ë¥•" in result

    @pytest.mark.asyncio
    async def test_success_resets_failure_counter(self) -> None:
        """ÊàêÂäüÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®ÈáçÁΩÆËøûÁª≠Â§±Ë¥•ËÆ°Êï∞„ÄÇ"""
        config = _make_config(max_consecutive_failures=3)
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        # Á¨¨‰∏ÄËΩÆÔºöÂ§±Ë¥•
        fail_resp_1 = _make_tool_call_response([("c1", "fail_tool", "{}")])
        # Á¨¨‰∫åËΩÆÔºöÊàêÂäüÔºàÈáçÁΩÆËÆ°Êï∞Ôºâ
        success_resp = _make_tool_call_response(
            [("c2", "add_numbers", json.dumps({"a": 1, "b": 1}))]
        )
        # Á¨¨‰∏âËΩÆÔºöÂ§±Ë¥•
        fail_resp_2 = _make_tool_call_response([("c3", "fail_tool", "{}")])
        # Á¨¨ÂõõËΩÆÔºöÁ∫ØÊñáÊú¨ÁªìÊùü
        text_resp = _make_text_response("ÂÆåÊàê")

        engine._client.chat.completions.create = AsyncMock(
            side_effect=[fail_resp_1, success_resp, fail_resp_2, text_resp]
        )

        result = await engine.chat("Ê∑∑ÂêàÊàêÂäüÂ§±Ë¥•")
        # ‰∏çÂ∫îËß¶ÂèëÁÜîÊñ≠ÔºåÂ∫îÊ≠£Â∏∏ËøîÂõûÊñáÊú¨
        assert result == "ÂÆåÊàê"

    @pytest.mark.asyncio
    async def test_circuit_breaker_keeps_tool_call_result_pairs(self) -> None:
        """ÂçïËΩÆÂ§ö tool_calls ÁÜîÊñ≠ÂêéÔºå‰πüÂ∫î‰∏∫ÊØè‰∏™ tool_call ÂõûÂ°´ÁªìÊûú„ÄÇ"""
        config = _make_config(max_consecutive_failures=1)
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        # ÂçïËΩÆ‰∏§‰∏™Â§±Ë¥•Ë∞ÉÁî®ÔºõÁ¨¨‰∏ÄÊ¨°Â§±Ë¥•Âç≥Ëß¶ÂèëÁÜîÊñ≠
        tool_response = _make_tool_call_response(
            [
                ("call_1", "fail_tool", "{}"),
                ("call_2", "fail_tool", "{}"),
            ]
        )
        engine._client.chat.completions.create = AsyncMock(
            side_effect=[tool_response]
        )

        result = await engine.chat("Ëß¶ÂèëÁÜîÊñ≠")
        assert "ÁªàÊ≠¢ÊâßË°å" in result

        msgs = engine.memory.get_messages()
        tool_results = [m for m in msgs if m.get("role") == "tool"]
        assert {m["tool_call_id"] for m in tool_results} == {"call_1", "call_2"}


class TestIterationLimit:
    """Ëø≠‰ª£‰∏äÈôê‰øùÊä§Âú∫ÊôØÔºàRequirement 1.4Ôºâ„ÄÇ"""

    @pytest.mark.asyncio
    async def test_truncates_at_max_iterations(self) -> None:
        """ËææÂà∞Ëø≠‰ª£‰∏äÈôêÊó∂Êà™Êñ≠ËøîÂõû„ÄÇ"""
        config = _make_config(max_iterations=3)
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        # ÊØèËΩÆÈÉΩËøîÂõû tool_callÔºåÊ∞∏‰∏çËøîÂõûÁ∫ØÊñáÊú¨
        tool_responses = [
            _make_tool_call_response(
                [(f"call_{i}", "add_numbers", json.dumps({"a": i, "b": i}))]
            )
            for i in range(1, 5)  # Â§öÂáÜÂ§áÂá†‰∏™
        ]

        engine._client.chat.completions.create = AsyncMock(
            side_effect=tool_responses
        )

        result = await engine.chat("Êó†ÈôêÂæ™ÁéØÊµãËØï")
        assert "ÊúÄÂ§ßËø≠‰ª£Ê¨°Êï∞" in result or "3" in result
        assert result.truncated is True
        assert result.iterations == 3


class TestAsyncToolExecution:
    """ÂºÇÊ≠•Â∑•ÂÖ∑ÊâßË°åÂú∫ÊôØÔºàRequirement 1.10Ôºâ„ÄÇ"""

    @pytest.mark.asyncio
    async def test_blocking_tool_runs_in_thread(self) -> None:
        """ÈòªÂ°ûÂûãÂ∑•ÂÖ∑ÈÄöËøá asyncio.to_thread ÈöîÁ¶ªÊâßË°å„ÄÇ"""
        config = _make_config()
        registry = _make_registry_with_tools()
        engine = AgentEngine(config, registry)

        tool_response = _make_tool_call_response(
            [("call_1", "add_numbers", json.dumps({"a": 5, "b": 10}))]
        )
        text_response = _make_text_response("ÁªìÊûúÊòØ 15")

        engine._client.chat.completions.create = AsyncMock(
            side_effect=[tool_response, text_response]
        )

        # ‰ΩøÁî® patch È™åËØÅ asyncio.to_thread Ë¢´Ë∞ÉÁî®
        with patch("excelmanus.engine.asyncio.to_thread", new_callable=AsyncMock) as mock_to_thread:
            mock_to_thread.return_value = 15
            result = await engine.chat("ËÆ°ÁÆó 5 + 10")

            # È™åËØÅ to_thread Ë¢´Ë∞ÉÁî®
            mock_to_thread.assert_called_once()
            call_args = mock_to_thread.call_args
            # ÂΩìÂâçÂÆûÁé∞‰ΩøÁî®Èó≠ÂåÖÂ∞ÅË£Ö registry.call_toolÔºåÂÜçÊèê‰∫§Áªô to_thread„ÄÇ
            assert len(call_args.args) == 1
            assert callable(call_args.args[0])
            assert result == "ÁªìÊûúÊòØ 15"


class TestClearMemory:
    """Ê∏ÖÈô§ËÆ∞ÂøÜÊµãËØï„ÄÇ"""

    def test_clear_memory(self) -> None:
        """clear_memory Ê∏ÖÈô§ÂØπËØùÂéÜÂè≤„ÄÇ"""
        config = _make_config()
        registry = ToolRegistry()
        engine = AgentEngine(config, registry)

        engine.memory.add_user_message("ÊµãËØïÊ∂àÊÅØ")
        assert len(engine.memory.get_messages()) > 1  # system + user

        engine.clear_memory()
        # Ê∏ÖÈô§ÂêéÂè™Ââ© system prompt
        assert len(engine.memory.get_messages()) == 1
        assert engine.memory.get_messages()[0]["role"] == "system"


class TestDataModels:
    """Êï∞ÊçÆÊ®°ÂûãÊµãËØï„ÄÇ"""

    def test_tool_call_result_defaults(self) -> None:
        """ToolCallResult ÈªòËÆ§ÂÄºÊ≠£Á°Æ„ÄÇ"""
        r = ToolCallResult(
            tool_name="test", arguments={}, result="ok", success=True
        )
        assert r.error is None
        assert r.success is True

    def test_chat_result_defaults(self) -> None:
        """ChatResult ÈªòËÆ§ÂÄºÊ≠£Á°Æ„ÄÇ"""
        r = ChatResult(reply="hello")
        assert r.tool_calls == []
        assert r.iterations == 0
        assert r.truncated is False


# ‚îÄ‚îÄ Â±ûÊÄßÊµãËØïÔºàProperty-Based TestsÔºâ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‰ΩøÁî® hypothesis Ê°ÜÊû∂ÔºåÊØèÈ°πËá≥Â∞ë 100 Ê¨°Ëø≠‰ª£

import string
from hypothesis import given, assume
from hypothesis import strategies as st


# ‚îÄ‚îÄ ËæÖÂä©Á≠ñÁï• ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# ÁîüÊàêÂêàÊ≥ïÁöÑÂ∑•ÂÖ∑ÂêçÁß∞
tool_name_st = st.from_regex(r"[a-z][a-z0-9_]{2,20}", fullmatch=True)

# ÁîüÊàêÈùûÁ©∫ÊñáÊú¨ÂÜÖÂÆπ
nonempty_text_st = st.text(
    alphabet=st.characters(whitelist_categories=("L", "N", "P", "Z")),
    min_size=1,
    max_size=200,
)

# ÁîüÊàê tool_call_id
tool_call_id_st = st.from_regex(r"call_[a-z0-9]{4,10}", fullmatch=True)


# ---------------------------------------------------------------------------
# Property 1ÔºöÊ∂àÊÅØÊûÑÂª∫ÂÆåÊï¥ÊÄß
# **Validates: Requirements 1.1, 1.7**
# ---------------------------------------------------------------------------


@given(
    history=st.lists(
        st.tuples(
            st.sampled_from(["user", "assistant"]),
            nonempty_text_st,
        ),
        min_size=0,
        max_size=10,
    ),
    new_input=nonempty_text_st,
)
def test_property_1_message_construction_completeness(
    history: list[tuple[str, str]],
    new_input: str,
) -> None:
    """Property 1ÔºöÊ∂àÊÅØÊûÑÂª∫ÂÆåÊï¥ÊÄß„ÄÇ

    ÂØπ‰∫é‰ªªÊÑèÂéÜÂè≤‰∏éÊñ∞ËæìÂÖ•ÔºåÊûÑÂª∫Âá∫ÁöÑÊ∂àÊÅØÂ∫èÂàóÂøÖÈ°ª‰øùÊåÅÔºö
    - system Âú®È¶ñ‰Ωç
    - ÂéÜÂè≤ÊúâÂ∫è
    - Êñ∞Áî®Êà∑Ê∂àÊÅØÂú®Êú´‰Ωç

    **Validates: Requirements 1.1, 1.7**
    """
    from excelmanus.memory import ConversationMemory, _DEFAULT_SYSTEM_PROMPT

    config = _make_config()
    mem = ConversationMemory(config)

    # Â°´ÂÖÖÂéÜÂè≤Ê∂àÊÅØ
    for role, content in history:
        if role == "user":
            mem.add_user_message(content)
        else:
            mem.add_assistant_message(content)

    # Ê∑ªÂä†Êñ∞Áî®Êà∑Ê∂àÊÅØ
    mem.add_user_message(new_input)

    messages = mem.get_messages()

    # ‰∏çÂèòÈáè 1Ôºösystem Ê∂àÊÅØÂú®È¶ñ‰Ωç
    assert messages[0]["role"] == "system"
    assert messages[0]["content"] == _DEFAULT_SYSTEM_PROMPT

    # ‰∏çÂèòÈáè 2ÔºöÊúÄÂêé‰∏ÄÊù°Ê∂àÊÅØÊòØÊñ∞Áî®Êà∑ËæìÂÖ•
    assert messages[-1]["role"] == "user"
    assert messages[-1]["content"] == new_input

    # ‰∏çÂèòÈáè 3ÔºöÊ∂àÊÅØÂ∫èÂàóÈïøÂ∫¶ = 1(system) + ÂéÜÂè≤Êù°Êï∞ + 1(Êñ∞ËæìÂÖ•)
    # Ê≥®ÊÑèÔºöÊà™Êñ≠ÂèØËÉΩÂáèÂ∞ëÂéÜÂè≤Êù°Êï∞Ôºå‰ΩÜ system ÂíåÊúÄÂêé‰∏ÄÊù°ÂßãÁªà‰øùÁïô
    assert len(messages) >= 2  # Ëá≥Â∞ë system + Êñ∞ËæìÂÖ•

    # ‰∏çÂèòÈáè 4ÔºöÊâÄÊúâÊ∂àÊÅØ role ÂêàÊ≥ï
    valid_roles = {"system", "user", "assistant", "tool"}
    for m in messages:
        assert m["role"] in valid_roles


@given(
    n_tools=st.integers(min_value=1, max_value=5),
)
def test_property_1_tools_schema_attached(n_tools: int) -> None:
    """Property 1 Ë°•ÂÖÖÔºöEngine ÊûÑÂª∫ËØ∑Ê±ÇÊó∂ÈôÑÂÖ®Èáè tools schema„ÄÇ

    **Validates: Requirements 1.1, 1.7**
    """
    registry = ToolRegistry()
    tools = []
    for i in range(n_tools):
        tools.append(
            ToolDef(
                name=f"tool_{i}",
                description=f"ÊµãËØïÂ∑•ÂÖ∑ {i}",
                input_schema={"type": "object", "properties": {}},
                func=lambda: "ok",
            )
        )
    registry.register_tools(tools)

    schemas = registry.get_openai_schemas()

    # ‰∏çÂèòÈáèÔºöschema Êï∞ÈáèÁ≠â‰∫éÊ≥®ÂÜåÁöÑÂ∑•ÂÖ∑Êï∞Èáè
    assert len(schemas) == n_tools

    # ‰∏çÂèòÈáèÔºöÊØè‰∏™ schema ÂåÖÂê´ÂøÖË¶ÅÂ≠óÊÆµ
    for s in schemas:
        assert s["type"] == "function"
        # ÂÖºÂÆπ‰∏§ÁßçÊ†ºÂºèÔºöÊâÅÂπ≥ÁªìÊûÑÊàñÂµåÂ•ó function ÁªìÊûÑ
        if "function" in s:
            assert "name" in s["function"]
            assert "description" in s["function"]
            assert "parameters" in s["function"]
        else:
            assert "name" in s
            assert "description" in s
            assert "parameters" in s


# ---------------------------------------------------------------------------
# Property 2ÔºöTool Call Ëß£Êûê‰∏éË∞ÉÁî®
# **Validates: Requirements 1.2**
# ---------------------------------------------------------------------------


@given(
    n_calls=st.integers(min_value=1, max_value=4),
    a_values=st.lists(st.integers(min_value=0, max_value=100), min_size=4, max_size=4),
    b_values=st.lists(st.integers(min_value=0, max_value=100), min_size=4, max_size=4),
)
@pytest.mark.asyncio
async def test_property_2_tool_call_parsing_and_invocation(
    n_calls: int,
    a_values: list[int],
    b_values: list[int],
) -> None:
    """Property 2ÔºöTool Call Ëß£Êûê‰∏éË∞ÉÁî®„ÄÇ

    ÂØπ‰∫é‰ªªÊÑèÂåÖÂê´ tool_calls ÁöÑÂìçÂ∫îÔºåEngine ÂøÖÈ°ªÊ≠£Á°ÆËß£ÊûêÂπ∂ÈÄê‰∏™Ë∞ÉÁî®Â∑•ÂÖ∑Ôºå
    ‰∏î tool_call_id ÂØπÂ∫î‰∏ÄËá¥„ÄÇ

    **Validates: Requirements 1.2**
    """
    config = _make_config()
    registry = _make_registry_with_tools()
    engine = AgentEngine(config, registry)

    # ÊûÑÈÄ† n_calls ‰∏™ tool_calls
    tc_list = []
    expected_results = []
    for i in range(n_calls):
        call_id = f"call_{i}"
        a, b = a_values[i], b_values[i]
        tc_list.append((call_id, "add_numbers", json.dumps({"a": a, "b": b})))
        expected_results.append((call_id, str(a + b)))

    tool_response = _make_tool_call_response(tc_list)
    text_response = _make_text_response("ÂÆåÊàê")

    engine._client.chat.completions.create = AsyncMock(
        side_effect=[tool_response, text_response]
    )

    result = await engine.chat("ÊµãËØïÂ§öÂ∑•ÂÖ∑Ë∞ÉÁî®")

    # ‰∏çÂèòÈáè 1ÔºöËøîÂõûÁ∫ØÊñáÊú¨ÁªìÊûú
    assert result == "ÂÆåÊàê"

    # ‰∏çÂèòÈáè 2ÔºöËÆ∞ÂøÜ‰∏≠ÂåÖÂê´Ê≠£Á°ÆÊï∞ÈáèÁöÑ tool result Ê∂àÊÅØ
    messages = engine.memory.get_messages()
    tool_msgs = [m for m in messages if m.get("role") == "tool"]
    assert len(tool_msgs) == n_calls

    # ‰∏çÂèòÈáè 3ÔºöÊØè‰∏™ tool_call_id ÈÉΩÊúâÂØπÂ∫îÁöÑ tool result
    for call_id, expected_val in expected_results:
        matching = [m for m in tool_msgs if m.get("tool_call_id") == call_id]
        assert len(matching) == 1, f"tool_call_id {call_id} Â∫îÊúâ‰∏î‰ªÖÊúâ‰∏Ä‰∏™ÂØπÂ∫îÁªìÊûú"
        assert expected_val in matching[0]["content"]


# ---------------------------------------------------------------------------
# Property 3ÔºöÁ∫ØÊñáÊú¨ÁªàÊ≠¢Âæ™ÁéØ
# **Validates: Requirements 1.3**
# ---------------------------------------------------------------------------


@given(
    reply_text=nonempty_text_st,
)
@pytest.mark.asyncio
async def test_property_3_pure_text_terminates_loop(reply_text: str) -> None:
    """Property 3ÔºöÁ∫ØÊñáÊú¨ÁªàÊ≠¢Âæ™ÁéØ„ÄÇ

    ÂØπ‰∫é‰ªªÊÑè‰∏çÂê´ tool_calls ÁöÑÂìçÂ∫îÔºåEngine ÂøÖÈ°ªÁ´ãÂç≥ÁªàÊ≠¢Âæ™ÁéØÂπ∂ËøîÂõûÊñáÊú¨„ÄÇ

    **Validates: Requirements 1.3**
    """
    config = _make_config()
    registry = _make_registry_with_tools()
    engine = AgentEngine(config, registry)

    mock_response = _make_text_response(reply_text)
    engine._client.chat.completions.create = AsyncMock(return_value=mock_response)

    result = await engine.chat("‰ªªÊÑèËæìÂÖ•")

    # ‰∏çÂèòÈáè 1ÔºöËøîÂõûÁöÑÊñáÊú¨‰∏é LLM ÂìçÂ∫î‰∏ÄËá¥
    assert result == reply_text

    # ‰∏çÂèòÈáè 2ÔºöLLM Âè™Ë¢´Ë∞ÉÁî®‰∫Ü‰∏ÄÊ¨°ÔºàÁ´ãÂç≥ÁªàÊ≠¢ÔºåÊó†Âæ™ÁéØÔºâ
    assert engine._client.chat.completions.create.call_count == 1

    # ‰∏çÂèòÈáè 3ÔºöËÆ∞ÂøÜ‰∏≠ÂåÖÂê´ user + assistant Ê∂àÊÅØ
    messages = engine.memory.get_messages()
    roles = [m["role"] for m in messages]
    assert roles[0] == "system"
    assert "user" in roles
    assert "assistant" in roles
    # ‰∏çÂ∫îÊúâ tool Ê∂àÊÅØ
    assert "tool" not in roles


# ---------------------------------------------------------------------------
# Property 4ÔºöËø≠‰ª£‰∏äÈôê‰øùÊä§
# **Validates: Requirements 1.4**
# ---------------------------------------------------------------------------


@given(
    max_iter=st.integers(min_value=1, max_value=10),
)
@pytest.mark.asyncio
async def test_property_4_iteration_limit_protection(max_iter: int) -> None:
    """Property 4ÔºöËø≠‰ª£‰∏äÈôê‰øùÊä§„ÄÇ

    ÂΩìËøûÁª≠ N ËΩÆÂùáÈúÄË¶ÅÂ∑•ÂÖ∑Ë∞ÉÁî®Êó∂ÔºåEngine Âú®Á¨¨ N ËΩÆÂêéÂøÖÈ°ªÁªàÊ≠¢„ÄÇ

    **Validates: Requirements 1.4**
    """
    config = _make_config(max_iterations=max_iter)
    registry = _make_registry_with_tools()
    engine = AgentEngine(config, registry)

    # ÊûÑÈÄ†Êó†Èôê tool_call ÂìçÂ∫îÔºàÊØèËΩÆÈÉΩËøîÂõû tool_callÔºåÊ∞∏‰∏çËøîÂõûÁ∫ØÊñáÊú¨Ôºâ
    infinite_tool_responses = [
        _make_tool_call_response(
            [(f"call_{i}", "add_numbers", json.dumps({"a": i, "b": i}))]
        )
        for i in range(max_iter + 5)  # Â§öÂáÜÂ§áÂá†‰∏™
    ]

    engine._client.chat.completions.create = AsyncMock(
        side_effect=infinite_tool_responses
    )

    result = await engine.chat("Êó†ÈôêÂæ™ÁéØÊµãËØï")

    # ‰∏çÂèòÈáè 1ÔºöLLM Ë¢´Ë∞ÉÁî®ÁöÑÊ¨°Êï∞‰∏çË∂ÖËøá max_iter
    assert engine._client.chat.completions.create.call_count <= max_iter

    # ‰∏çÂèòÈáè 2ÔºöËøîÂõûÁªìÊûúÂåÖÂê´Ëø≠‰ª£‰∏äÈôêÊèêÁ§∫
    assert "ÊúÄÂ§ßËø≠‰ª£Ê¨°Êï∞" in result or str(max_iter) in result


# ---------------------------------------------------------------------------
# Property 5ÔºöÂ∑•ÂÖ∑ÂºÇÂ∏∏ÂèçÈ¶à
# **Validates: Requirements 1.5**
# ---------------------------------------------------------------------------


@given(
    error_msg=st.text(
        alphabet=st.characters(whitelist_categories=("L", "N")),
        min_size=1,
        max_size=100,
    ),
)
@pytest.mark.asyncio
async def test_property_5_tool_exception_feedback(error_msg: str) -> None:
    """Property 5ÔºöÂ∑•ÂÖ∑ÂºÇÂ∏∏ÂèçÈ¶à„ÄÇ

    ‰ªªÊÑèÂ∑•ÂÖ∑ÂºÇÂ∏∏ÂøÖÈ°ªË¢´ÊçïËé∑Âπ∂‰Ωú‰∏∫ tool message ÂèçÈ¶àÁªô LLMÔºå‰∏çÁõ¥Êé•ÂêëË∞ÉÁî®ÊñπÊäõÂá∫„ÄÇ

    **Validates: Requirements 1.5**
    """
    # ÂàõÂª∫‰∏Ä‰∏™‰ºöÊäõÂá∫ÊåáÂÆöÂºÇÂ∏∏ÁöÑÂ∑•ÂÖ∑
    def failing_tool() -> str:
        raise RuntimeError(error_msg)

    registry = ToolRegistry()
    registry.register_tools([
        ToolDef(
            name="custom_fail",
            description="Ëá™ÂÆö‰πâÂ§±Ë¥•Â∑•ÂÖ∑",
            input_schema={"type": "object", "properties": {}},
            func=failing_tool,
        ),
    ])

    config = _make_config(max_consecutive_failures=10)  # È´òÈòàÂÄºÈÅøÂÖçÁÜîÊñ≠
    engine = AgentEngine(config, registry)
    engine._full_access_enabled = True

    # Á¨¨‰∏ÄËΩÆÔºöË∞ÉÁî®‰ºöÂ§±Ë¥•ÁöÑÂ∑•ÂÖ∑
    tool_response = _make_tool_call_response([("call_err", "custom_fail", "{}")])
    # Á¨¨‰∫åËΩÆÔºöLLM ËøîÂõûÁ∫ØÊñáÊú¨
    text_response = _make_text_response("Â∑≤Â§ÑÁêÜÈîôËØØ")

    engine._client.chat.completions.create = AsyncMock(
        side_effect=[tool_response, text_response]
    )

    # ‰∏çÂèòÈáè 1Ôºöchat ‰∏çÂ∫îÊäõÂá∫ÂºÇÂ∏∏ÔºàÂºÇÂ∏∏Ë¢´ÂÜÖÈÉ®ÊçïËé∑Ôºâ
    result = await engine.chat("ÊµãËØïÂºÇÂ∏∏ÂèçÈ¶à")

    # ‰∏çÂèòÈáè 2ÔºöËøîÂõûÊ≠£Â∏∏ÊñáÊú¨
    assert result == "Â∑≤Â§ÑÁêÜÈîôËØØ"

    # ‰∏çÂèòÈáè 3ÔºöËÆ∞ÂøÜ‰∏≠ÂåÖÂê´ tool result Ê∂àÊÅØÔºå‰∏îÂÜÖÂÆπÂåÖÂê´ÈîôËØØ‰ø°ÊÅØ
    messages = engine.memory.get_messages()
    tool_msgs = [m for m in messages if m.get("role") == "tool"]
    assert len(tool_msgs) >= 1
    # ÈîôËØØ‰ø°ÊÅØË¢´ÂèçÈ¶àÂà∞ tool message ‰∏≠
    assert any("ÈîôËØØ" in m["content"] or "error" in m["content"].lower() for m in tool_msgs)


# ---------------------------------------------------------------------------
# Property 6ÔºöËøûÁª≠Â§±Ë¥•ÁÜîÊñ≠
# **Validates: Requirements 1.6**
# ---------------------------------------------------------------------------


@given(
    max_failures=st.integers(min_value=1, max_value=5),
)
@pytest.mark.asyncio
async def test_property_6_consecutive_failure_circuit_breaker(
    max_failures: int,
) -> None:
    """Property 6ÔºöËøûÁª≠Â§±Ë¥•ÁÜîÊñ≠„ÄÇ

    ËøûÁª≠ M Ê¨°Â∑•ÂÖ∑Â§±Ë¥•ÂêéÔºåEngine ÂøÖÈ°ªÁªàÊ≠¢Âπ∂ËøîÂõûÈîôËØØÊëòË¶Å„ÄÇ

    **Validates: Requirements 1.6**
    """
    registry = ToolRegistry()
    registry.register_tools([
        ToolDef(
            name="always_fail",
            description="ÊÄªÊòØÂ§±Ë¥•",
            input_schema={"type": "object", "properties": {}},
            func=lambda: (_ for _ in ()).throw(RuntimeError("boom")),
        ),
    ])

    config = _make_config(
        max_consecutive_failures=max_failures,
        max_iterations=max_failures + 5,  # Á°Æ‰øù‰∏ç‰ºöÂÖàËß¶ÂèëËø≠‰ª£‰∏äÈôê
    )
    engine = AgentEngine(config, registry)
    engine._full_access_enabled = True

    # ÊûÑÈÄ†Ë∂≥Â§üÂ§öÁöÑÂ§±Ë¥• tool_call ÂìçÂ∫î
    fail_responses = [
        _make_tool_call_response([(f"call_{i}", "always_fail", "{}")])
        for i in range(max_failures + 3)
    ]

    engine._client.chat.completions.create = AsyncMock(side_effect=fail_responses)

    result = await engine.chat("ÁÜîÊñ≠ÊµãËØï")

    # ‰∏çÂèòÈáè 1ÔºöËøîÂõûÁªìÊûúÂåÖÂê´Â§±Ë¥•/ÁÜîÊñ≠Áõ∏ÂÖ≥‰ø°ÊÅØ
    assert "Â§±Ë¥•" in result or "ÁªàÊ≠¢" in result or "ÈîôËØØ" in result

    # ‰∏çÂèòÈáè 2ÔºöLLM Ë∞ÉÁî®Ê¨°Êï∞‰∏çË∂ÖËøá max_failuresÔºàÂú®Á¨¨ max_failures ËΩÆÁÜîÊñ≠Ôºâ
    assert engine._client.chat.completions.create.call_count <= max_failures


# ---------------------------------------------------------------------------
# Property 20ÔºöÂºÇÊ≠•‰∏çÈòªÂ°û
# **Validates: Requirements 1.10, 5.7**
# ---------------------------------------------------------------------------


@given(
    n_calls=st.integers(min_value=1, max_value=3),
)
@pytest.mark.asyncio
async def test_property_20_async_non_blocking(n_calls: int) -> None:
    """Property 20ÔºöÂºÇÊ≠•‰∏çÈòªÂ°û„ÄÇ

    Âπ∂ÂèëËØ∑Ê±ÇÂú∫ÊôØ‰∏ãÔºåÈòªÂ°ûÂ∑•ÂÖ∑ÊâßË°å‰∏çÂæóÈòªÂ°û‰∏ª‰∫ã‰ª∂Âæ™ÁéØ„ÄÇ
    È™åËØÅ asyncio.to_thread Ë¢´Áî®‰∫éÂ∑•ÂÖ∑ÊâßË°å„ÄÇ

    **Validates: Requirements 1.10, 5.7**
    """
    config = _make_config()
    registry = _make_registry_with_tools()
    engine = AgentEngine(config, registry)

    # ÊûÑÈÄ† n_calls ‰∏™ tool_calls Âú®ÂçïËΩÆÂìçÂ∫î‰∏≠
    tc_list = [
        (f"call_{i}", "add_numbers", json.dumps({"a": i, "b": i}))
        for i in range(n_calls)
    ]
    tool_response = _make_tool_call_response(tc_list)
    text_response = _make_text_response("ÂÆåÊàê")

    engine._client.chat.completions.create = AsyncMock(
        side_effect=[tool_response, text_response]
    )

    with patch(
        "excelmanus.engine.asyncio.to_thread", new_callable=AsyncMock
    ) as mock_to_thread:
        # Ê®°Êãü to_thread ËøîÂõûÂ∑•ÂÖ∑ÁªìÊûú
        mock_to_thread.side_effect = [i + i for i in range(n_calls)]

        result = await engine.chat("ÂºÇÊ≠•ÊµãËØï")

        # ‰∏çÂèòÈáè 1Ôºöasyncio.to_thread Ë¢´Ë∞ÉÁî®‰∫Ü n_calls Ê¨°
        assert mock_to_thread.call_count == n_calls

        # ‰∏çÂèòÈáè 2ÔºöÊØèÊ¨°Ë∞ÉÁî®ÈÉΩ‰º†ÂÖ•‰∫ÜÂèØÂú®Á∫øÁ®ãÊ±†ÊâßË°åÁöÑÂèØË∞ÉÁî®ÂØπË±°
        for call in mock_to_thread.call_args_list:
            assert len(call.args) == 1
            assert callable(call.args[0])

        # ‰∏çÂèòÈáè 3ÔºöÊµÅÁ®ãÂèØÊ≠£Â∏∏Êî∂ÊïõÂà∞ÊúÄÁªàÊñáÊú¨ÁªìÊûú
        assert result == "ÂÆåÊàê"


class TestApprovalFlow:
    """Accept Èó®Á¶Å‰∏ªÊµÅÁ®ãÊµãËØï„ÄÇ"""

    def _make_registry_with_write_tool(self, workspace: Path) -> ToolRegistry:
        registry = ToolRegistry()

        def write_text_file(
            file_path: str,
            content: str,
            overwrite: bool = True,
            encoding: str = "utf-8",
        ) -> str:
            target = workspace / file_path
            if target.exists() and not overwrite:
                return json.dumps({"status": "error", "error": "exists"}, ensure_ascii=False)
            target.parent.mkdir(parents=True, exist_ok=True)
            target.write_text(content, encoding=encoding)
            return json.dumps({"status": "success", "file": file_path}, ensure_ascii=False)

        registry.register_tools([
            ToolDef(
                name="write_text_file",
                description="ÂÜôÊñá‰ª∂",
                input_schema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string"},
                        "content": {"type": "string"},
                        "overwrite": {"type": "boolean"},
                        "encoding": {"type": "string"},
                    },
                    "required": ["file_path", "content"],
                },
                func=write_text_file,
            ),
        ])
        return registry

    def _make_registry_with_audit_tool(self, workspace: Path) -> ToolRegistry:
        registry = ToolRegistry()

        def create_chart(output_path: str) -> str:
            target = workspace / output_path
            target.parent.mkdir(parents=True, exist_ok=True)
            target.write_text("chart", encoding="utf-8")
            return json.dumps({"status": "success", "output_path": output_path}, ensure_ascii=False)

        registry.register_tools([
            ToolDef(
                name="create_chart",
                description="ÁîüÊàêÂõæË°®Êñá‰ª∂",
                input_schema={
                    "type": "object",
                    "properties": {
                        "output_path": {"type": "string"},
                    },
                    "required": ["output_path"],
                },
                func=create_chart,
            ),
        ])
        return registry

    def _make_registry_with_failing_write_tool(self, workspace: Path) -> ToolRegistry:
        registry = ToolRegistry()

        def write_text_file(file_path: str, content: str) -> str:
            target = workspace / file_path
            target.parent.mkdir(parents=True, exist_ok=True)
            target.write_text(content, encoding="utf-8")
            raise RuntimeError("intentional_write_failure")

        registry.register_tools([
            ToolDef(
                name="write_text_file",
                description="ÂÜôÊñá‰ª∂ÂêéÊäõÈîô",
                input_schema={
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string"},
                        "content": {"type": "string"},
                    },
                    "required": ["file_path", "content"],
                },
                func=write_text_file,
            ),
        ])
        return registry

    def _make_registry_with_custom_tool(self) -> ToolRegistry:
        registry = ToolRegistry()

        def custom_tool() -> str:
            return "custom-ok"

        registry.register_tools([
            ToolDef(
                name="custom_tool",
                description="Ëá™ÂÆö‰πâÂ∑•ÂÖ∑",
                input_schema={"type": "object", "properties": {}},
                func=custom_tool,
            ),
        ])
        return registry

    @pytest.mark.asyncio
    async def test_high_risk_tool_requires_accept(self, tmp_path: Path) -> None:
        config = _make_config(workspace_root=str(tmp_path))
        registry = self._make_registry_with_write_tool(tmp_path)
        engine = AgentEngine(config, registry)

        tool_response = _make_tool_call_response([
            ("call_1", "write_text_file", json.dumps({"file_path": "a.txt", "content": "hello"}))
        ])
        engine._client.chat.completions.create = AsyncMock(side_effect=[tool_response])

        first_reply = await engine.chat("ÂÜôÂÖ•Êñá‰ª∂")
        assert "ÂæÖÁ°ÆËÆ§" in first_reply
        assert "accept" in first_reply
        assert not (tmp_path / "a.txt").exists()
        assert engine._approval.pending is not None
        approval_id = engine._approval.pending.approval_id

        blocked = await engine.chat("ÁªßÁª≠ÊâßË°å")
        assert "Â≠òÂú®ÂæÖÁ°ÆËÆ§Êìç‰Ωú" in blocked

        accept_reply = await engine.chat(f"/accept {approval_id}")
        assert "Â∑≤ÊâßË°åÂæÖÁ°ÆËÆ§Êìç‰Ωú" in accept_reply
        assert (tmp_path / "a.txt").read_text(encoding="utf-8") == "hello"
        assert (tmp_path / "outputs" / "approvals" / approval_id / "manifest.json").exists()

    @pytest.mark.asyncio
    async def test_reject_pending(self, tmp_path: Path) -> None:
        config = _make_config(workspace_root=str(tmp_path))
        registry = self._make_registry_with_write_tool(tmp_path)
        engine = AgentEngine(config, registry)

        tool_response = _make_tool_call_response([
            ("call_1", "write_text_file", json.dumps({"file_path": "b.txt", "content": "world"}))
        ])
        engine._client.chat.completions.create = AsyncMock(side_effect=[tool_response])
        await engine.chat("ÂÜôÊñá‰ª∂")
        assert engine._approval.pending is not None
        approval_id = engine._approval.pending.approval_id

        reject_reply = await engine.chat(f"/reject {approval_id}")
        assert "Â∑≤ÊãíÁªù" in reject_reply
        assert engine._approval.pending is None
        assert not (tmp_path / "b.txt").exists()

    @pytest.mark.asyncio
    async def test_undo_after_accept(self, tmp_path: Path) -> None:
        config = _make_config(workspace_root=str(tmp_path))
        registry = self._make_registry_with_write_tool(tmp_path)
        engine = AgentEngine(config, registry)

        tool_response = _make_tool_call_response([
            ("call_1", "write_text_file", json.dumps({"file_path": "c.txt", "content": "undo"}))
        ])
        engine._client.chat.completions.create = AsyncMock(side_effect=[tool_response])
        await engine.chat("ÂÜôÊñá‰ª∂")
        assert engine._approval.pending is not None
        approval_id = engine._approval.pending.approval_id
        await engine.chat(f"/accept {approval_id}")
        assert (tmp_path / "c.txt").exists()

        undo_reply = await engine.chat(f"/undo {approval_id}")
        assert "Â∑≤ÂõûÊªö" in undo_reply
        assert not (tmp_path / "c.txt").exists()

    @pytest.mark.asyncio
    async def test_failed_accept_still_writes_failed_manifest(self, tmp_path: Path) -> None:
        config = _make_config(workspace_root=str(tmp_path))
        registry = self._make_registry_with_failing_write_tool(tmp_path)
        engine = AgentEngine(config, registry)

        tool_response = _make_tool_call_response([
            ("call_1", "write_text_file", json.dumps({"file_path": "err.txt", "content": "x"}))
        ])
        engine._client.chat.completions.create = AsyncMock(side_effect=[tool_response])
        await engine.chat("ÂÜôÊñá‰ª∂")
        assert engine._approval.pending is not None
        approval_id = engine._approval.pending.approval_id

        accept_reply = await engine.chat(f"/accept {approval_id}")
        assert "accept ÊâßË°åÂ§±Ë¥•" in accept_reply
        manifest_path = tmp_path / "outputs" / "approvals" / approval_id / "manifest.json"
        assert manifest_path.exists()
        manifest = json.loads(manifest_path.read_text(encoding="utf-8"))
        assert manifest["execution"]["status"] == "failed"
        assert manifest["execution"]["error_type"] == "ToolExecutionError"

    @pytest.mark.asyncio
    async def test_undo_after_restart_loads_manifest(self, tmp_path: Path) -> None:
        config = _make_config(workspace_root=str(tmp_path))
        registry = self._make_registry_with_write_tool(tmp_path)
        engine1 = AgentEngine(config, registry)

        tool_response = _make_tool_call_response([
            ("call_1", "write_text_file", json.dumps({"file_path": "restart.txt", "content": "v"}))
        ])
        engine1._client.chat.completions.create = AsyncMock(side_effect=[tool_response])
        await engine1.chat("ÂÜôÊñá‰ª∂")
        assert engine1._approval.pending is not None
        approval_id = engine1._approval.pending.approval_id
        await engine1.chat(f"/accept {approval_id}")
        assert (tmp_path / "restart.txt").exists()

        engine2 = AgentEngine(config, registry)
        undo_reply = await engine2.chat(f"/undo {approval_id}")
        assert "Â∑≤ÂõûÊªö" in undo_reply
        assert not (tmp_path / "restart.txt").exists()

    @pytest.mark.asyncio
    async def test_fullaccess_bypass_accept(self, tmp_path: Path) -> None:
        config = _make_config(workspace_root=str(tmp_path))
        registry = self._make_registry_with_write_tool(tmp_path)
        engine = AgentEngine(config, registry)

        on_reply = await engine.chat("/fullAccess on")
        assert "Â∑≤ÂºÄÂêØ" in on_reply

        tool_response = _make_tool_call_response([
            ("call_1", "write_text_file", json.dumps({"file_path": "d.txt", "content": "full"}))
        ])
        text_response = _make_text_response("ÂÆåÊàê")
        engine._client.chat.completions.create = AsyncMock(
            side_effect=[tool_response, text_response]
        )

        reply = await engine.chat("Áõ¥Êé•ÂÜô")
        assert reply == "ÂÆåÊàê"
        assert engine._approval.pending is None
        assert (tmp_path / "d.txt").read_text(encoding="utf-8") == "full"

    @pytest.mark.asyncio
    async def test_default_mode_non_whitelist_tool_executes_directly(self, tmp_path: Path) -> None:
        config = _make_config(workspace_root=str(tmp_path))
        registry = self._make_registry_with_custom_tool()
        engine = AgentEngine(config, registry)
        engine._active_skill = None

        tool_response = _make_tool_call_response([("call_1", "custom_tool", "{}")])
        text_response = _make_text_response("ÂÆåÊàê")
        engine._client.chat.completions.create = AsyncMock(
            side_effect=[tool_response, text_response]
        )

        reply = await engine.chat("ÊâßË°åËá™ÂÆö‰πâÂ∑•ÂÖ∑")
        assert reply == "ÂÆåÊàê"
        assert engine._approval.pending is None

    @pytest.mark.asyncio
    async def test_fullaccess_executes_non_whitelist_tool(self, tmp_path: Path) -> None:
        config = _make_config(workspace_root=str(tmp_path))
        registry = self._make_registry_with_custom_tool()
        engine = AgentEngine(config, registry)
        engine._active_skill = Skillpack(
            name="test/custom",
            description="test",
            allowed_tools=["custom_tool"],
            triggers=[],
            instructions="",
            source="project",
            root_dir=str(tmp_path),
        )

        on_reply = await engine.chat("/fullAccess on")
        assert "Â∑≤ÂºÄÂêØ" in on_reply

        tool_response = _make_tool_call_response([("call_1", "custom_tool", "{}")])
        text_response = _make_text_response("ÂÆåÊàê")
        engine._client.chat.completions.create = AsyncMock(
            side_effect=[tool_response, text_response]
        )

        reply = await engine.chat("ÊâßË°åËá™ÂÆö‰πâÂ∑•ÂÖ∑")
        assert reply == "ÂÆåÊàê"
        assert engine._approval.pending is None

    @pytest.mark.asyncio
    async def test_default_mode_audit_only_tool_executes_without_accept(self, tmp_path: Path) -> None:
        config = _make_config(workspace_root=str(tmp_path))
        registry = self._make_registry_with_audit_tool(tmp_path)
        engine = AgentEngine(config, registry)
        engine._execute_tool_with_audit = AsyncMock(return_value=('{"status":"success"}', None))

        tool_response = _make_tool_call_response([
            ("call_1", "create_chart", json.dumps({"output_path": "charts/a.png"}))
        ])
        text_response = _make_text_response("ÂÆåÊàê")
        engine._client.chat.completions.create = AsyncMock(
            side_effect=[tool_response, text_response]
        )

        reply = await engine.chat("ÁîüÊàêÂõæË°®")
        assert reply == "ÂÆåÊàê"
        assert engine._approval.pending is None
        engine._execute_tool_with_audit.assert_awaited_once()
