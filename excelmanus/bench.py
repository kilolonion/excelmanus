"""Bench æµ‹è¯•è¿è¡Œå™¨ï¼šåŠ è½½ç”¨ä¾‹ JSON â†’ è°ƒç”¨ engine.chat() â†’ æ”¶é›†äº‹ä»¶è½¨è¿¹ â†’ è¾“å‡º JSON æ—¥å¿—ã€‚

è¿è¡Œæ–¹å¼ï¼š
    python -m excelmanus.bench --all
    python -m excelmanus.bench --suite bench/cases/suite_basic.json
    python -m excelmanus.bench bench/cases/suite_basic.json
    python -m excelmanus.bench --message "è¯»å–é”€å”®æ˜ç»†å‰10è¡Œ"
    python -m excelmanus.bench "è¯»å–é”€å”®æ˜ç»†å‰10è¡Œ"
"""

from __future__ import annotations

import asyncio
import argparse
import json
import os
import sys
import time
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

from rich.console import Console
from rich.live import Live
from rich.markdown import Markdown
from rich.panel import Panel
from rich.table import Table

from excelmanus.config import ExcelManusConfig, load_config
from excelmanus.engine import AgentEngine, ChatResult
from excelmanus.events import EventType, ToolCallEvent
from excelmanus.logger import get_logger, setup_logging
from excelmanus.renderer import StreamRenderer
from excelmanus.skillpacks import SkillpackLoader, SkillRouter
from excelmanus.tools import ToolRegistry

logger = get_logger("bench")

# å·¥å…·ç»“æœæœ€å¤§ä¿ç•™å­—ç¬¦æ•°ï¼ˆé¿å…æ—¥å¿—è¿‡å¤§ï¼‰
_TOOL_RESULT_MAX_CHARS = 8000

# trace æ¨¡å¼ä¸‹ç³»ç»Ÿæç¤ºæœ€å¤§ä¿ç•™å­—ç¬¦æ•°
_TRACE_SYSTEM_PROMPT_MAX_CHARS = 50000

# â”€â”€ æ•°æ®æ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@dataclass
class BenchCase:
    """å•ä¸ªæµ‹è¯•ç”¨ä¾‹ã€‚

    æ”¯æŒå•è½®å’Œå¤šè½®ä¸¤ç§æ ¼å¼ï¼š
    - å•è½®ï¼šä»…è®¾ç½® ``message``ï¼ˆå‘åå…¼å®¹ï¼‰
    - å¤šè½®ï¼šè®¾ç½® ``messages`` åˆ—è¡¨ï¼ŒæŒ‰é¡ºåºå‘é€ç»™åŒä¸€ä¸ª engine å®ä¾‹
    åŠ è½½æ—¶ä¼šç»Ÿä¸€å½’ä¸€åŒ–ä¸º ``messages`` åˆ—è¡¨ã€‚
    """

    id: str
    name: str
    message: str = ""
    messages: list[str] = field(default_factory=list)
    tags: list[str] = field(default_factory=list)
    expected: dict[str, Any] = field(default_factory=dict)


@dataclass
class ToolCallLog:
    """å•æ¬¡å·¥å…·è°ƒç”¨æ—¥å¿—ã€‚"""

    tool_name: str
    arguments: dict[str, Any]
    success: bool
    result: str
    error: str | None
    iteration: int
    duration_ms: float = 0.0

    def to_dict(self) -> dict[str, Any]:
        return {
            "tool_name": self.tool_name,
            "arguments": self.arguments,
            "success": self.success,
            "result": self.result[:_TOOL_RESULT_MAX_CHARS] if self.result else "",
            "error": self.error,
            "iteration": self.iteration,
            "duration_ms": round(self.duration_ms, 1),
        }


@dataclass
class TurnResult:
    """å¤šè½®å¯¹è¯ä¸­å•è½®çš„æ‰§è¡Œç»“æœã€‚"""

    turn_index: int
    message: str
    reply: str
    duration_seconds: float
    iterations: int
    route_mode: str
    skills_used: list[str]
    tool_scope: list[str]
    tool_calls: list[ToolCallLog]
    thinking_log: list[str]
    subagent_events: list[dict[str, Any]]
    llm_calls: list[dict[str, Any]]
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    status: str = "ok"
    error: dict[str, Any] | None = None
    # engine å†…éƒ¨äº¤äº’è½¨è¿¹ï¼ˆ--trace å¯ç”¨æ—¶æœ‰å€¼ï¼‰
    engine_trace: list[dict[str, Any]] = field(default_factory=list)

    def to_dict(self) -> dict[str, Any]:
        tool_successes = sum(1 for tc in self.tool_calls if tc.success)
        tool_failures = sum(1 for tc in self.tool_calls if not tc.success)
        result = {
            "turn_index": self.turn_index,
            "message": self.message,
            "reply": self.reply,
            "duration_seconds": round(self.duration_seconds, 2),
            "iterations": self.iterations,
            "route_mode": self.route_mode,
            "skills_used": self.skills_used,
            "tool_scope": self.tool_scope,
            "tool_calls": [tc.to_dict() for tc in self.tool_calls],
            "thinking_log": self.thinking_log,
            "subagent_events": self.subagent_events,
            "llm_calls": self.llm_calls,
            "tokens": {
                "prompt_tokens": self.prompt_tokens,
                "completion_tokens": self.completion_tokens,
                "total_tokens": self.total_tokens,
            },
            "status": self.status,
            "error": self.error,
            "stats": {
                "tool_call_count": len(self.tool_calls),
                "tool_successes": tool_successes,
                "tool_failures": tool_failures,
                "llm_call_count": len(self.llm_calls),
            },
        }
        if self.engine_trace:
            result["engine_trace"] = self.engine_trace
        return result


@dataclass
class BenchResult:
    """å•ä¸ªç”¨ä¾‹çš„æ‰§è¡Œç»“æœã€‚"""

    case_id: str
    case_name: str
    message: str
    timestamp: str
    duration_seconds: float
    iterations: int
    route_mode: str
    skills_used: list[str]
    tool_scope: list[str]
    tool_calls: list[ToolCallLog]
    thinking_log: list[str]
    reply: str
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    # subagent äº‹ä»¶
    subagent_events: list[dict[str, Any]] = field(default_factory=list)
    # å®Œæ•´ LLM äº¤äº’è®°å½•ï¼šæ¯æ¬¡ API è°ƒç”¨çš„è¯·æ±‚å’Œå“åº”
    llm_calls: list[dict[str, Any]] = field(default_factory=list)
    # æœ€ç»ˆå¯¹è¯è®°å¿†å¿«ç…§ï¼ˆsystem prompt + æ‰€æœ‰æ¶ˆæ¯ï¼‰
    conversation_messages: list[dict[str, Any]] = field(default_factory=list)
    # å¤šè½®å¯¹è¯å„è½®æ¬¡çš„ç‹¬ç«‹ç»“æœ
    turns: list[TurnResult] = field(default_factory=list)
    # æ‰§è¡ŒçŠ¶æ€
    status: str = "ok"
    # ç»“æ„åŒ–é”™è¯¯ä¿¡æ¯ï¼ˆstatus=error æ—¶æœ‰å€¼ï¼‰
    error: dict[str, Any] | None = None
    # engine å†…éƒ¨äº¤äº’è½¨è¿¹ï¼ˆ--trace å¯ç”¨æ—¶æœ‰å€¼ï¼‰
    engine_trace: list[dict[str, Any]] = field(default_factory=list)

    def to_dict(self) -> dict[str, Any]:
        tool_successes = sum(1 for tc in self.tool_calls if tc.success)
        tool_failures = sum(1 for tc in self.tool_calls if not tc.success)
        result: dict[str, Any] = {
            "schema_version": 2,
            "kind": "case_result",
            "timestamp": self.timestamp,
            "meta": {
                "case_id": self.case_id,
                "case_name": self.case_name,
                "message": self.message,
                "turn_count": len(self.turns) if self.turns else 1,
            },
            "execution": {
                "duration_seconds": round(self.duration_seconds, 2),
                "iterations": self.iterations,
                "route_mode": self.route_mode,
                "skills_used": self.skills_used,
                "tool_scope": self.tool_scope,
                "status": self.status,
                "error": self.error,
            },
            "artifacts": {
                "tool_calls": [tc.to_dict() for tc in self.tool_calls],
                "thinking_log": self.thinking_log,
                "subagent_events": self.subagent_events,
                "llm_calls": self.llm_calls,
                "conversation_messages": self.conversation_messages,
            },
            "result": {
                "reply": self.reply,
            },
            "stats": {
                "prompt_tokens": self.prompt_tokens,
                "completion_tokens": self.completion_tokens,
                "total_tokens": self.total_tokens,
                "tool_call_count": len(self.tool_calls),
                "tool_successes": tool_successes,
                "tool_failures": tool_failures,
                "llm_call_count": len(self.llm_calls),
            },
        }
        # å¤šè½®å¯¹è¯æ—¶è¾“å‡ºå„è½®æ¬¡è¯¦æƒ…
        if self.turns:
            result["turns"] = [t.to_dict() for t in self.turns]
        # engine å†…éƒ¨äº¤äº’è½¨è¿¹
        if self.engine_trace:
            result["engine_trace"] = self.engine_trace
        return result


# â”€â”€ äº‹ä»¶æ”¶é›†å™¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


# å…¨å±€ Rich Console
_console = Console()


class _EventCollector:
    """é€šè¿‡ on_event å›è°ƒæ”¶é›†å¼•æ“äº‹ä»¶ï¼ŒåŒæ—¶å®æ—¶æ¸²æŸ“åˆ°ç»ˆç«¯ã€‚"""

    def __init__(self, *, render_enabled: bool = True) -> None:
        self._renderer = StreamRenderer(_console)
        self._render_enabled = render_enabled
        self.thinking_log: list[str] = []
        self.tool_calls: list[ToolCallLog] = []
        self.route_mode: str = ""
        self.skills_used: list[str] = []
        self.tool_scope: list[str] = []
        self.subagent_events: list[dict[str, Any]] = []
        # ç”¨äºè®¡ç®—å·¥å…·è°ƒç”¨è€—æ—¶
        self._pending_tool_starts: dict[str, float] = {}

    def on_event(self, event: ToolCallEvent) -> None:
        """å¼•æ“äº‹ä»¶å›è°ƒï¼šå®æ—¶æ¸²æŸ“ + æ”¶é›†æ—¥å¿—ã€‚"""
        # å®æ—¶æ¸²æŸ“åˆ°ç»ˆç«¯ï¼ˆå’Œ CLI ä¸€æ ·çš„æ•ˆæœï¼‰
        if self._render_enabled:
            self._renderer.handle_event(event)

        # åŒæ—¶æ”¶é›†åˆ°æ—¥å¿—
        if event.event_type == EventType.THINKING:
            if event.thinking and event.thinking.strip():
                self.thinking_log.append(event.thinking.strip())

        elif event.event_type == EventType.TOOL_CALL_START:
            # è®°å½•å¼€å§‹æ—¶é—´
            key = f"{event.tool_name}_{event.iteration}_{len(self.tool_calls)}"
            self._pending_tool_starts[key] = time.monotonic()

        elif event.event_type == EventType.TOOL_CALL_END:
            # è®¡ç®—è€—æ—¶
            key_prefix = f"{event.tool_name}_{event.iteration}_"
            duration_ms = 0.0
            for k in list(self._pending_tool_starts.keys()):
                if k.startswith(key_prefix):
                    start_time = self._pending_tool_starts.pop(k)
                    duration_ms = (time.monotonic() - start_time) * 1000
                    break

            self.tool_calls.append(ToolCallLog(
                tool_name=event.tool_name,
                arguments=dict(event.arguments) if event.arguments else {},
                success=event.success,
                result=event.result or "",
                error=event.error,
                iteration=event.iteration,
                duration_ms=duration_ms,
            ))

        elif event.event_type == EventType.ROUTE_END:
            self.route_mode = event.route_mode
            self.skills_used = list(event.skills_used)
            self.tool_scope = list(event.tool_scope)

        elif event.event_type in {
            EventType.SUBAGENT_START,
            EventType.SUBAGENT_SUMMARY,
            EventType.SUBAGENT_END,
        }:
            self.subagent_events.append({
                "event_type": event.event_type.value,
                "name": event.subagent_name,
                "reason": event.subagent_reason,
                "summary": event.subagent_summary,
                "success": event.subagent_success,
                "iterations": event.subagent_iterations,
                "tool_calls": event.subagent_tool_calls,
            })

    def snapshot_and_reset(self) -> dict[str, Any]:
        """å¿«ç…§å½“å‰æ”¶é›†çš„æ•°æ®å¹¶é‡ç½®ï¼Œç”¨äºå¤šè½®å¯¹è¯åˆ†è½®è®°å½•ã€‚

        è¿”å›æœ¬è½®æ”¶é›†åˆ°çš„æ‰€æœ‰æ•°æ®å‰¯æœ¬ï¼Œç„¶åæ¸…ç©ºå†…éƒ¨çŠ¶æ€ã€‚
        """
        snapshot = {
            "thinking_log": list(self.thinking_log),
            "tool_calls": list(self.tool_calls),
            "route_mode": self.route_mode,
            "skills_used": list(self.skills_used),
            "tool_scope": list(self.tool_scope),
            "subagent_events": list(self.subagent_events),
        }
        # é‡ç½®
        self.thinking_log = []
        self.tool_calls = []
        self.route_mode = ""
        self.skills_used = []
        self.tool_scope = []
        self.subagent_events = []
        self._pending_tool_starts.clear()
        return snapshot


# â”€â”€ LLM è°ƒç”¨æ‹¦æˆªå™¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _serialize_message(msg: dict[str, Any]) -> dict[str, Any]:
    """å°†å•æ¡æ¶ˆæ¯åºåˆ—åŒ–ä¸ºå¯ JSON åŒ–çš„å­—å…¸ã€‚"""
    out: dict[str, Any] = {}
    for key, value in msg.items():
        if value is None:
            out[key] = None
        elif isinstance(value, (str, int, float, bool)):
            out[key] = value
        elif isinstance(value, list):
            out[key] = value
        elif isinstance(value, dict):
            out[key] = value
        else:
            out[key] = str(value)
    return out


def _serialize_tool_call_obj(tc: Any) -> dict[str, Any]:
    """å°† LLM å“åº”ä¸­çš„ tool_call å¯¹è±¡åºåˆ—åŒ–ã€‚"""
    function = getattr(tc, "function", None)
    return {
        "id": getattr(tc, "id", ""),
        "type": getattr(tc, "type", "function"),
        "function": {
            "name": getattr(function, "name", "") if function else "",
            "arguments": getattr(function, "arguments", "") if function else "",
        },
    }


def _serialize_llm_response(response: Any) -> dict[str, Any]:
    """å°† LLM API å®Œæ•´å“åº”åºåˆ—åŒ–ä¸ºå¯ JSON åŒ–çš„å­—å…¸ã€‚"""
    choice = response.choices[0] if response.choices else None
    message = choice.message if choice else None

    result: dict[str, Any] = {}

    if message is not None:
        result["content"] = message.content
        result["role"] = getattr(message, "role", "assistant")

        # æå– thinking / reasoning å†…å®¹
        for thinking_key in ("thinking", "reasoning", "reasoning_content"):
            val = getattr(message, thinking_key, None)
            if val:
                result["thinking"] = str(val)
                break

        # åºåˆ—åŒ– tool_calls
        if message.tool_calls:
            result["tool_calls"] = [
                _serialize_tool_call_obj(tc) for tc in message.tool_calls
            ]

    # finish_reason
    if choice is not None:
        result["finish_reason"] = getattr(choice, "finish_reason", None)

    # token ä½¿ç”¨
    usage = getattr(response, "usage", None)
    if usage is not None:
        result["usage"] = {
            "prompt_tokens": getattr(usage, "prompt_tokens", 0),
            "completion_tokens": getattr(usage, "completion_tokens", 0),
            "total_tokens": getattr(usage, "total_tokens", 0),
        }

    return result


class _LLMCallInterceptor:
    """æ‹¦æˆª engine çš„ LLM API è°ƒç”¨ï¼Œè®°å½•å®Œæ•´çš„è¯·æ±‚å’Œå“åº”ã€‚

    é€šè¿‡ monkey-patch engine._client.chat.completions.create å®ç°ï¼Œ
    æ— éœ€ä¿®æ”¹ engine æºä»£ç ã€‚
    """

    def __init__(self, engine: AgentEngine) -> None:
        self.calls: list[dict[str, Any]] = []
        self._engine = engine
        self._original_create = engine._client.chat.completions.create
        # monkey-patch
        engine._client.chat.completions.create = self._intercepted_create

    async def _intercepted_create(self, **kwargs: Any) -> Any:
        """æ‹¦æˆª LLM API è°ƒç”¨ï¼Œè®°å½•è¯·æ±‚å’Œå“åº”ã€‚"""
        call_record: dict[str, Any] = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "request": {
                "model": kwargs.get("model"),
                "messages": [
                    _serialize_message(m) for m in kwargs.get("messages", [])
                ],
            },
        }

        # è®°å½• tools å®šä¹‰ï¼ˆä»…åç§°åˆ—è¡¨ï¼Œå®Œæ•´ schema å¤ªå¤§ï¼‰
        tools = kwargs.get("tools")
        if tools:
            call_record["request"]["tool_names"] = [
                t.get("function", {}).get("name", "")
                for t in tools
                if isinstance(t, dict)
            ]

        start = time.monotonic()
        try:
            response = await self._original_create(**kwargs)
        except Exception as exc:
            call_record["error"] = str(exc)
            call_record["duration_ms"] = round(
                (time.monotonic() - start) * 1000, 1
            )
            self.calls.append(call_record)
            raise

        call_record["duration_ms"] = round(
            (time.monotonic() - start) * 1000, 1
        )
        call_record["response"] = _serialize_llm_response(response)
        self.calls.append(call_record)
        return response

    def restore(self) -> None:
        """æ¢å¤åŸå§‹çš„ create æ–¹æ³•ã€‚"""
        self._engine._client.chat.completions.create = self._original_create


class _EngineTracer:
    """æ‹¦æˆª engine å…³é”®æ–¹æ³•ï¼Œè®°å½•ç¨‹åºå‘ agent æ³¨å…¥çš„æŒ‡ä»¤å’Œå†…éƒ¨å†³ç­–ã€‚

    é€šè¿‡ monkey-patch ä»¥ä¸‹æ–¹æ³•å®ç°ï¼š
    - ``_prepare_system_prompts_for_request`` â†’ è®°å½•æ¯è½®æ³¨å…¥çš„ç³»ç»Ÿæç¤ºï¼ˆåˆ†è§£å„ç»„ä»¶ï¼‰
    - ``_enrich_tool_result_with_window_perception`` â†’ è®°å½•çª—å£æ„ŸçŸ¥å¢å¼ºå‰åå¯¹æ¯”
    - ``_get_current_tool_scope`` â†’ è®°å½•æ¯è½®å·¥å…·èŒƒå›´å†³ç­–

    é€šè¿‡ç¯å¢ƒå˜é‡ ``EXCELMANUS_BENCH_TRACE=1`` æˆ– CLI ``--trace`` å¯ç”¨ã€‚
    """

    def __init__(self, engine: AgentEngine) -> None:
        self.entries: list[dict[str, Any]] = []
        self._engine = engine
        self._iteration = 0

        # ä¿å­˜åŸå§‹æ–¹æ³•
        self._orig_prepare = engine._prepare_system_prompts_for_request
        self._orig_enrich = engine._enrich_tool_result_with_window_perception
        self._orig_scope = engine._get_current_tool_scope

        # monkey-patch
        engine._prepare_system_prompts_for_request = self._traced_prepare  # type: ignore[assignment]
        engine._enrich_tool_result_with_window_perception = self._traced_enrich  # type: ignore[assignment]
        engine._get_current_tool_scope = self._traced_scope  # type: ignore[assignment]

    def _traced_prepare(
        self, skill_contexts: list[str],
    ) -> tuple[list[str], str | None]:
        """æ‹¦æˆªç³»ç»Ÿæç¤ºæ„å»ºï¼Œè®°å½•å„ç»„ä»¶å†…å®¹ã€‚"""
        self._iteration += 1
        prompts, error = self._orig_prepare(skill_contexts)

        # åˆ†è§£è®°å½•å„ç»„ä»¶
        components: list[dict[str, Any]] = []
        for idx, prompt in enumerate(prompts):
            label = "base_system_prompt" if idx == 0 else f"context_{idx}"
            # å°è¯•è¯†åˆ«ç»„ä»¶ç±»å‹
            if idx > 0:
                snippet = prompt[:200]
                if "çª—å£æ„ŸçŸ¥" in snippet or "Window Perception" in snippet:
                    label = "window_perception_notice"
                elif "æƒé™æç¤º" in snippet or "fullAccess" in snippet:
                    label = "access_notice"
                elif "MCP" in snippet:
                    label = "mcp_context_notice"
                elif "Hook" in snippet:
                    label = "hook_context"
                elif "è®¡åˆ’" in snippet and "å·²æ‰¹å‡†" in snippet:
                    label = "approved_plan_context"
                else:
                    label = f"skill_context_{idx}"
            components.append({
                "label": label,
                "char_count": len(prompt),
                "content": prompt[:_TRACE_SYSTEM_PROMPT_MAX_CHARS],
                "truncated": len(prompt) > _TRACE_SYSTEM_PROMPT_MAX_CHARS,
            })

        self.entries.append({
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "event": "system_prompts_injected",
            "iteration": self._iteration,
            "data": {
                "prompt_count": len(prompts),
                "total_chars": sum(len(p) for p in prompts),
                "skill_context_count": len(skill_contexts),
                "context_error": error,
                "components": components,
            },
        })
        return prompts, error

    def _traced_enrich(
        self,
        *,
        tool_name: str,
        arguments: dict[str, Any],
        result_text: str,
        success: bool,
    ) -> str:
        """æ‹¦æˆªçª—å£æ„ŸçŸ¥å¢å¼ºï¼Œè®°å½•å‰åå¯¹æ¯”ã€‚"""
        enriched = self._orig_enrich(
            tool_name=tool_name,
            arguments=arguments,
            result_text=result_text,
            success=success,
        )
        # ä»…åœ¨å†…å®¹å®é™…è¢«å¢å¼ºæ—¶è®°å½•
        if enriched != result_text:
            self.entries.append({
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "event": "window_perception_enrichment",
                "iteration": self._iteration,
                "data": {
                    "tool_name": tool_name,
                    "original_chars": len(result_text),
                    "enriched_chars": len(enriched),
                    "added_chars": len(enriched) - len(result_text),
                    "enriched_suffix": enriched[len(result_text):][:2000],
                },
            })
        return enriched

    def _traced_scope(self, **kwargs: Any) -> list[str]:
        """æ‹¦æˆªå·¥å…·èŒƒå›´å†³ç­–ï¼Œè®°å½•å¯ç”¨å·¥å…·åˆ—è¡¨ã€‚"""
        scope = self._orig_scope(**kwargs)
        self.entries.append({
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "event": "tool_scope_resolved",
            "iteration": self._iteration,
            "data": {
                "tool_count": len(scope),
                "tools": list(scope),
            },
        })
        return scope

    def snapshot_and_reset(self) -> list[dict[str, Any]]:
        """å¿«ç…§å½“å‰ trace æ•°æ®å¹¶é‡ç½®ï¼Œç”¨äºå¤šè½®åˆ†è½®è®°å½•ã€‚"""
        snapshot = list(self.entries)
        self.entries = []
        self._iteration = 0
        return snapshot

    def restore(self) -> None:
        """æ¢å¤åŸå§‹æ–¹æ³•ã€‚"""
        self._engine._prepare_system_prompts_for_request = self._orig_prepare  # type: ignore[assignment]
        self._engine._enrich_tool_result_with_window_perception = self._orig_enrich  # type: ignore[assignment]
        self._engine._get_current_tool_scope = self._orig_scope  # type: ignore[assignment]


# â”€â”€ æ‰§è¡Œå™¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _create_engine(config: ExcelManusConfig) -> AgentEngine:
    """åˆ›å»ºç‹¬ç«‹çš„ AgentEngine å®ä¾‹ï¼ˆä¸å¤ç”¨ sessionï¼‰ã€‚

    è‡ªåŠ¨å¯ç”¨ bench sandbox æ¨¡å¼ï¼Œè§£é™¤æ‰€æœ‰äº¤äº’å¼é˜»å¡
    ï¼ˆfullAccess / plan æ‹¦æˆª / ç¡®è®¤é—¨ç¦ï¼‰ã€‚
    """
    registry = ToolRegistry()
    registry.register_builtin_tools(config.workspace_root)
    loader = SkillpackLoader(config, registry)
    loader.load_all()
    router = SkillRouter(config, loader)
    engine = AgentEngine(
        config=config,
        registry=registry,
        skill_router=router,
    )
    engine.enable_bench_sandbox()
    return engine


def _dump_conversation_messages(engine: AgentEngine) -> list[dict[str, Any]]:
    """ä» engine çš„ memory ä¸­å¯¼å‡ºå®Œæ•´å¯¹è¯æ¶ˆæ¯å¿«ç…§ã€‚"""
    try:
        messages = engine._memory.get_messages()
        return [_serialize_message(m) for m in messages]
    except Exception:
        return []


async def run_case(
    case: BenchCase,
    config: ExcelManusConfig,
    *,
    render_enabled: bool = True,
    trace_enabled: bool = False,
) -> BenchResult:
    """æ‰§è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œè¿”å›å®Œæ•´ç»“æœï¼ˆå«å®Œæ•´ LLM äº¤äº’æ—¥å¿—ï¼‰ã€‚

    æ”¯æŒå¤šè½®å¯¹è¯ï¼šå½“ case.messages åŒ…å«å¤šæ¡æ¶ˆæ¯æ—¶ï¼Œä¾æ¬¡å‘é€ç»™åŒä¸€ä¸ª
    engine å®ä¾‹ï¼ŒConversationMemory è‡ªç„¶ä¿æŒä¸Šä¸‹æ–‡ã€‚

    Args:
        trace_enabled: å¯ç”¨ engine å†…éƒ¨äº¤äº’è½¨è¿¹è®°å½•ï¼ˆç³»ç»Ÿæç¤ºæ³¨å…¥ã€
            çª—å£æ„ŸçŸ¥å¢å¼ºã€å·¥å…·èŒƒå›´å†³ç­–ç­‰ï¼‰ã€‚é€šè¿‡ ``--trace`` æˆ–
            ``EXCELMANUS_BENCH_TRACE=1`` å¯ç”¨ã€‚
    """
    engine = _create_engine(config)
    collector = _EventCollector(render_enabled=render_enabled)
    interceptor = _LLMCallInterceptor(engine)
    tracer = _EngineTracer(engine) if trace_enabled else None
    timestamp = datetime.now(timezone.utc).isoformat()

    # å½’ä¸€åŒ–æ¶ˆæ¯åˆ—è¡¨ï¼šå…¼å®¹å•è½® message å’Œå¤šè½® messages
    messages = case.messages if case.messages else [case.message]
    is_multi_turn = len(messages) > 1

    logger.info(
        "â–¶ å¼€å§‹æ‰§è¡Œç”¨ä¾‹: %s (%s) [%d è½®]",
        case.id, case.name, len(messages),
    )
    case_start = time.monotonic()

    # ç´¯è®¡ç»Ÿè®¡
    all_turns: list[TurnResult] = []
    all_tool_calls: list[ToolCallLog] = []
    all_thinking_log: list[str] = []
    all_subagent_events: list[dict[str, Any]] = []
    total_iterations = 0
    total_prompt_tokens = 0
    total_completion_tokens = 0
    total_total_tokens = 0
    last_reply = ""
    last_route_mode = ""
    last_skills_used: list[str] = []
    last_tool_scope: list[str] = []
    case_status = "ok"
    case_error: dict[str, Any] | None = None

    try:
        for turn_idx, msg in enumerate(messages):
            if is_multi_turn:
                logger.info(
                    "  â”€â”€ è½®æ¬¡ %d/%d â”€â”€", turn_idx + 1, len(messages),
                )

            # æ¯è½®å¼€å§‹å‰è®°å½• interceptor çš„è°ƒç”¨æ•°ï¼Œç”¨äºåˆ‡åˆ†æœ¬è½®çš„ llm_calls
            llm_calls_before = len(interceptor.calls)
            turn_start = time.monotonic()

            try:
                chat_result: ChatResult = await engine.chat(
                    msg,
                    on_event=collector.on_event,
                )
            except Exception as exc:
                logger.error(
                    "ç”¨ä¾‹ %s è½®æ¬¡ %d æ‰§è¡Œå¼‚å¸¸: %s",
                    case.id, turn_idx, exc, exc_info=True,
                )
                turn_elapsed = time.monotonic() - turn_start
                # å¿«ç…§æœ¬è½®æ”¶é›†å™¨æ•°æ®
                snap = collector.snapshot_and_reset()
                turn_llm_calls = list(interceptor.calls[llm_calls_before:])

                turn_result = TurnResult(
                    turn_index=turn_idx,
                    message=msg,
                    reply=f"[ERROR] {exc}",
                    duration_seconds=turn_elapsed,
                    iterations=0,
                    route_mode=snap["route_mode"] or "error",
                    skills_used=snap["skills_used"],
                    tool_scope=snap["tool_scope"],
                    tool_calls=snap["tool_calls"],
                    thinking_log=snap["thinking_log"],
                    subagent_events=snap["subagent_events"],
                    llm_calls=turn_llm_calls,
                    status="error",
                    error={
                        "type": type(exc).__name__,
                        "message": str(exc),
                    },
                    engine_trace=tracer.snapshot_and_reset() if tracer else [],
                )
                all_turns.append(turn_result)
                all_tool_calls.extend(snap["tool_calls"])
                all_thinking_log.extend(snap["thinking_log"])
                all_subagent_events.extend(snap["subagent_events"])
                last_reply = f"[ERROR] {exc}"
                case_status = "error"
                case_error = {
                    "type": type(exc).__name__,
                    "message": str(exc),
                }
                # æŸè½®å¼‚å¸¸åä¸­æ­¢åç»­è½®æ¬¡
                break

            turn_elapsed = time.monotonic() - turn_start
            # å¿«ç…§æœ¬è½®æ”¶é›†å™¨æ•°æ®
            snap = collector.snapshot_and_reset()
            turn_llm_calls = list(interceptor.calls[llm_calls_before:])

            # å›é€€è·¯ç”±ä¿¡æ¯
            last_route = getattr(engine, "last_route_result", None)
            fallback_route_mode = getattr(last_route, "route_mode", "")
            fallback_skills = list(getattr(last_route, "skills_used", []) or [])
            fallback_scope = list(getattr(last_route, "tool_scope", []) or [])

            turn_route_mode = snap["route_mode"] or fallback_route_mode or "unknown"
            turn_skills = snap["skills_used"] or fallback_skills
            turn_scope = snap["tool_scope"] or fallback_scope

            turn_result = TurnResult(
                turn_index=turn_idx,
                message=msg,
                reply=chat_result.reply,
                duration_seconds=turn_elapsed,
                iterations=chat_result.iterations,
                route_mode=turn_route_mode,
                skills_used=turn_skills,
                tool_scope=turn_scope,
                tool_calls=snap["tool_calls"],
                thinking_log=snap["thinking_log"],
                subagent_events=snap["subagent_events"],
                llm_calls=turn_llm_calls,
                prompt_tokens=chat_result.prompt_tokens,
                completion_tokens=chat_result.completion_tokens,
                total_tokens=chat_result.total_tokens,
                engine_trace=tracer.snapshot_and_reset() if tracer else [],
            )
            all_turns.append(turn_result)

            # ç´¯è®¡
            all_tool_calls.extend(snap["tool_calls"])
            all_thinking_log.extend(snap["thinking_log"])
            all_subagent_events.extend(snap["subagent_events"])
            total_iterations += chat_result.iterations
            total_prompt_tokens += chat_result.prompt_tokens
            total_completion_tokens += chat_result.completion_tokens
            total_total_tokens += chat_result.total_tokens
            last_reply = chat_result.reply
            last_route_mode = turn_route_mode
            last_skills_used = turn_skills
            last_tool_scope = turn_scope

            # å¤šè½®æ—¶æ‰“å°æ¯è½®å›å¤
            if render_enabled and is_multi_turn and chat_result.reply:
                _console.print()
                _console.print(
                    Panel(
                        Markdown(chat_result.reply),
                        title=f"è½®æ¬¡ {turn_idx + 1}/{len(messages)}",
                        border_style="#5f875f",
                        padding=(1, 2),
                        expand=False,
                    )
                )
    finally:
        interceptor.restore()
        if tracer is not None:
            tracer.restore()

    case_elapsed = time.monotonic() - case_start

    # å•è½®ç”¨ä¾‹çš„ engine_trace ç›´æ¥ä» tracer è·å–ï¼ˆå¤šè½®å·²åœ¨å„ turn ä¸­è®°å½•ï¼‰
    case_engine_trace: list[dict[str, Any]] = []
    if tracer is not None and not is_multi_turn:
        case_engine_trace = tracer.snapshot_and_reset()

    result = BenchResult(
        case_id=case.id,
        case_name=case.name,
        message=case.message or (messages[0] if messages else ""),
        timestamp=timestamp,
        duration_seconds=case_elapsed,
        iterations=total_iterations,
        route_mode=last_route_mode or "unknown",
        skills_used=last_skills_used,
        tool_scope=last_tool_scope,
        tool_calls=all_tool_calls,
        thinking_log=all_thinking_log,
        reply=last_reply,
        prompt_tokens=total_prompt_tokens,
        completion_tokens=total_completion_tokens,
        total_tokens=total_total_tokens,
        subagent_events=all_subagent_events,
        llm_calls=interceptor.calls,
        conversation_messages=_dump_conversation_messages(engine),
        turns=all_turns if is_multi_turn else [],
        status=case_status,
        error=case_error,
        engine_trace=case_engine_trace,
    )

    # å•è½®æ—¶æ‰“å°æœ€ç»ˆå›å¤ï¼ˆå¤šè½®å·²åœ¨å¾ªç¯ä¸­é€è½®æ‰“å°ï¼‰
    if render_enabled and not is_multi_turn and result.reply:
        _console.print()
        _console.print(
            Panel(
                Markdown(result.reply),
                border_style="#5f875f",
                padding=(1, 2),
                expand=False,
            )
        )

    failures = sum(1 for tc in result.tool_calls if not tc.success)
    turn_info = f" ({len(messages)} è½®)" if is_multi_turn else ""
    logger.info(
        "âœ“ ç”¨ä¾‹ %s å®Œæˆ%s: %d è¿­ä»£ â”‚ %d å·¥å…·è°ƒç”¨(å¤±è´¥%d) â”‚ %d tokens â”‚ %.1fs â”‚ %d æ¬¡ LLM è°ƒç”¨",
        case.id,
        turn_info,
        result.iterations,
        len(result.tool_calls),
        failures,
        result.total_tokens,
        result.duration_seconds,
        len(result.llm_calls),
    )
    return result


def _load_suite(path: str | Path) -> tuple[str, list[BenchCase]]:
    """ä» JSON æ–‡ä»¶åŠ è½½æµ‹è¯•å¥—ä»¶ã€‚

    å…¼å®¹ä¸¤ç§ case æ ¼å¼ï¼š
    - å•è½®ï¼š``{"message": "..."}``
    - å¤šè½®ï¼š``{"messages": ["...", "..."]}``
    åŠ è½½æ—¶ç»Ÿä¸€å½’ä¸€åŒ–ä¸º messages åˆ—è¡¨ã€‚
    """
    with open(path, encoding="utf-8") as f:
        data = json.load(f)

    suite_name = data.get("suite_name", Path(path).stem)
    cases: list[BenchCase] = []
    for item in data.get("cases", []):
        # å¤šè½®æ ¼å¼ä¼˜å…ˆ
        raw_messages = item.get("messages")
        raw_message = item.get("message", "")
        if raw_messages and isinstance(raw_messages, list):
            messages = [str(m) for m in raw_messages if m]
            message = messages[0] if messages else ""
        else:
            message = raw_message
            messages = [message] if message else []

        cases.append(BenchCase(
            id=item["id"],
            name=item.get("name", item["id"]),
            message=message,
            messages=messages,
            tags=item.get("tags", []),
            expected=item.get("expected", {}),
        ))
    return suite_name, cases


def _save_result(result: BenchResult, output_dir: Path) -> Path:
    """ä¿å­˜å•ä¸ªç”¨ä¾‹ç»“æœåˆ° JSON æ–‡ä»¶ã€‚"""
    output_dir.mkdir(parents=True, exist_ok=True)
    ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%S")
    short_id = uuid.uuid4().hex[:6]
    filename = f"run_{ts}_{result.case_id}_{short_id}.json"
    filepath = output_dir / filename
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(result.to_dict(), f, ensure_ascii=False, indent=2)
    return filepath


def _save_suite_summary(
    suite_name: str,
    suite_path: str | Path,
    results: list[BenchResult],
    output_dir: Path,
    *,
    concurrency: int,
    case_log_files: list[Path],
) -> Path:
    """ä¿å­˜å¥—ä»¶æ±‡æ€»ç»“æœã€‚"""
    output_dir.mkdir(parents=True, exist_ok=True)
    ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%S")
    short_id = uuid.uuid4().hex[:6]
    filename = f"suite_{ts}_{short_id}.json"
    filepath = output_dir / filename

    total_tokens = sum(r.total_tokens for r in results)
    total_duration = sum(r.duration_seconds for r in results)
    avg_iterations = (
        sum(r.iterations for r in results) / len(results) if results else 0
    )
    total_prompt_tokens = sum(r.prompt_tokens for r in results)
    total_completion_tokens = sum(r.completion_tokens for r in results)
    total_tool_calls = sum(len(r.tool_calls) for r in results)
    total_tool_failures = sum(
        sum(1 for tc in r.tool_calls if not tc.success) for r in results
    )
    failed_case_ids = [r.case_id for r in results if r.status != "ok"]
    suite_status = "ok" if not failed_case_ids else "completed_with_errors"

    summary = {
        "schema_version": 2,
        "kind": "suite_summary",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "meta": {
            "suite_name": suite_name,
            "suite_path": str(suite_path),
            "case_count": len(results),
        },
        "execution": {
            "concurrency": concurrency,
            "status": suite_status,
        },
        "artifacts": {
            "case_log_files": [str(p) for p in case_log_files],
            "cases": [r.to_dict() for r in results],
        },
        "result": {
            "failed_case_ids": failed_case_ids,
        },
        "stats": {
            "total_tokens": total_tokens,
            "total_prompt_tokens": total_prompt_tokens,
            "total_completion_tokens": total_completion_tokens,
            "total_duration_seconds": round(total_duration, 2),
            "average_iterations": round(avg_iterations, 2),
            "tool_call_count": total_tool_calls,
            "tool_failures": total_tool_failures,
        },
    }

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    return filepath


async def run_suite(
    suite_path: str | Path,
    config: ExcelManusConfig,
    output_dir: Path,
    *,
    concurrency: int = 1,
    trace_enabled: bool = False,
) -> list[BenchResult]:
    """è¿è¡Œæ•´ä¸ªæµ‹è¯•å¥—ä»¶ã€‚"""
    if concurrency < 1:
        raise ValueError("concurrency å¿…é¡» >= 1")

    suite_name, cases = _load_suite(suite_path)
    logger.info("â•" * 50)
    logger.info(
        "å¼€å§‹æ‰§è¡Œå¥—ä»¶: %s (%d ä¸ªç”¨ä¾‹, å¹¶å‘=%d%s)",
        suite_name,
        len(cases),
        concurrency,
        ", trace=ON" if trace_enabled else "",
    )
    logger.info("â•" * 50)

    async def _execute_case(
        index: int,
        case: BenchCase,
        *,
        render_enabled: bool,
    ) -> tuple[int, BenchResult, Path]:
        try:
            result = await run_case(
                case, config,
                render_enabled=render_enabled,
                trace_enabled=trace_enabled,
            )
        except Exception as exc:  # pragma: no cover - å…œåº•ä¿æŠ¤
            logger.error("ç”¨ä¾‹ %s æ‰§è¡Œå´©æºƒ: %s", case.id, exc, exc_info=True)
            result = BenchResult(
                case_id=case.id,
                case_name=case.name,
                message=case.message,
                timestamp=datetime.now(timezone.utc).isoformat(),
                duration_seconds=0.0,
                iterations=0,
                route_mode="error",
                skills_used=[],
                tool_scope=[],
                tool_calls=[],
                thinking_log=[],
                reply=f"[CRASH] {exc}",
                prompt_tokens=0,
                completion_tokens=0,
                total_tokens=0,
                status="error",
                error={
                    "type": type(exc).__name__,
                    "message": str(exc),
                },
            )
        try:
            filepath = _save_result(result, output_dir)
            logger.info("  æ—¥å¿—å·²ä¿å­˜: %s", filepath)
        except Exception as exc:  # pragma: no cover - æ–‡ä»¶ç³»ç»Ÿå¼‚å¸¸å…œåº•
            logger.error("ç”¨ä¾‹ %s æ—¥å¿—ä¿å­˜å¤±è´¥: %s", case.id, exc, exc_info=True)
            filepath = output_dir / f"run_save_error_{case.id}.json"
        return index, result, filepath

    results: list[BenchResult | None] = [None] * len(cases)
    case_log_files: list[Path | None] = [None] * len(cases)

    if concurrency == 1:
        for i, case in enumerate(cases, 1):
            logger.info("â”€â”€ ç”¨ä¾‹ %d/%d â”€â”€", i, len(cases))
            index, result, filepath = await _execute_case(
                i - 1,
                case,
                render_enabled=True,
            )
            results[index] = result
            case_log_files[index] = filepath
    else:
        logger.info("å¹¶å‘æ¨¡å¼å·²å¯ç”¨ï¼šå…³é—­é€äº‹ä»¶ç»ˆç«¯æ¸²æŸ“ï¼Œé¿å…è¾“å‡ºäº¤é”™ã€‚")
        semaphore = asyncio.Semaphore(concurrency)

        async def _worker(index: int, case: BenchCase) -> tuple[int, BenchResult, Path]:
            async with semaphore:
                logger.info("â”€â”€ ç”¨ä¾‹ %d/%d (å¹¶å‘) â”€â”€", index + 1, len(cases))
                return await _execute_case(
                    index,
                    case,
                    render_enabled=False,
                )

        tasks = [
            asyncio.create_task(_worker(index, case))
            for index, case in enumerate(cases)
        ]
        for index, result, filepath in await asyncio.gather(*tasks):
            results[index] = result
            case_log_files[index] = filepath

    # ç†è®ºä¸Š results ä¸ä¼šä¸º Noneï¼Œæ­¤å¤„åŠ å…œåº•ä¿è¯ç±»å‹ç¨³å®š
    normalized_results: list[BenchResult] = []
    normalized_case_files: list[Path] = []
    for index, case in enumerate(cases):
        current = results[index]
        if current is None:  # pragma: no cover - é˜²å¾¡æ€§é€»è¾‘
            current = BenchResult(
                case_id=case.id,
                case_name=case.name,
                message=case.message,
                timestamp=datetime.now(timezone.utc).isoformat(),
                duration_seconds=0.0,
                iterations=0,
                route_mode="error",
                skills_used=[],
                tool_scope=[],
                tool_calls=[],
                thinking_log=[],
                reply="[CRASH] case result missing",
                prompt_tokens=0,
                completion_tokens=0,
                total_tokens=0,
                status="error",
                error={
                    "type": "InternalError",
                    "message": "missing case result",
                },
            )
        normalized_results.append(current)
        if case_log_files[index] is not None:
            normalized_case_files.append(case_log_files[index])

    # ä¿å­˜å¥—ä»¶æ±‡æ€»
    summary_path = _save_suite_summary(
        suite_name,
        suite_path,
        normalized_results,
        output_dir,
        concurrency=concurrency,
        case_log_files=normalized_case_files,
    )
    logger.info("â•" * 50)
    logger.info("å¥—ä»¶æ‰§è¡Œå®Œæ¯•: %s", suite_name)
    logger.info("  æ±‡æ€»æ—¥å¿—: %s", summary_path)

    # æ‰“å°ç®€è¦ç»Ÿè®¡
    total_tokens = sum(r.total_tokens for r in normalized_results)
    total_duration = sum(r.duration_seconds for r in normalized_results)
    total_failures = sum(
        sum(1 for tc in r.tool_calls if not tc.success) for r in normalized_results
    )
    case_errors = sum(
        1 for r in normalized_results if r.status != "ok"
    )
    logger.info(
        "  ç»Ÿè®¡: %d ç”¨ä¾‹ â”‚ æ€» %d tokens â”‚ æ€» %.1fs â”‚ å·¥å…·å¤±è´¥ %d æ¬¡ â”‚ ç”¨ä¾‹å¤±è´¥ %d",
        len(normalized_results),
        total_tokens,
        total_duration,
        total_failures,
        case_errors,
    )
    logger.info("â•" * 50)
    return normalized_results


# â”€â”€ å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


async def run_single(
    message: str,
    config: ExcelManusConfig,
    output_dir: Path,
    *,
    trace_enabled: bool = False,
) -> BenchResult:
    """ç›´æ¥è¿è¡Œä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½œä¸ºæµ‹è¯•ç”¨ä¾‹ã€‚"""
    case = BenchCase(
        id="adhoc",
        name="ä¸´æ—¶ç”¨ä¾‹",
        message=message,
        messages=[message],
    )
    result = await run_case(
        case, config,
        render_enabled=True,
        trace_enabled=trace_enabled,
    )
    filepath = _save_result(result, output_dir)
    logger.info("æ—¥å¿—å·²ä¿å­˜: %s", filepath)
    return result


@dataclass
class _RunPlan:
    """bench CLI è§£æåçš„æ‰§è¡Œè®¡åˆ’ã€‚"""

    mode: str
    suite_paths: list[Path] = field(default_factory=list)
    message: str = ""


def _positive_int(raw: str) -> int:
    """argparse ä½¿ç”¨çš„æ­£æ•´æ•°è§£æå™¨ã€‚"""
    try:
        value = int(raw)
    except ValueError as exc:
        raise argparse.ArgumentTypeError("å¿…é¡»æ˜¯æ•´æ•°") from exc
    if value < 1:
        raise argparse.ArgumentTypeError("å¿…é¡»æ˜¯ >= 1 çš„æ•´æ•°")
    return value


def _build_parser() -> argparse.ArgumentParser:
    """æ„å»º bench CLI å‚æ•°è§£æå™¨ã€‚"""
    parser = argparse.ArgumentParser(
        prog="python -m excelmanus.bench",
        description="Bench æµ‹è¯•è¿è¡Œå™¨",
    )
    parser.add_argument(
        "targets",
        nargs="*",
        help="ä½ç½®å‚æ•°ï¼šæ™ºèƒ½è¯†åˆ«ä¸º suite è·¯å¾„ï¼ˆ*.jsonï¼‰æˆ– message æ–‡æœ¬",
    )
    group = parser.add_mutually_exclusive_group()
    group.add_argument(
        "--suite",
        nargs="+",
        metavar="PATH",
        help="æ˜¾å¼æŒ‡å®šä¸€ä¸ªæˆ–å¤šä¸ª suite JSON æ–‡ä»¶",
    )
    group.add_argument(
        "--all",
        action="store_true",
        help="è¿è¡Œ bench/cases/ ä¸‹æ‰€æœ‰ suite",
    )
    group.add_argument(
        "--message",
        help="æ˜¾å¼æŒ‡å®šå•æ¡æ¶ˆæ¯ä½œä¸ºç”¨ä¾‹",
    )
    parser.add_argument(
        "--concurrency",
        type=_positive_int,
        default=1,
        help="å•ä¸ª suite å†…ç”¨ä¾‹å¹¶å‘åº¦ï¼ˆé»˜è®¤ 1ï¼‰",
    )
    parser.add_argument(
        "--suite-concurrency",
        type=_positive_int,
        default=1,
        help="suite é—´å¹¶å‘åº¦ï¼ˆé»˜è®¤ 1ï¼Œä»…å¤š suite æ—¶ç”Ÿæ•ˆï¼‰",
    )
    parser.add_argument(
        "--output-dir",
        default="outputs/bench",
        help="æ—¥å¿—è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ outputs/benchï¼‰",
    )
    parser.add_argument(
        "--trace",
        action="store_true",
        default=False,
        help="å¯ç”¨ engine å†…éƒ¨äº¤äº’è½¨è¿¹è®°å½•ï¼ˆç³»ç»Ÿæç¤ºæ³¨å…¥ã€çª—å£æ„ŸçŸ¥å¢å¼ºã€å·¥å…·èŒƒå›´ç­‰ï¼‰ã€‚"
        "ä¹Ÿå¯é€šè¿‡ EXCELMANUS_BENCH_TRACE=1 ç¯å¢ƒå˜é‡å¯ç”¨ã€‚",
    )
    return parser


def _is_json_like_path(raw: str) -> bool:
    """åˆ¤æ–­å‚æ•°æ˜¯å¦æ»¡è¶³ suite æ–‡ä»¶è·¯å¾„è¯­ä¹‰ã€‚"""
    return raw.lower().endswith(".json")


def _resolve_run_mode(args: argparse.Namespace) -> _RunPlan:
    """å°† argparse ç»“æœæ˜ å°„ä¸ºæ‰§è¡Œè®¡åˆ’ã€‚"""
    targets = list(args.targets or [])

    if args.suite is not None:
        if targets:
            raise ValueError("ä½¿ç”¨ --suite æ—¶ä¸åº”å†ä¼ å…¥ä½ç½®å‚æ•°ã€‚")
        return _RunPlan(
            mode="suite",
            suite_paths=[Path(p) for p in args.suite],
        )

    if args.all:
        if targets:
            raise ValueError("ä½¿ç”¨ --all æ—¶ä¸åº”å†ä¼ å…¥ä½ç½®å‚æ•°ã€‚")
        return _RunPlan(mode="all")

    if args.message is not None:
        if targets:
            raise ValueError("ä½¿ç”¨ --message æ—¶ä¸åº”å†ä¼ å…¥ä½ç½®å‚æ•°ã€‚")
        return _RunPlan(mode="message", message=args.message)

    if not targets:
        return _RunPlan(mode="help")

    # æ™ºèƒ½è¯†åˆ«ï¼šå…¨éƒ¨çœ‹èµ·æ¥åƒ *.json æ—¶ï¼Œè§†ä¸º suite æ¨¡å¼ï¼›å¦åˆ™æŒ‰ message æ¨¡å¼ã€‚
    if all(_is_json_like_path(item) for item in targets):
        return _RunPlan(
            mode="suite",
            suite_paths=[Path(item) for item in targets],
        )

    return _RunPlan(mode="message", message=" ".join(targets))


async def _run_suites(
    suite_paths: list[Path],
    config: ExcelManusConfig,
    output_dir: Path,
    *,
    concurrency: int = 1,
    suite_concurrency: int = 1,
    trace_enabled: bool = False,
) -> int:
    """å¹¶å‘è¿è¡Œå¤šä¸ª suiteï¼Œå¸¦ Rich Live è¿›åº¦é¢æ¿å’Œå…¨å±€æ±‡æ€»ã€‚

    è¿”å› shell é€€å‡ºç ï¼ˆ0 = å…¨éƒ¨æˆåŠŸï¼Œ1 = å­˜åœ¨å¤±è´¥ï¼‰ã€‚
    """
    total_suites = len(suite_paths)

    # æ¯ä¸ª suite çš„çŠ¶æ€è¿½è¸ª
    suite_states: dict[str, str] = {}  # suite_name -> çŠ¶æ€æ–‡æœ¬
    suite_results: list[tuple[str, list[BenchResult]]] = []
    results_lock = asyncio.Lock()

    # ç”¨ suite æ–‡ä»¶ååšç®€çŸ­æ ‡è¯†
    def _short_name(p: Path) -> str:
        return p.stem

    for p in suite_paths:
        suite_states[_short_name(p)] = "â³ ç­‰å¾…ä¸­"

    def _build_progress_table() -> Table:
        """æ„å»ºå®æ—¶è¿›åº¦è¡¨æ ¼ã€‚"""
        table = Table(
            title="Bench å¹¶å‘æ‰§è¡Œé¢æ¿",
            show_lines=True,
            expand=False,
        )
        table.add_column("Suite", style="cyan", min_width=25)
        table.add_column("çŠ¶æ€", min_width=20)

        for name, status in suite_states.items():
            table.add_row(name, status)
        return table

    async def _suite_worker(
        suite_path: Path,
        sem: asyncio.Semaphore,
    ) -> None:
        name = _short_name(suite_path)
        async with sem:
            suite_states[name] = "ğŸ”„ æ‰§è¡Œä¸­..."
            try:
                results = await run_suite(
                    suite_path,
                    config,
                    output_dir,
                    concurrency=concurrency,
                    trace_enabled=trace_enabled,
                )
                ok_count = sum(1 for r in results if r.status == "ok")
                fail_count = len(results) - ok_count
                if fail_count:
                    suite_states[name] = f"âš ï¸  å®Œæˆ ({ok_count}âœ… {fail_count}âŒ)"
                else:
                    suite_states[name] = f"âœ… å®Œæˆ ({ok_count} ç”¨ä¾‹)"
                async with results_lock:
                    suite_results.append((name, results))
            except Exception as exc:
                suite_states[name] = f"ğŸ’¥ å´©æºƒ: {exc}"
                async with results_lock:
                    suite_results.append((name, []))

    sem = asyncio.Semaphore(suite_concurrency)
    is_parallel = suite_concurrency > 1 and total_suites > 1

    if is_parallel:
        # å¹¶å‘æ¨¡å¼ï¼šå¯åŠ¨ Live è¿›åº¦é¢æ¿
        logger.info(
            "å¯åŠ¨å¹¶å‘æ¨¡å¼ï¼š%d ä¸ª suiteï¼Œsuite å¹¶å‘=%dï¼Œcase å¹¶å‘=%d",
            total_suites,
            suite_concurrency,
            concurrency,
        )
        tasks = [
            asyncio.create_task(_suite_worker(p, sem))
            for p in suite_paths
        ]

        console = Console()
        with Live(
            _build_progress_table(),
            console=console,
            refresh_per_second=2,
        ) as live:
            # å®šæœŸåˆ·æ–°è¿›åº¦è¡¨
            while not all(t.done() for t in tasks):
                live.update(_build_progress_table())
                await asyncio.sleep(0.5)
            # æœ€ç»ˆåˆ·æ–°ä¸€æ¬¡
            live.update(_build_progress_table())

        # æ”¶é›†å¼‚å¸¸ï¼ˆä¸åº”å‘ç”Ÿï¼Œ_suite_worker å†…éƒ¨å·²å…œåº•ï¼‰
        for t in tasks:
            if t.exception():  # pragma: no cover
                logger.error("suite ä»»åŠ¡å¼‚å¸¸: %s", t.exception())
    else:
        # ä¸²è¡Œæ¨¡å¼ï¼šé€ä¸ªæ‰§è¡Œï¼Œä¿æŒåŸæœ‰æ—¥å¿—è¾“å‡º
        for suite_path in suite_paths:
            await _suite_worker(suite_path, sem)

    # â”€â”€ å…¨å±€æ±‡æ€»æŠ¥å‘Š â”€â”€
    all_results = [r for _, results in suite_results for r in results]
    if all_results:
        total_cases = len(all_results)
        total_ok = sum(1 for r in all_results if r.status == "ok")
        total_fail = total_cases - total_ok
        total_tokens = sum(r.total_tokens for r in all_results)
        total_duration = sum(r.duration_seconds for r in all_results)
        total_tool_failures = sum(
            sum(1 for tc in r.tool_calls if not tc.success) for r in all_results
        )

        # ä¿å­˜å…¨å±€æ±‡æ€» JSON
        ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%S")
        short_id = uuid.uuid4().hex[:6]
        global_summary = {
            "schema_version": 2,
            "kind": "global_summary",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "execution": {
                "suite_count": total_suites,
                "suite_concurrency": suite_concurrency,
                "case_concurrency": concurrency,
            },
            "stats": {
                "total_cases": total_cases,
                "passed": total_ok,
                "failed": total_fail,
                "total_tokens": total_tokens,
                "total_duration_seconds": round(total_duration, 2),
                "tool_failures": total_tool_failures,
            },
            "suites": [
                {
                    "name": name,
                    "case_count": len(results),
                    "passed": sum(1 for r in results if r.status == "ok"),
                    "failed": sum(1 for r in results if r.status != "ok"),
                }
                for name, results in suite_results
            ],
        }
        output_dir.mkdir(parents=True, exist_ok=True)
        global_path = output_dir / f"global_{ts}_{short_id}.json"
        with open(global_path, "w", encoding="utf-8") as f:
            json.dump(global_summary, f, ensure_ascii=False, indent=2)

        logger.info("â•" * 60)
        logger.info("å…¨å±€æ±‡æ€»")
        logger.info("â•" * 60)
        logger.info(
            "  %d ä¸ª suite â”‚ %d ç”¨ä¾‹ â”‚ %d é€šè¿‡ â”‚ %d å¤±è´¥",
            total_suites,
            total_cases,
            total_ok,
            total_fail,
        )
        logger.info(
            "  æ€» %d tokens â”‚ æ€» %.1fs â”‚ å·¥å…·å¤±è´¥ %d æ¬¡",
            total_tokens,
            total_duration,
            total_tool_failures,
        )
        logger.info("  å…¨å±€æ±‡æ€»: %s", global_path)
        logger.info("â•" * 60)

    return 0 if all(r.status == "ok" for r in all_results) else 1


async def _main(argv: list[str] | None = None) -> int:
    """è„šæœ¬å…¥å£ï¼Œè¿”å› shell é€€å‡ºç ã€‚"""
    parser = _build_parser()
    args = parser.parse_args(argv)

    try:
        plan = _resolve_run_mode(args)
    except ValueError as exc:
        print(f"å‚æ•°é”™è¯¯ï¼š{exc}", file=sys.stderr)
        return 1

    if plan.mode == "help":
        parser.print_help()
        return 0

    # trace æ¨¡å¼ï¼šCLI --trace æˆ–ç¯å¢ƒå˜é‡ EXCELMANUS_BENCH_TRACE=1
    trace_enabled = args.trace or os.environ.get("EXCELMANUS_BENCH_TRACE", "0") == "1"

    if plan.mode == "message":
        config = load_config()
        setup_logging(config.log_level)
        output_dir = Path(args.output_dir)
        await run_single(plan.message, config, output_dir, trace_enabled=trace_enabled)
        return 0

    if plan.mode == "suite":
        missing_paths = [p for p in plan.suite_paths if not p.is_file()]
        if missing_paths:
            for p in missing_paths:
                logger.error("æœªæ‰¾åˆ° suite æ–‡ä»¶: %s", p)
            return 1
        config = load_config()
        setup_logging(config.log_level)
        output_dir = Path(args.output_dir)
        return await _run_suites(
            plan.suite_paths,
            config,
            output_dir,
            concurrency=args.concurrency,
            suite_concurrency=args.suite_concurrency,
            trace_enabled=trace_enabled,
        )

    # all æ¨¡å¼
    cases_dir = Path("bench/cases")
    if not cases_dir.is_dir():
        logger.error("æœªæ‰¾åˆ°æµ‹è¯•ç”¨ä¾‹ç›®å½•: %s", cases_dir)
        return 1

    suite_paths = sorted(cases_dir.glob("*.json"))
    if not suite_paths:
        logger.error("ç›®å½• %s ä¸‹æ—  JSON ç”¨ä¾‹æ–‡ä»¶", cases_dir)
        return 1

    config = load_config()
    setup_logging(config.log_level)
    output_dir = Path(args.output_dir)
    return await _run_suites(
        suite_paths,
        config,
        output_dir,
        concurrency=args.concurrency,
        suite_concurrency=args.suite_concurrency,
        trace_enabled=trace_enabled,
    )


if __name__ == "__main__":
    raise SystemExit(asyncio.run(_main()))
