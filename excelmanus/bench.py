"""Bench æµ‹è¯•è¿è¡Œå™¨ï¼šåŠ è½½ç”¨ä¾‹ JSON â†’ è°ƒç”¨ engine.chat() â†’ æ”¶é›†äº‹ä»¶è½¨è¿¹ â†’ è¾“å‡º JSON æ—¥å¿—ã€‚

è¿è¡Œæ–¹å¼ï¼š
    python -m excelmanus.bench --all
    python -m excelmanus.bench --suite bench/cases/suite_basic.json
    python -m excelmanus.bench bench/cases/suite_basic.json
    python -m excelmanus.bench --message "è¯»å–é”€å”®æ˜ç»†å‰10è¡Œ"
    python -m excelmanus.bench "è¯»å–é”€å”®æ˜ç»†å‰10è¡Œ"
"""

from __future__ import annotations

import asyncio
import argparse
import json
import os
import re
import shutil
import sys
import time
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Callable

from rich.console import Console
from rich.live import Live
from rich.markdown import Markdown
from rich.panel import Panel
from rich.table import Table

from excelmanus.config import ExcelManusConfig, load_config
from excelmanus.engine import AgentEngine, ChatResult
from excelmanus.events import EventType, ToolCallEvent
from excelmanus.logger import get_logger, setup_logging
from excelmanus.renderer import StreamRenderer
from excelmanus.skillpacks import SkillpackLoader, SkillRouter
from excelmanus.tools import ToolRegistry
from excelmanus.bench_validator import (
    ValidationSummary,
    aggregate_suite_validation,
    merge_assertions,
    validate_case,
)
from excelmanus.bench_reporter import save_suite_report

logger = get_logger("bench")

# å·¥å…·ç»“æœæœ€å¤§ä¿ç•™å­—ç¬¦æ•°ï¼ˆé¿å…æ—¥å¿—è¿‡å¤§ï¼‰
_TOOL_RESULT_MAX_CHARS = 8000

# trace æ¨¡å¼ä¸‹ç³»ç»Ÿæç¤ºæœ€å¤§ä¿ç•™å­—ç¬¦æ•°
_TRACE_SYSTEM_PROMPT_MAX_CHARS = 50000

# â”€â”€ æ•°æ®æ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@dataclass
class BenchCase:
    """å•ä¸ªæµ‹è¯•ç”¨ä¾‹ã€‚

    æ”¯æŒå•è½®å’Œå¤šè½®ä¸¤ç§æ ¼å¼ï¼š
    - å•è½®ï¼šä»…è®¾ç½® ``message``
    - å¤šè½®ï¼šè®¾ç½® ``messages`` åˆ—è¡¨ï¼ŒæŒ‰é¡ºåºå‘é€ç»™åŒä¸€ä¸ª engine å®ä¾‹
    åŠ è½½æ—¶ä¼šç»Ÿä¸€å½’ä¸€åŒ–ä¸º ``messages`` åˆ—è¡¨ã€‚
    """

    id: str
    name: str
    message: str = ""
    messages: list[str] = field(default_factory=list)
    tags: list[str] = field(default_factory=list)
    expected: dict[str, Any] = field(default_factory=dict)
    source_files: list[str] = field(default_factory=list)
    auto_replies: list[str] = field(default_factory=list)
    assertions: dict[str, Any] = field(default_factory=dict)


@dataclass
class ToolCallLog:
    """å•æ¬¡å·¥å…·è°ƒç”¨æ—¥å¿—ã€‚"""

    tool_name: str
    arguments: dict[str, Any]
    success: bool
    result: str
    error: str | None
    iteration: int
    duration_ms: float = 0.0

    def to_dict(self) -> dict[str, Any]:
        return {
            "tool_name": self.tool_name,
            "arguments": self.arguments,
            "success": self.success,
            "result": self.result[:_TOOL_RESULT_MAX_CHARS] if self.result else "",
            "error": self.error,
            "iteration": self.iteration,
            "duration_ms": round(self.duration_ms, 1),
        }


@dataclass
class TurnResult:
    """å¤šè½®å¯¹è¯ä¸­å•è½®çš„æ‰§è¡Œç»“æœã€‚"""

    turn_index: int
    message: str
    reply: str
    duration_seconds: float
    iterations: int
    route_mode: str
    skills_used: list[str]
    tool_scope: list[str]
    tool_calls: list[ToolCallLog]
    thinking_log: list[str]
    subagent_events: list[dict[str, Any]]
    llm_calls: list[dict[str, Any]]
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    status: str = "ok"
    error: dict[str, Any] | None = None
    # engine å†…éƒ¨äº¤äº’è½¨è¿¹ï¼ˆ--trace å¯ç”¨æ—¶æœ‰å€¼ï¼‰
    engine_trace: list[dict[str, Any]] = field(default_factory=list)
    # å½“å‰ä½¿ç”¨çš„æ¨¡å‹æ ‡è¯†
    active_model: str = ""
    # ä»»åŠ¡/é—®ç­”/å®¡æ‰¹äº‹ä»¶
    task_events: list[dict[str, Any]] = field(default_factory=list)
    question_events: list[dict[str, Any]] = field(default_factory=list)
    approval_events: list[dict[str, Any]] = field(default_factory=list)
    # Think-Act æ¨ç†è´¨é‡æŒ‡æ ‡
    reasoning_metrics: dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> dict[str, Any]:
        tool_successes = sum(1 for tc in self.tool_calls if tc.success)
        tool_failures = sum(1 for tc in self.tool_calls if not tc.success)
        result = {
            "turn_index": self.turn_index,
            "message": self.message,
            "reply": self.reply,
            "duration_seconds": round(self.duration_seconds, 2),
            "iterations": self.iterations,
            "route_mode": self.route_mode,
            "skills_used": self.skills_used,
            "tool_scope": self.tool_scope,
            "tool_calls": [tc.to_dict() for tc in self.tool_calls],
            "thinking_log": self.thinking_log,
            "subagent_events": self.subagent_events,
            "llm_calls": self.llm_calls,
            "tokens": {
                "prompt_tokens": self.prompt_tokens,
                "completion_tokens": self.completion_tokens,
                "total_tokens": self.total_tokens,
            },
            "status": self.status,
            "error": self.error,
            "active_model": self.active_model,
            "stats": {
                "tool_call_count": len(self.tool_calls),
                "tool_successes": tool_successes,
                "tool_failures": tool_failures,
                "llm_call_count": len(self.llm_calls),
                "reasoning_metrics": self.reasoning_metrics,
            },
        }
        if self.engine_trace:
            result["engine_trace"] = self.engine_trace
        if self.task_events:
            result["task_events"] = self.task_events
        if self.question_events:
            result["question_events"] = self.question_events
        if self.approval_events:
            result["approval_events"] = self.approval_events
        return result


@dataclass
class BenchResult:
    """å•ä¸ªç”¨ä¾‹çš„æ‰§è¡Œç»“æœã€‚"""

    case_id: str
    case_name: str
    message: str
    timestamp: str
    duration_seconds: float
    iterations: int
    route_mode: str
    skills_used: list[str]
    tool_scope: list[str]
    tool_calls: list[ToolCallLog]
    thinking_log: list[str]
    reply: str
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    # subagent äº‹ä»¶
    subagent_events: list[dict[str, Any]] = field(default_factory=list)
    # å®Œæ•´ LLM äº¤äº’è®°å½•ï¼šæ¯æ¬¡ API è°ƒç”¨çš„è¯·æ±‚å’Œå“åº”
    llm_calls: list[dict[str, Any]] = field(default_factory=list)
    # æœ€ç»ˆå¯¹è¯è®°å¿†å¿«ç…§ï¼ˆsystem prompt + æ‰€æœ‰æ¶ˆæ¯ï¼‰
    conversation_messages: list[dict[str, Any]] = field(default_factory=list)
    # å¤šè½®å¯¹è¯å„è½®æ¬¡çš„ç‹¬ç«‹ç»“æœ
    turns: list[TurnResult] = field(default_factory=list)
    # æ‰§è¡ŒçŠ¶æ€
    status: str = "ok"
    # ç»“æ„åŒ–é”™è¯¯ä¿¡æ¯ï¼ˆstatus=error æ—¶æœ‰å€¼ï¼‰
    error: dict[str, Any] | None = None
    # engine å†…éƒ¨äº¤äº’è½¨è¿¹ï¼ˆ--trace å¯ç”¨æ—¶æœ‰å€¼ï¼‰
    engine_trace: list[dict[str, Any]] = field(default_factory=list)
    # å½“å‰ä½¿ç”¨çš„æ¨¡å‹æ ‡è¯†
    active_model: str = ""
    # write_hint åˆ†ç±»ç»“æœï¼ˆmay_write / read_only / unknownï¼‰
    write_hint: str = "unknown"
    # å…³é”®é…ç½®å¿«ç…§ï¼ˆç”¨äºäº‹åå®¡è®¡ï¼‰
    config_snapshot: dict[str, Any] = field(default_factory=dict)
    # ä»»åŠ¡/é—®ç­”/å®¡æ‰¹äº‹ä»¶
    task_events: list[dict[str, Any]] = field(default_factory=list)
    question_events: list[dict[str, Any]] = field(default_factory=list)
    approval_events: list[dict[str, Any]] = field(default_factory=list)
    # Think-Act æ¨ç†è´¨é‡æŒ‡æ ‡ï¼ˆèšåˆï¼‰
    reasoning_metrics: dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> dict[str, Any]:
        tool_successes = sum(1 for tc in self.tool_calls if tc.success)
        tool_failures = sum(1 for tc in self.tool_calls if not tc.success)
        result: dict[str, Any] = {
            "schema_version": 3,
            "kind": "case_result",
            "timestamp": self.timestamp,
            "meta": {
                "case_id": self.case_id,
                "case_name": self.case_name,
                "message": self.message,
                "turn_count": len(self.turns) if self.turns else 1,
                "active_model": self.active_model,
                "config_snapshot": self.config_snapshot,
            },
            "execution": {
                "duration_seconds": round(self.duration_seconds, 2),
                "iterations": self.iterations,
                "route_mode": self.route_mode,
                "skills_used": self.skills_used,
                "tool_scope": self.tool_scope,
                "status": self.status,
                "error": self.error,
                "write_hint": self.write_hint,
            },
            "artifacts": {
                "tool_calls": [tc.to_dict() for tc in self.tool_calls],
                "thinking_log": self.thinking_log,
                "subagent_events": self.subagent_events,
                "llm_calls": self.llm_calls,
                "conversation_messages": self.conversation_messages,
            },
            "result": {
                "reply": self.reply,
            },
            "stats": {
                "prompt_tokens": self.prompt_tokens,
                "completion_tokens": self.completion_tokens,
                "total_tokens": self.total_tokens,
                "tool_call_count": len(self.tool_calls),
                "tool_successes": tool_successes,
                "tool_failures": tool_failures,
                "llm_call_count": len(self.llm_calls),
                "reasoning_metrics": self.reasoning_metrics,
            },
        }
        # å¤šè½®å¯¹è¯æ—¶è¾“å‡ºå„è½®æ¬¡è¯¦æƒ…
        if self.turns:
            result["turns"] = [t.to_dict() for t in self.turns]
        # engine å†…éƒ¨äº¤äº’è½¨è¿¹
        if self.engine_trace:
            result["engine_trace"] = self.engine_trace
        # ä»»åŠ¡/é—®ç­”/å®¡æ‰¹äº‹ä»¶
        if self.task_events:
            result["artifacts"]["task_events"] = self.task_events
        if self.question_events:
            result["artifacts"]["question_events"] = self.question_events
        if self.approval_events:
            result["artifacts"]["approval_events"] = self.approval_events
        return result


# â”€â”€ è¿›åº¦è¿½è¸ª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@dataclass
class _SuiteProgress:
    """è¿½è¸ªå•ä¸ª suite çš„ case çº§æ‰§è¡Œè¿›åº¦ï¼ˆç”¨äºå¹¶å‘é¢æ¿å®æ—¶æ˜¾ç¤ºï¼‰ã€‚"""

    suite_name: str
    total_cases: int = 0
    done_cases: int = 0
    ok_cases: int = 0
    fail_cases: int = 0
    total_tokens: int = 0
    current_case: str = ""  # å½“å‰æ­£åœ¨æ‰§è¡Œçš„ case å
    start_time: float = field(default_factory=time.monotonic)
    status: str = "â³ ç­‰å¾…ä¸­"  # é¢æ¿æ˜¾ç¤ºçš„çŠ¶æ€æ–‡æœ¬

    def elapsed(self) -> float:
        return time.monotonic() - self.start_time

    def elapsed_str(self) -> str:
        s = self.elapsed()
        if s < 60:
            return f"{s:.0f}s"
        return f"{s / 60:.1f}m"

    def progress_bar(self) -> str:
        """ç”Ÿæˆç®€æ˜“è¿›åº¦æ¡ï¼Œå¦‚ â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 3/8ã€‚"""
        if self.total_cases == 0:
            return ""
        filled = int(self.done_cases / self.total_cases * 8)
        bar = "â–ˆ" * filled + "â–‘" * (8 - filled)
        return f"{bar} {self.done_cases}/{self.total_cases}"

    def status_line(self) -> str:
        """æ„å»ºé¢æ¿ä¸­çš„çŠ¶æ€è¡Œã€‚"""
        if self.status.startswith("â³"):
            return self.status
        if self.status.startswith("âœ…") or self.status.startswith("âš "):
            # å·²å®Œæˆ
            tok = f"{self.total_tokens:,}" if self.total_tokens else "0"
            return f"{self.status}  {self.elapsed_str()}  {tok} tok"
        if self.status.startswith("ğŸ’¥"):
            return self.status
        # æ‰§è¡Œä¸­
        parts = [f"ğŸ”„ {self.progress_bar()}"]
        if self.ok_cases or self.fail_cases:
            parts.append(f"{self.ok_cases}âœ…")
            if self.fail_cases:
                parts.append(f"{self.fail_cases}âŒ")
        parts.append(self.elapsed_str())
        if self.total_tokens:
            parts.append(f"{self.total_tokens:,} tok")
        if self.current_case:
            parts.append(f"â–¸ {self.current_case}")
        return "  ".join(parts)


# è¿›åº¦å›è°ƒç±»å‹ï¼š(case_id, case_name, result_or_none) â€” result ä¸º None è¡¨ç¤ºå¼€å§‹æ‰§è¡Œ
ProgressCallback = Callable[[str, str, BenchResult | None], None]


# â”€â”€ äº‹ä»¶æ”¶é›†å™¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


# å…¨å±€ Rich Console
_console = Console()


class _EventCollector:
    """é€šè¿‡ on_event å›è°ƒæ”¶é›†å¼•æ“äº‹ä»¶ï¼ŒåŒæ—¶å®æ—¶æ¸²æŸ“åˆ°ç»ˆç«¯ã€‚"""

    def __init__(self, *, render_enabled: bool = True) -> None:
        self._renderer = StreamRenderer(_console)
        self._render_enabled = render_enabled
        self.thinking_log: list[str] = []
        self.tool_calls: list[ToolCallLog] = []
        self.route_mode: str = ""
        self.skills_used: list[str] = []
        self.tool_scope: list[str] = []
        self.subagent_events: list[dict[str, Any]] = []
        self.task_events: list[dict[str, Any]] = []
        self.question_events: list[dict[str, Any]] = []
        self.approval_events: list[dict[str, Any]] = []
        # CHAT_SUMMARY äº‹ä»¶ä¸­çš„ token ç»Ÿè®¡
        self.summary_prompt_tokens: int = 0
        self.summary_completion_tokens: int = 0
        self.summary_total_tokens: int = 0
        # ç”¨äºè®¡ç®—å·¥å…·è°ƒç”¨è€—æ—¶
        self._pending_tool_starts: dict[str, float] = {}

    def on_event(self, event: ToolCallEvent) -> None:
        """å¼•æ“äº‹ä»¶å›è°ƒï¼šå®æ—¶æ¸²æŸ“ + æ”¶é›†æ—¥å¿—ã€‚"""
        # å®æ—¶æ¸²æŸ“åˆ°ç»ˆç«¯ï¼ˆå’Œ CLI ä¸€æ ·çš„æ•ˆæœï¼‰
        if self._render_enabled:
            self._renderer.handle_event(event)

        # åŒæ—¶æ”¶é›†åˆ°æ—¥å¿—
        if event.event_type == EventType.THINKING:
            if event.thinking and event.thinking.strip():
                self.thinking_log.append(event.thinking.strip())

        elif event.event_type == EventType.TOOL_CALL_START:
            # è®°å½•å¼€å§‹æ—¶é—´
            key = f"{event.tool_name}_{event.iteration}_{len(self.tool_calls)}"
            self._pending_tool_starts[key] = time.monotonic()

        elif event.event_type == EventType.TOOL_CALL_END:
            # è®¡ç®—è€—æ—¶
            key_prefix = f"{event.tool_name}_{event.iteration}_"
            duration_ms = 0.0
            for k in list(self._pending_tool_starts.keys()):
                if k.startswith(key_prefix):
                    start_time = self._pending_tool_starts.pop(k)
                    duration_ms = (time.monotonic() - start_time) * 1000
                    break

            self.tool_calls.append(ToolCallLog(
                tool_name=event.tool_name,
                arguments=dict(event.arguments) if event.arguments else {},
                success=event.success,
                result=event.result or "",
                error=event.error,
                iteration=event.iteration,
                duration_ms=duration_ms,
            ))

        elif event.event_type == EventType.ROUTE_END:
            self.route_mode = event.route_mode
            self.skills_used = list(event.skills_used)
            self.tool_scope = list(event.tool_scope)

        elif event.event_type in {
            EventType.SUBAGENT_START,
            EventType.SUBAGENT_SUMMARY,
            EventType.SUBAGENT_END,
        }:
            self.subagent_events.append({
                "event_type": event.event_type.value,
                "name": event.subagent_name,
                "reason": event.subagent_reason,
                "summary": event.subagent_summary,
                "success": event.subagent_success,
                "iterations": event.subagent_iterations,
                "tool_calls": event.subagent_tool_calls,
            })

        elif event.event_type == EventType.CHAT_SUMMARY:
            self.summary_prompt_tokens = event.prompt_tokens
            self.summary_completion_tokens = event.completion_tokens
            self.summary_total_tokens = event.total_tokens

        elif event.event_type == EventType.TASK_LIST_CREATED:
            self.task_events.append({
                "event_type": event.event_type.value,
                "task_list_data": event.task_list_data,
            })

        elif event.event_type == EventType.TASK_ITEM_UPDATED:
            self.task_events.append({
                "event_type": event.event_type.value,
                "task_index": event.task_index,
                "task_status": event.task_status,
                "task_result": event.task_result,
            })

        elif event.event_type == EventType.USER_QUESTION:
            self.question_events.append({
                "event_type": event.event_type.value,
                "question_id": event.question_id,
                "question_header": event.question_header,
                "question_text": event.question_text,
            })

        elif event.event_type == EventType.PENDING_APPROVAL:
            self.approval_events.append({
                "event_type": event.event_type.value,
                "approval_id": event.approval_id,
                "approval_tool_name": event.approval_tool_name,
            })

    def snapshot_and_reset(self) -> dict[str, Any]:
        """å¿«ç…§å½“å‰æ”¶é›†çš„æ•°æ®å¹¶é‡ç½®ï¼Œç”¨äºå¤šè½®å¯¹è¯åˆ†è½®è®°å½•ã€‚

        è¿”å›æœ¬è½®æ”¶é›†åˆ°çš„æ‰€æœ‰æ•°æ®å‰¯æœ¬ï¼Œç„¶åæ¸…ç©ºå†…éƒ¨çŠ¶æ€ã€‚
        """
        snapshot = {
            "thinking_log": list(self.thinking_log),
            "tool_calls": list(self.tool_calls),
            "route_mode": self.route_mode,
            "skills_used": list(self.skills_used),
            "tool_scope": list(self.tool_scope),
            "subagent_events": list(self.subagent_events),
            "task_events": list(self.task_events),
            "question_events": list(self.question_events),
            "approval_events": list(self.approval_events),
            "summary_prompt_tokens": self.summary_prompt_tokens,
            "summary_completion_tokens": self.summary_completion_tokens,
            "summary_total_tokens": self.summary_total_tokens,
        }
        # é‡ç½®
        self.thinking_log = []
        self.tool_calls = []
        self.route_mode = ""
        self.skills_used = []
        self.tool_scope = []
        self.subagent_events = []
        self.task_events = []
        self.question_events = []
        self.approval_events = []
        self.summary_prompt_tokens = 0
        self.summary_completion_tokens = 0
        self.summary_total_tokens = 0
        self._pending_tool_starts.clear()
        return snapshot


# â”€â”€ LLM è°ƒç”¨æ‹¦æˆªå™¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _serialize_message(msg: dict[str, Any]) -> dict[str, Any]:
    """å°†å•æ¡æ¶ˆæ¯åºåˆ—åŒ–ä¸ºå¯ JSON åŒ–çš„å­—å…¸ã€‚"""
    out: dict[str, Any] = {}
    for key, value in msg.items():
        if value is None:
            out[key] = None
        elif isinstance(value, (str, int, float, bool)):
            out[key] = value
        elif isinstance(value, list):
            out[key] = value
        elif isinstance(value, dict):
            out[key] = value
        else:
            out[key] = str(value)
    return out


def _serialize_tool_call_obj(tc: Any) -> dict[str, Any]:
    """å°† LLM å“åº”ä¸­çš„ tool_call å¯¹è±¡åºåˆ—åŒ–ã€‚"""
    function = getattr(tc, "function", None)
    return {
        "id": getattr(tc, "id", ""),
        "type": getattr(tc, "type", "function"),
        "function": {
            "name": getattr(function, "name", "") if function else "",
            "arguments": getattr(function, "arguments", "") if function else "",
        },
    }


def _serialize_llm_response(response: Any) -> dict[str, Any]:
    """å°† LLM API å®Œæ•´å“åº”åºåˆ—åŒ–ä¸ºå¯ JSON åŒ–çš„å­—å…¸ã€‚"""
    choice = response.choices[0] if response.choices else None
    message = choice.message if choice else None

    result: dict[str, Any] = {}

    if message is not None:
        result["content"] = message.content
        result["role"] = getattr(message, "role", "assistant")

        # æå– thinking / reasoning å†…å®¹
        for thinking_key in ("thinking", "reasoning", "reasoning_content"):
            val = getattr(message, thinking_key, None)
            if val:
                result["thinking"] = str(val)
                break

        # åºåˆ—åŒ– tool_calls
        if message.tool_calls:
            result["tool_calls"] = [
                _serialize_tool_call_obj(tc) for tc in message.tool_calls
            ]

    # finish_reason
    if choice is not None:
        result["finish_reason"] = getattr(choice, "finish_reason", None)

    # token ä½¿ç”¨
    usage = getattr(response, "usage", None)
    if usage is not None:
        result["usage"] = {
            "prompt_tokens": getattr(usage, "prompt_tokens", 0),
            "completion_tokens": getattr(usage, "completion_tokens", 0),
            "total_tokens": getattr(usage, "total_tokens", 0),
        }

    return result


class _StreamRecorder:
    """åŒ…è£…å¼‚æ­¥æµå¼å“åº”ï¼Œé€ä¼  chunk åŒæ—¶ç´¯è®¡ call çº§æŒ‡æ ‡ã€‚

    åœ¨ stream æ¶ˆè´¹å®Œæ¯•åï¼Œå°†ç´¯è®¡çš„ finish_reason / usage
    å›å†™åˆ° call_record["response"]ï¼Œä½¿ run_*.json ä¸­æ¯ä¸ª llm_call
    éƒ½å…·å¤‡å®Œæ•´çš„è§‚æµ‹æ•°æ®ã€‚
    """

    def __init__(self, stream: Any, call_record: dict[str, Any]) -> None:
        self._stream = stream
        self._call_record = call_record
        # ç´¯è®¡æŒ‡æ ‡
        self._finish_reason: str | None = None
        self._usage: dict[str, int] | None = None

    def __aiter__(self):  # noqa: D105
        return self

    async def __anext__(self):  # noqa: D105
        try:
            chunk = await self._stream.__anext__()
        except StopAsyncIteration:
            self._finalize()
            raise

        # â”€â”€ ä» chunk ä¸­æå–æŒ‡æ ‡ â”€â”€

        # openai ChatCompletionChunk æ ¼å¼è§£æ
        choices = getattr(chunk, "choices", None)
        if choices:
            fr = getattr(choices[0], "finish_reason", None)
            if fr:
                self._finish_reason = fr

        # è‡ªå®šä¹‰ provider _StreamDelta æ ¼å¼
        if hasattr(chunk, "content_delta"):
            if getattr(chunk, "finish_reason", None):
                self._finish_reason = chunk.finish_reason

        # usageï¼ˆé€šå¸¸åœ¨æœ€åä¸€ä¸ª chunkï¼‰
        chunk_usage = getattr(chunk, "usage", None)
        if chunk_usage is not None:
            self._usage = {
                "prompt_tokens": getattr(chunk_usage, "prompt_tokens", 0),
                "completion_tokens": getattr(chunk_usage, "completion_tokens", 0),
                "total_tokens": getattr(chunk_usage, "total_tokens", 0),
            }

        return chunk

    # æ”¯æŒ async for ... ä»¥å¤–çš„ aclose è°ƒç”¨ï¼ˆå¦‚æå‰ä¸­æ–­ï¼‰
    async def aclose(self):  # noqa: D102
        close_fn = getattr(self._stream, "aclose", None)
        if close_fn:
            await close_fn()
        self._finalize()

    def _finalize(self) -> None:
        """å°†ç´¯è®¡æŒ‡æ ‡å†™å…¥ call_recordã€‚"""
        resp: dict[str, Any] = {"_stream": True}
        if self._finish_reason is not None:
            resp["finish_reason"] = self._finish_reason
        if self._usage is not None:
            resp["usage"] = self._usage
        self._call_record["response"] = resp


class _LLMCallInterceptor:
    """æ‹¦æˆª engine çš„ LLM API è°ƒç”¨ï¼Œè®°å½•å®Œæ•´çš„è¯·æ±‚å’Œå“åº”ã€‚

    é€šè¿‡ monkey-patch engine._client.chat.completions.create å®ç°ï¼Œ
    æ— éœ€ä¿®æ”¹ engine æºä»£ç ã€‚
    """

    def __init__(self, engine: AgentEngine) -> None:
        self.calls: list[dict[str, Any]] = []
        self._engine = engine
        if not hasattr(engine, "_client"):
            raise AttributeError(
                "bench requires engine._client (openai AsyncOpenAI client); "
                "engine may have been refactored"
            )
        self._original_create = engine._client.chat.completions.create
        # çŒ´å­è¡¥ä¸ï¼šæ‹¦æˆª LLM API è°ƒç”¨
        engine._client.chat.completions.create = self._intercepted_create

    async def _intercepted_create(self, **kwargs: Any) -> Any:
        """æ‹¦æˆª LLM API è°ƒç”¨ï¼Œè®°å½•è¯·æ±‚å’Œå“åº”ã€‚"""
        call_record: dict[str, Any] = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "request": {
                "model": kwargs.get("model"),
                "messages": [
                    _serialize_message(m) for m in kwargs.get("messages", [])
                ],
            },
        }

        # è®°å½• tools å®šä¹‰ï¼ˆä»…åç§°åˆ—è¡¨ï¼Œå®Œæ•´ schema å¤ªå¤§ï¼‰
        tools = kwargs.get("tools")
        if tools:
            call_record["request"]["tool_names"] = [
                t.get("function", {}).get("name", "")
                for t in tools
                if isinstance(t, dict)
            ]

        is_stream = bool(kwargs.get("stream"))

        start = time.monotonic()
        try:
            response = await self._original_create(**kwargs)
        except Exception as exc:
            call_record["error"] = str(exc)
            call_record["duration_ms"] = round(
                (time.monotonic() - start) * 1000, 1
            )
            self.calls.append(call_record)
            raise

        call_record["duration_ms"] = round(
            (time.monotonic() - start) * 1000, 1
        )

        if is_stream:
            # å…ˆ appendï¼Œ_StreamRecorder æ¶ˆè´¹å®Œæ¯•åå›å†™ response
            call_record["response"] = {"_stream": True}
            self.calls.append(call_record)
            return _StreamRecorder(response, call_record)
        else:
            call_record["response"] = _serialize_llm_response(response)
            self.calls.append(call_record)
            return response

    def restore(self) -> None:
        """æ¢å¤åŸå§‹çš„ create æ–¹æ³•ã€‚"""
        self._engine._client.chat.completions.create = self._original_create


class _EngineTracer:
    """æ‹¦æˆª engine å…³é”®æ–¹æ³•ï¼Œè®°å½•ç¨‹åºå‘ agent æ³¨å…¥çš„æŒ‡ä»¤å’Œå†…éƒ¨å†³ç­–ã€‚

    é€šè¿‡ monkey-patch ä»¥ä¸‹æ–¹æ³•å®ç°ï¼š
    - ``_prepare_system_prompts_for_request`` â†’ è®°å½•æ¯è½®æ³¨å…¥çš„ç³»ç»Ÿæç¤ºï¼ˆåˆ†è§£å„ç»„ä»¶ï¼‰
    - ``_enrich_tool_result_with_window_perception`` â†’ è®°å½•çª—å£æ„ŸçŸ¥å¢å¼ºå‰åå¯¹æ¯”

    é€šè¿‡ç¯å¢ƒå˜é‡ ``EXCELMANUS_BENCH_TRACE=0`` æˆ– CLI ``--no-trace`` ç¦ç”¨ã€‚
    """

    def __init__(self, engine: AgentEngine) -> None:
        self.entries: list[dict[str, Any]] = []
        self._engine = engine
        self._iteration = 0
        # label â†’ (first_seen_iter, content_hash) â€” ç”¨äºæŠ˜å ä¸å˜çš„ component
        self._component_seen: dict[str, tuple[int, int]] = {}

        # ä¿å­˜åŸå§‹æ–¹æ³•
        if not hasattr(engine, "_context_builder"):
            raise AttributeError(
                "bench requires engine._context_builder; engine may have been refactored"
            )
        if not hasattr(engine, "_enrich_tool_result_with_window_perception"):
            raise AttributeError(
                "bench requires engine._enrich_tool_result_with_window_perception; engine may have been refactored"
            )
        self._orig_prepare = engine._context_builder._prepare_system_prompts_for_request
        self._orig_enrich = engine._enrich_tool_result_with_window_perception

        # çŒ´å­è¡¥ä¸ï¼šæ‹¦æˆªç³»ç»Ÿæç¤ºå’Œçª—å£æ„ŸçŸ¥æ–¹æ³•
        engine._context_builder._prepare_system_prompts_for_request = self._traced_prepare  # type: ignore[assignment]
        engine._enrich_tool_result_with_window_perception = self._traced_enrich  # type: ignore[assignment]

    def _traced_prepare(
        self, skill_contexts: list[str], **kwargs: Any,
    ) -> tuple[list[str], str | None]:
        """æ‹¦æˆªç³»ç»Ÿæç¤ºæ„å»ºï¼Œè®°å½•å„ç»„ä»¶å†…å®¹ã€‚

        å¯¹è·¨è½®æ¬¡å†…å®¹å®Œå…¨ä¸å˜çš„ componentï¼Œçœç•¥ ``content`` å­—æ®µï¼Œ
        æ”¹ä¸ºè®°å½• ``same_as_iter`` æŒ‡å‘é¦–æ¬¡å‡ºç°çš„è¿­ä»£è½®æ¬¡ï¼Œé¿å…é‡å¤å­˜å‚¨ã€‚
        """
        self._iteration += 1
        prompts, error = self._orig_prepare(skill_contexts, **kwargs)

        # åˆ†è§£è®°å½•å„ç»„ä»¶
        components: list[dict[str, Any]] = []
        for idx, prompt in enumerate(prompts):
            label = "base_system_prompt" if idx == 0 else f"context_{idx}"
            # å°è¯•è¯†åˆ«ç»„ä»¶ç±»å‹
            if idx > 0:
                snippet = prompt[:200]
                if "çª—å£æ„ŸçŸ¥" in snippet or "Window Perception" in snippet:
                    label = "window_perception_notice"
                elif "æƒé™æç¤º" in snippet or "fullAccess" in snippet:
                    label = "access_notice"
                elif "MCP" in snippet:
                    label = "mcp_context_notice"
                elif "Hook" in snippet:
                    label = "hook_context"
                elif "è®¡åˆ’" in snippet and "å·²æ‰¹å‡†" in snippet:
                    label = "approved_plan_context"
                else:
                    label = f"skill_context_{idx}"

            content_hash = hash(prompt)
            prev = self._component_seen.get(label)
            if prev is not None and prev[1] == content_hash:
                # å†…å®¹ä¸é¦–æ¬¡å‡ºç°å®Œå…¨ç›¸åŒ â€” æŠ˜å ï¼Œä¸é‡å¤å­˜ content
                comp: dict[str, Any] = {
                    "label": label,
                    "char_count": len(prompt),
                    "same_as_iter": prev[0],
                }
            else:
                # é¦–æ¬¡å‡ºç°æˆ–å†…å®¹å·²å˜åŒ– â€” å®Œæ•´è®°å½•
                self._component_seen[label] = (self._iteration, content_hash)
                comp = {
                    "label": label,
                    "char_count": len(prompt),
                    "content": prompt[:_TRACE_SYSTEM_PROMPT_MAX_CHARS],
                    "truncated": len(prompt) > _TRACE_SYSTEM_PROMPT_MAX_CHARS,
                }
            components.append(comp)

        self.entries.append({
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "event": "system_prompts_injected",
            "iteration": self._iteration,
            "data": {
                "prompt_count": len(prompts),
                "total_chars": sum(len(p) for p in prompts),
                "skill_context_count": len(skill_contexts),
                "context_error": error,
                "components": components,
            },
        })
        return prompts, error

    def _traced_enrich(
        self,
        *,
        tool_name: str,
        arguments: dict[str, Any],
        result_text: str,
        success: bool,
    ) -> str:
        """æ‹¦æˆªçª—å£æ„ŸçŸ¥å¢å¼ºï¼Œè®°å½•å‰åå¯¹æ¯”ã€‚"""
        enriched = self._orig_enrich(
            tool_name=tool_name,
            arguments=arguments,
            result_text=result_text,
            success=success,
        )
        # ä»…åœ¨å†…å®¹å®é™…è¢«å¢å¼ºæ—¶è®°å½•
        if enriched != result_text:
            self.entries.append({
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "event": "window_perception_enrichment",
                "iteration": self._iteration,
                "data": {
                    "tool_name": tool_name,
                    "original_chars": len(result_text),
                    "enriched_chars": len(enriched),
                    "added_chars": len(enriched) - len(result_text),
                    "enriched_suffix": enriched[len(result_text):][:2000],
                },
            })
        return enriched

    def snapshot_and_reset(self) -> list[dict[str, Any]]:
        """å¿«ç…§å½“å‰ trace æ•°æ®å¹¶é‡ç½®ï¼Œç”¨äºå¤šè½®åˆ†è½®è®°å½•ã€‚

        æ³¨æ„ï¼š``_iteration`` ä¸é‡ç½®ï¼ˆä¿æŒå•è°ƒé€’å¢ï¼‰ï¼Œç¡®ä¿ ``same_as_iter``
        å¼•ç”¨åœ¨æ•´ä¸ª engine ç”Ÿå‘½å‘¨æœŸå†…å§‹ç»ˆæœ‰æ•ˆã€‚``_component_seen`` åŒæ ·ä¿ç•™ï¼Œ
        è·¨è½®æ¬¡å†…å®¹ä¸å˜çš„ component ç»§ç»­æŠ˜å ã€‚
        """
        snapshot = list(self.entries)
        self.entries = []
        return snapshot

    def restore(self) -> None:
        """æ¢å¤åŸå§‹æ–¹æ³•ã€‚"""
        self._engine._context_builder._prepare_system_prompts_for_request = self._orig_prepare  # type: ignore[assignment]
        self._engine._enrich_tool_result_with_window_perception = self._orig_enrich  # type: ignore[assignment]


# â”€â”€ æ‰§è¡Œå™¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _create_engine(config: ExcelManusConfig) -> AgentEngine:
    """åˆ›å»ºç‹¬ç«‹çš„ AgentEngine å®ä¾‹ï¼ˆä¸å¤ç”¨ sessionï¼‰ã€‚

    è‡ªåŠ¨å¯ç”¨ bench sandbox æ¨¡å¼ï¼Œè§£é™¤æ‰€æœ‰äº¤äº’å¼é˜»å¡
    ï¼ˆfullAccess / plan æ‹¦æˆª / ç¡®è®¤é—¨ç¦ï¼‰ã€‚
    """
    registry = ToolRegistry()
    registry.register_builtin_tools(config.workspace_root)
    loader = SkillpackLoader(config, registry)
    loader.load_all()
    router = SkillRouter(config, loader)
    engine = AgentEngine(
        config=config,
        registry=registry,
        skill_router=router,
    )
    engine.enable_bench_sandbox()
    return engine


def _dump_conversation_messages(
    engine: AgentEngine,
    interceptor: _LLMCallInterceptor | None = None,
) -> list[dict[str, Any]]:
    """å¯¼å‡ºå®Œæ•´å¯¹è¯æ¶ˆæ¯å¿«ç…§ï¼Œåæ˜ å®é™…å‘é€ç»™ LLM çš„è¯·æ±‚å†…å®¹ã€‚

    å½“ interceptor æœ‰è°ƒç”¨è®°å½•æ—¶ï¼Œä½¿ç”¨æœ€åä¸€æ¬¡è¯·æ±‚çš„ messagesï¼ˆå®Œå…¨å‡†ç¡®ï¼Œ
    åŒ…å«æ‰€æœ‰åŠ¨æ€æ³¨å…¥çš„ system promptsï¼šfile_structure_previewã€
    skill_contextã€window_perception ç­‰ï¼‰ã€‚
    å¦åˆ™å›é€€åˆ° memory.get_messages()ï¼ˆä»…å«é™æ€ base system promptï¼‰ã€‚
    """
    try:
        if interceptor is not None and interceptor.calls:
            last_messages = interceptor.calls[-1]["request"].get("messages", [])
            if last_messages:
                return list(last_messages)
        messages = engine.memory.get_messages()
        return [_serialize_message(m) for m in messages]
    except Exception:
        return []


# â”€â”€ æ–‡ä»¶éš”ç¦» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ä» message æ–‡æœ¬ä¸­è‡ªåŠ¨æå–æ–‡ä»¶è·¯å¾„çš„æ­£åˆ™ï¼ˆæ”¯æŒ .xlsx / .csv / .xlsï¼‰
_FILE_PATH_RE = re.compile(
    r"""(?:^|[\s"'(])"""           # å‰å¯¼ï¼šè¡Œé¦– / ç©ºç™½ / å¼•å· / æ‹¬å·
    r"""((?:[\w./\\-]+/)*"""       # ç›®å½•éƒ¨åˆ†
    r"""[\w.()-]+"""               # æ–‡ä»¶å
    r"""\.(?:xlsx|csv|xls))""",    # æ‰©å±•å
    re.IGNORECASE,
)


def _extract_file_paths(text: str) -> list[str]:
    """ä»æ–‡æœ¬ä¸­æå–å¯èƒ½çš„æ–‡ä»¶è·¯å¾„ã€‚"""
    return list(dict.fromkeys(_FILE_PATH_RE.findall(text)))


def _isolate_source_files(
    case: BenchCase,
    workdir: Path,
) -> list[str]:
    """å°† case å¼•ç”¨çš„æºæ–‡ä»¶å¤åˆ¶åˆ°å·¥ä½œç›®å½•ï¼Œè¿”å›æ›¿æ¢åçš„ messagesã€‚

    ä¼˜å…ˆä½¿ç”¨ case.source_filesï¼ˆæ˜¾å¼å£°æ˜ï¼‰ï¼Œå¦åˆ™ä» messages ä¸­
    è‡ªåŠ¨æå–æ–‡ä»¶è·¯å¾„ä½œä¸º fallbackã€‚

    å¤åˆ¶åå°† messages ä¸­çš„åŸå§‹è·¯å¾„æ›¿æ¢ä¸ºå‰¯æœ¬è·¯å¾„ã€‚
    """
    messages = list(case.messages) if case.messages else [case.message]

    # æ”¶é›†éœ€è¦éš”ç¦»çš„æ–‡ä»¶è·¯å¾„
    source_files = list(case.source_files) if case.source_files else []
    if not source_files:
        for msg in messages:
            source_files.extend(_extract_file_paths(msg))
        # å»é‡ä¿åº
        source_files = list(dict.fromkeys(source_files))

    if not source_files:
        return messages

    workdir.mkdir(parents=True, exist_ok=True)

    # å¤åˆ¶æ–‡ä»¶å¹¶æ„å»ºè·¯å¾„æ˜ å°„
    path_map: dict[str, str] = {}
    for src_path_str in source_files:
        src = Path(src_path_str)
        if not src.exists():
            logger.warning("æºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡éš”ç¦»: %s", src)
            continue
        dst = workdir / src.name
        shutil.copy2(src, dst)
        path_map[src_path_str] = str(dst)
        logger.debug("éš”ç¦»å¤åˆ¶: %s â†’ %s", src, dst)

    if not path_map:
        return messages

    # æ›¿æ¢ messages ä¸­çš„è·¯å¾„ï¼ˆæŒ‰è·¯å¾„é•¿åº¦é™åºæ›¿æ¢ï¼Œé¿å…çŸ­è·¯å¾„è¯¯åŒ¹é…é•¿è·¯å¾„çš„å­ä¸²ï¼‰
    sorted_paths = sorted(path_map.keys(), key=len, reverse=True)
    replaced: list[str] = []
    for msg in messages:
        for old_path in sorted_paths:
            msg = msg.replace(old_path, path_map[old_path])
        replaced.append(msg)

    logger.info("æ–‡ä»¶éš”ç¦»å®Œæˆ: %d ä¸ªæ–‡ä»¶ â†’ %s", len(path_map), workdir)
    return replaced


async def run_case(
    case: BenchCase,
    config: ExcelManusConfig,
    *,
    render_enabled: bool = True,
    trace_enabled: bool = True,
    output_dir: Path | None = None,
    suite_name: str = "",
) -> BenchResult:
    """æ‰§è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œè¿”å›å®Œæ•´ç»“æœï¼ˆå«å®Œæ•´ LLM äº¤äº’æ—¥å¿—ï¼‰ã€‚

    æ”¯æŒå¤šè½®å¯¹è¯ï¼šå½“ case.messages åŒ…å«å¤šæ¡æ¶ˆæ¯æ—¶ï¼Œä¾æ¬¡å‘é€ç»™åŒä¸€ä¸ª
    engine å®ä¾‹ï¼ŒConversationMemory è‡ªç„¶ä¿æŒä¸Šä¸‹æ–‡ã€‚

    Args:
        trace_enabled: å¯ç”¨ engine å†…éƒ¨äº¤äº’è½¨è¿¹è®°å½•ï¼ˆç³»ç»Ÿæç¤ºæ³¨å…¥ã€
            çª—å£æ„ŸçŸ¥å¢å¼ºã€å·¥å…·èŒƒå›´å†³ç­–ç­‰ï¼‰ã€‚é»˜è®¤å¼€å¯ï¼Œå¯é€šè¿‡ ``--no-trace`` æˆ–
            ``EXCELMANUS_BENCH_TRACE=0`` ç¦ç”¨ã€‚
        output_dir: æ—¥å¿—è¾“å‡ºç›®å½•ï¼Œç”¨äºæ„å»ºæ–‡ä»¶éš”ç¦»å·¥ä½œç›®å½•ã€‚
    """
    engine = _create_engine(config)
    collector = _EventCollector(render_enabled=render_enabled)
    interceptor = _LLMCallInterceptor(engine)
    tracer: _EngineTracer | None = None
    if trace_enabled:
        try:
            tracer = _EngineTracer(engine)
        except AttributeError as exc:
            logger.debug("trace å·²é™çº§ï¼šengine ä¸æ”¯æŒå®Œæ•´ tracer é’©å­ï¼ˆ%sï¼‰", exc)
    timestamp = datetime.now(timezone.utc).isoformat()

    # æ–‡ä»¶éš”ç¦»ï¼šå°†æºæ–‡ä»¶å¤åˆ¶åˆ°å·¥ä½œç›®å½•ï¼Œæ›¿æ¢ messages ä¸­çš„è·¯å¾„
    if output_dir is not None:
        workdir = output_dir / "workfiles" / (suite_name or "adhoc") / case.id
        messages = _isolate_source_files(case, workdir)
    else:
        messages = list(case.messages) if case.messages else [case.message]
    # SpreadsheetBench å†™å…¥å¼•å¯¼ï¼šå¯¹ spreadsheetbench ç”¨ä¾‹è¿½åŠ æ¸©å’Œçš„å†™å…¥æç¤º
    if case.tags and "spreadsheetbench" in case.tags:
        _sb_hint = (
            "\n\nPlease implement your solution by writing the formula or values "
            "directly into the file, then verify the result."
        )
        messages = [m + _sb_hint if i == 0 else m for i, m in enumerate(messages)]

    is_multi_turn = len(messages) > 1

    logger.info(
        "â–¶ å¼€å§‹æ‰§è¡Œç”¨ä¾‹: %s (%s) [%d è½®]",
        case.id, case.name, len(messages),
    )
    case_start = time.monotonic()

    # ç´¯è®¡ç»Ÿè®¡
    all_turns: list[TurnResult] = []
    all_tool_calls: list[ToolCallLog] = []
    all_thinking_log: list[str] = []
    all_subagent_events: list[dict[str, Any]] = []
    total_iterations = 0
    total_prompt_tokens = 0
    total_completion_tokens = 0
    total_total_tokens = 0
    last_reply = ""
    last_route_mode = ""
    last_skills_used: list[str] = []
    last_tool_scope: list[str] = []
    case_status = "ok"
    case_error: dict[str, Any] | None = None

    # è‡ªåŠ¨å›å¤é˜Ÿåˆ—ï¼ˆç”¨äº ask_user è‡ªåŠ¨åº”ç­”ï¼‰
    _AUTO_REPLY_DEFAULT = "1"
    _MAX_AUTO_REPLY_ROUNDS = 10
    auto_reply_queue = list(case.auto_replies)
    auto_reply_count = 0
    chat_result: ChatResult | None = None  # å®‰å…¨é»˜è®¤å€¼ï¼Œé¿å…å¼‚å¸¸è·¯å¾„ UnboundLocalError

    try:
        for turn_idx, msg in enumerate(messages):
            if is_multi_turn:
                logger.info(
                    "  â”€â”€ è½®æ¬¡ %d/%d â”€â”€", turn_idx + 1, len(messages),
                )

            # æ¯è½®å¼€å§‹å‰è®°å½• interceptor çš„è°ƒç”¨æ•°ï¼Œç”¨äºåˆ‡åˆ†æœ¬è½®çš„ llm_calls
            llm_calls_before = len(interceptor.calls)
            turn_start = time.monotonic()

            try:
                chat_result: ChatResult = await engine.chat(
                    msg,
                    on_event=collector.on_event,
                )
            except Exception as exc:
                logger.error(
                    "ç”¨ä¾‹ %s è½®æ¬¡ %d æ‰§è¡Œå¼‚å¸¸: %s",
                    case.id, turn_idx, exc, exc_info=True,
                )
                turn_elapsed = time.monotonic() - turn_start
                # å¿«ç…§æœ¬è½®æ”¶é›†å™¨æ•°æ®
                snap = collector.snapshot_and_reset()
                turn_llm_calls = list(interceptor.calls[llm_calls_before:])

                turn_result = TurnResult(
                    turn_index=turn_idx,
                    message=msg,
                    reply=f"[ERROR] {exc}",
                    duration_seconds=turn_elapsed,
                    iterations=0,
                    route_mode=snap["route_mode"] or "error",
                    skills_used=snap["skills_used"],
                    tool_scope=snap["tool_scope"],
                    tool_calls=snap["tool_calls"],
                    thinking_log=snap["thinking_log"],
                    subagent_events=snap["subagent_events"],
                    llm_calls=turn_llm_calls,
                    status="error",
                    error={
                        "type": type(exc).__name__,
                        "message": str(exc),
                    },
                    engine_trace=(
                        tracer.snapshot_and_reset() if tracer else []
                    ) if is_multi_turn else [],
                    active_model=engine.current_model,
                    task_events=snap["task_events"],
                    question_events=snap["question_events"],
                    approval_events=snap["approval_events"],
                )
                all_turns.append(turn_result)
                all_tool_calls.extend(snap["tool_calls"])
                all_thinking_log.extend(snap["thinking_log"])
                all_subagent_events.extend(snap["subagent_events"])
                last_reply = f"[ERROR] {exc}"
                case_status = "error"
                case_error = {
                    "type": type(exc).__name__,
                    "message": str(exc),
                }
                # æŸè½®å¼‚å¸¸åä¸­æ­¢åç»­è½®æ¬¡
                break

            # â”€â”€ è‡ªåŠ¨å›å¤ ask_user é—®é¢˜ â”€â”€
            while (
                engine.has_pending_question()
                and auto_reply_count < _MAX_AUTO_REPLY_ROUNDS
            ):
                reply_text = (
                    auto_reply_queue.pop(0)
                    if auto_reply_queue
                    else _AUTO_REPLY_DEFAULT
                )
                auto_reply_count += 1
                pending_q = engine.current_pending_question()
                q_header = getattr(pending_q, "header", "") if pending_q else ""
                logger.info(
                    "  â¤· è‡ªåŠ¨å›å¤ ask_user #%d: %r â†’ %r",
                    auto_reply_count, q_header, reply_text,
                )
                if render_enabled:
                    _console.print(
                        f"  [dim]â¤· è‡ªåŠ¨å›å¤ ask_user:[/dim] {reply_text}"
                    )
                try:
                    chat_result = await engine.chat(
                        reply_text,
                        on_event=collector.on_event,
                    )
                except Exception as exc:
                    logger.warning(
                        "è‡ªåŠ¨å›å¤ ask_user å¼‚å¸¸: %s", exc, exc_info=True,
                    )
                    break

            turn_elapsed = time.monotonic() - turn_start
            # å¿«ç…§æœ¬è½®æ”¶é›†å™¨æ•°æ®
            snap = collector.snapshot_and_reset()
            turn_llm_calls = list(interceptor.calls[llm_calls_before:])

            # å›é€€è·¯ç”±ä¿¡æ¯
            last_route = getattr(engine, "last_route_result", None)
            fallback_route_mode = getattr(last_route, "route_mode", "")
            fallback_skills = list(getattr(last_route, "skills_used", []) or [])
            fallback_scope = list(getattr(last_route, "tool_scope", []) or [])

            turn_route_mode = snap["route_mode"] or fallback_route_mode or "unknown"
            turn_skills = snap["skills_used"] or fallback_skills
            turn_scope = snap["tool_scope"] or fallback_scope

            turn_result = TurnResult(
                turn_index=turn_idx,
                message=msg,
                reply=chat_result.reply,
                duration_seconds=turn_elapsed,
                iterations=chat_result.iterations,
                route_mode=turn_route_mode,
                skills_used=turn_skills,
                tool_scope=turn_scope,
                tool_calls=snap["tool_calls"],
                thinking_log=snap["thinking_log"],
                subagent_events=snap["subagent_events"],
                llm_calls=turn_llm_calls,
                prompt_tokens=chat_result.prompt_tokens,
                completion_tokens=chat_result.completion_tokens,
                total_tokens=chat_result.total_tokens,
                engine_trace=(
                    tracer.snapshot_and_reset() if tracer else []
                ) if is_multi_turn else [],
                active_model=engine.current_model,
                task_events=snap["task_events"],
                question_events=snap["question_events"],
                approval_events=snap["approval_events"],
                reasoning_metrics=getattr(chat_result, "reasoning_metrics", {}),
            )
            all_turns.append(turn_result)

            # ç´¯è®¡
            all_tool_calls.extend(snap["tool_calls"])
            all_thinking_log.extend(snap["thinking_log"])
            all_subagent_events.extend(snap["subagent_events"])
            total_iterations += chat_result.iterations
            total_prompt_tokens += chat_result.prompt_tokens
            total_completion_tokens += chat_result.completion_tokens
            total_total_tokens += chat_result.total_tokens
            last_reply = chat_result.reply
            last_route_mode = turn_route_mode
            last_skills_used = turn_skills
            last_tool_scope = turn_scope

            # å¤šè½®æ—¶æ‰“å°æ¯è½®å›å¤
            if render_enabled and is_multi_turn and chat_result.reply:
                _console.print()
                _console.print(
                    Panel(
                        Markdown(chat_result.reply),
                        title=f"è½®æ¬¡ {turn_idx + 1}/{len(messages)}",
                        border_style="#5f875f",
                        padding=(1, 2),
                        expand=False,
                    )
                )
    finally:
        interceptor.restore()
        if tracer is not None:
            tracer.restore()

    case_elapsed = time.monotonic() - case_start

    # å•è½®ç”¨ä¾‹çš„ engine_trace ç›´æ¥ä» tracer è·å–ï¼ˆå¤šè½®å·²åœ¨å„ turn ä¸­è®°å½•ï¼‰
    case_engine_trace: list[dict[str, Any]] = []
    if tracer is not None and not is_multi_turn:
        case_engine_trace = tracer.snapshot_and_reset()

    # é‡‡é›† write_hint
    _write_hint = str(getattr(engine, "_current_write_hint", "unknown"))

    # é‡‡é›†å…³é”® config å¿«ç…§
    _config_snapshot = {
        "model": config.model,
        "base_url": config.base_url,
        "aux_model": config.aux_model,
        "aux_base_url": config.aux_base_url,
    }

    result = BenchResult(
        case_id=case.id,
        case_name=case.name,
        message=case.message or (messages[0] if messages else ""),
        timestamp=timestamp,
        duration_seconds=case_elapsed,
        iterations=total_iterations,
        route_mode=last_route_mode or "unknown",
        skills_used=last_skills_used,
        tool_scope=last_tool_scope,
        tool_calls=all_tool_calls,
        thinking_log=all_thinking_log,
        reply=last_reply,
        prompt_tokens=total_prompt_tokens,
        completion_tokens=total_completion_tokens,
        total_tokens=total_total_tokens,
        subagent_events=all_subagent_events,
        llm_calls=interceptor.calls,
        conversation_messages=_dump_conversation_messages(engine, interceptor),
        turns=all_turns if is_multi_turn else [],
        status=case_status,
        error=case_error,
        engine_trace=case_engine_trace,
        active_model=engine.current_model,
        write_hint=_write_hint,
        config_snapshot=_config_snapshot,
        reasoning_metrics=getattr(chat_result, "reasoning_metrics", {}) if chat_result is not None else {},
    )

    # å•è½®æ—¶æ‰“å°æœ€ç»ˆå›å¤ï¼ˆå¤šè½®å·²åœ¨å¾ªç¯ä¸­é€è½®æ‰“å°ï¼‰
    if render_enabled and not is_multi_turn and result.reply:
        _console.print()
        _console.print(
            Panel(
                Markdown(result.reply),
                border_style="#5f875f",
                padding=(1, 2),
                expand=False,
            )
        )

    failures = sum(1 for tc in result.tool_calls if not tc.success)
    turn_info = f" ({len(messages)} è½®)" if is_multi_turn else ""
    auto_reply_info = f" â”‚ {auto_reply_count} æ¬¡è‡ªåŠ¨å›å¤" if auto_reply_count else ""
    logger.info(
        "âœ“ ç”¨ä¾‹ %s å®Œæˆ%s: %d è¿­ä»£ â”‚ %d å·¥å…·è°ƒç”¨(å¤±è´¥%d) â”‚ %d tokens â”‚ %.1fs â”‚ %d æ¬¡ LLM è°ƒç”¨%s",
        case.id,
        turn_info,
        result.iterations,
        len(result.tool_calls),
        failures,
        result.total_tokens,
        result.duration_seconds,
        len(result.llm_calls),
        auto_reply_info,
    )
    return result


def _load_suite(path: str | Path) -> tuple[str, list[BenchCase], bool]:
    """ä» JSON æ–‡ä»¶åŠ è½½æµ‹è¯•å¥—ä»¶ã€‚

    å…¼å®¹ä¸¤ç§ case æ ¼å¼ï¼š
    - å•è½®ï¼š``{"message": "..."}``
    - å¤šè½®ï¼š``{"messages": ["...", "..."]}``
    åŠ è½½æ—¶ç»Ÿä¸€å½’ä¸€åŒ–ä¸º messages åˆ—è¡¨ã€‚

    suite çº§ ``assertions`` ä¼šä¸æ¯ä¸ª case çš„ ``assertions`` åˆå¹¶
    ï¼ˆcase çº§è¦†ç›– suite çº§åŒåå­—æ®µï¼‰ã€‚

    è¿”å› (suite_name, cases, trace)ã€‚
    """
    with open(path, encoding="utf-8") as f:
        data = json.load(f)

    suite_name = data.get("suite_name", Path(path).stem)
    suite_trace = bool(data.get("trace", True))
    suite_assertions = data.get("assertions", {})
    cases: list[BenchCase] = []
    for item in data.get("cases", []):
        # å¤šè½®æ ¼å¼ä¼˜å…ˆ
        raw_messages = item.get("messages")
        raw_message = item.get("message", "")
        if raw_messages and isinstance(raw_messages, list):
            messages = [str(m) for m in raw_messages if m]
            message = messages[0] if messages else ""
        else:
            message = raw_message
            messages = [message] if message else []

        # åˆå¹¶ suite çº§ + case çº§ assertions
        case_assertions = merge_assertions(
            suite_assertions, item.get("assertions"),
        )

        cases.append(BenchCase(
            id=item["id"],
            name=item.get("name", item["id"]),
            message=message,
            messages=messages,
            tags=item.get("tags", []),
            expected=item.get("expected", {}),
            source_files=item.get("source_files", []),
            auto_replies=item.get("auto_replies", []),
            assertions=case_assertions,
        ))
    return suite_name, cases, suite_trace


def _save_result(
    result: BenchResult,
    output_dir: Path,
    assertions: dict[str, Any] | None = None,
    *,
    expected: dict[str, Any] | None = None,
    workfile_dir: Path | None = None,
) -> tuple[Path, ValidationSummary | None]:
    """ä¿å­˜å•ä¸ªç”¨ä¾‹ç»“æœåˆ° JSON æ–‡ä»¶ã€‚

    å¦‚æœæä¾›äº† assertionsï¼Œä¼šè‡ªåŠ¨æ‰§è¡Œæ–­è¨€æ ¡éªŒå¹¶å°†ç»“æœåµŒå…¥è¾“å‡º JSONã€‚
    å½“ expected åŒ…å« golden_file / answer_position æ—¶ï¼Œè‡ªåŠ¨è¿½åŠ  golden_cells æ–­è¨€ã€‚

    Returns:
        (filepath, validation_summary) â€” validation_summary ä»…åœ¨æœ‰ assertions æ—¶é Noneã€‚
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%S")
    short_id = uuid.uuid4().hex[:6]
    filename = f"run_{ts}_{result.case_id}_{short_id}.json"
    filepath = output_dir / filename

    result_dict = result.to_dict()

    # æ‰§è¡Œæ–­è¨€æ ¡éªŒï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œä¿ç•™ç”¨äºå…¼å®¹ï¼‰
    validation: ValidationSummary | None = None
    has_golden = bool(
        expected and expected.get("golden_file") and expected.get("answer_position")
    )
    if assertions or has_golden:
        validation = _validate_result_sync(
            result_dict,
            assertions or {},
            expected=expected,
            workfile_dir=workfile_dir,
        )

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(result_dict, f, ensure_ascii=False, indent=2)
    return filepath, validation


def _validate_result_sync(
    result_dict: dict[str, Any],
    assertions: dict[str, Any],
    *,
    expected: dict[str, Any] | None = None,
    workfile_dir: Path | None = None,
) -> ValidationSummary | None:
    """åŒæ­¥æ‰§è¡Œæ–­è¨€æ ¡éªŒï¼ˆä¸æ¨èï¼Œæ¨èä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ï¼‰ã€‚"""
    has_golden = bool(
        expected and expected.get("golden_file") and expected.get("answer_position")
    )
    if not assertions and not has_golden:
        return None

    validation = validate_case(
        result_dict,
        assertions,
        expected=expected,
        workfile_dir=workfile_dir,
    )
    result_dict["validation"] = validation.to_dict()
    if validation.failed > 0:
        logger.warning(
            "  âš  ç”¨ä¾‹ %s æ–­è¨€æ ¡éªŒ: %d/%d é€šè¿‡ (%d å¤±è´¥)",
            result_dict.get("case_id", "unknown"),
            validation.passed, validation.total, validation.failed,
        )
    else:
        logger.info(
            "  âœ“ ç”¨ä¾‹ %s æ–­è¨€æ ¡éªŒ: %d/%d å…¨éƒ¨é€šè¿‡",
            result_dict.get("case_id", "unknown"),
            validation.passed, validation.total,
        )
    return validation


def _save_suite_summary(
    suite_name: str,
    suite_path: str | Path,
    results: list[BenchResult],
    output_dir: Path,
    *,
    concurrency: int,
    case_log_files: list[Path],
) -> Path:
    """ä¿å­˜å¥—ä»¶æ±‡æ€»ç»“æœã€‚"""
    output_dir.mkdir(parents=True, exist_ok=True)
    ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%S")
    short_id = uuid.uuid4().hex[:6]
    filename = f"suite_{ts}_{short_id}.json"
    filepath = output_dir / filename

    total_tokens = sum(r.total_tokens for r in results)
    total_duration = sum(r.duration_seconds for r in results)
    avg_iterations = (
        sum(r.iterations for r in results) / len(results) if results else 0
    )
    total_prompt_tokens = sum(r.prompt_tokens for r in results)
    total_completion_tokens = sum(r.completion_tokens for r in results)
    total_tool_calls = sum(len(r.tool_calls) for r in results)
    total_tool_failures = sum(
        sum(1 for tc in r.tool_calls if not tc.success) for r in results
    )
    failed_case_ids = [r.case_id for r in results if r.status != "ok"]
    suite_status = "ok" if not failed_case_ids else "completed_with_errors"

    summary = {
        "schema_version": 3,
        "kind": "suite_summary",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "meta": {
            "suite_name": suite_name,
            "suite_path": str(suite_path),
            "case_count": len(results),
        },
        "execution": {
            "concurrency": concurrency,
            "status": suite_status,
        },
        "artifacts": {
            "case_log_files": [str(p) for p in case_log_files],
            "cases": [r.to_dict() for r in results],
        },
        "result": {
            "failed_case_ids": failed_case_ids,
        },
        "stats": {
            "total_tokens": total_tokens,
            "total_prompt_tokens": total_prompt_tokens,
            "total_completion_tokens": total_completion_tokens,
            "total_duration_seconds": round(total_duration, 2),
            "average_iterations": round(avg_iterations, 2),
            "tool_call_count": total_tool_calls,
            "tool_failures": total_tool_failures,
        },
    }

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    return filepath


async def run_suite(
    suite_path: str | Path,
    config: ExcelManusConfig,
    output_dir: Path,
    *,
    concurrency: int = 1,
    trace_enabled: bool = True,
    on_progress: ProgressCallback | None = None,
) -> list[BenchResult]:
    """è¿è¡Œæ•´ä¸ªæµ‹è¯•å¥—ä»¶ã€‚"""
    if concurrency < 1:
        raise ValueError("concurrency å¿…é¡» >= 1")

    suite_name, cases, suite_trace = _load_suite(suite_path)
    # suite JSON ä¸­çš„ trace å­—æ®µä¸å‚æ•°å– OR
    trace_enabled = trace_enabled or suite_trace
    logger.info("â•" * 50)
    logger.info(
        "å¼€å§‹æ‰§è¡Œå¥—ä»¶: %s (%d ä¸ªç”¨ä¾‹, å¹¶å‘=%d%s)",
        suite_name,
        len(cases),
        concurrency,
        ", trace=ON" if trace_enabled else "",
    )
    logger.info("â•" * 50)

    # æ”¶é›†æ¯ä¸ª case çš„ validation ç»“æœï¼ˆç”¨äº suite çº§èšåˆï¼‰
    case_validations: list[tuple[str, ValidationSummary]] = []
    _validations_lock = asyncio.Lock()

    async def _execute_case(
        index: int,
        case: BenchCase,
        *,
        render_enabled: bool,
    ) -> tuple[int, BenchResult, Path]:
        # é€šçŸ¥å¼€å§‹æ‰§è¡Œ
        if on_progress:
            on_progress(case.id, case.name, None)
        try:
            result = await run_case(
                case, config,
                render_enabled=render_enabled,
                trace_enabled=trace_enabled,
                output_dir=output_dir,
                suite_name=suite_name,
            )
        except Exception as exc:  # pragma: no cover - å…œåº•ä¿æŠ¤
            logger.error("ç”¨ä¾‹ %s æ‰§è¡Œå´©æºƒ: %s", case.id, exc, exc_info=True)
            result = BenchResult(
                case_id=case.id,
                case_name=case.name,
                message=case.message,
                timestamp=datetime.now(timezone.utc).isoformat(),
                duration_seconds=0.0,
                iterations=0,
                route_mode="error",
                skills_used=[],
                tool_calls=[],
                thinking_log=[],
                reply=f"[CRASH] {exc}",
                prompt_tokens=0,
                completion_tokens=0,
                total_tokens=0,
                status="error",
                error={
                    "type": type(exc).__name__,
                    "message": str(exc),
                },
            )
        # æ„å»º workfile ç›®å½•è·¯å¾„ï¼ˆä¸ _isolate_source_files ä¸€è‡´ï¼‰
        workfile_dir = output_dir / "workfiles" / (suite_name or "adhoc") / case.id
        
        # å…ˆä¿å­˜ç»“æœï¼ˆä¸å«éªŒè¯ï¼‰
        output_dir.mkdir(parents=True, exist_ok=True)
        ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%S")
        short_id = uuid.uuid4().hex[:6]
        filename = f"run_{ts}_{result.case_id}_{short_id}.json"
        filepath = output_dir / filename
        
        result_dict = result.to_dict()
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(result_dict, f, ensure_ascii=False, indent=2)
        logger.info("  æ—¥å¿—å·²ä¿å­˜: %s", filepath)
        
        # å¼‚æ­¥åœ¨åå°æ‰§è¡ŒéªŒè¯ï¼ˆä¸é˜»å¡ä»»åŠ¡ç»“æŸï¼‰
        if case.assertions or (case.expected and case.expected.get("golden_file") and case.expected.get("answer_position")):
            asyncio.create_task(
                _run_validation_async(
                    filepath, result_dict,
                    case.assertions or {},
                    case.expected or {},
                    workfile_dir if workfile_dir.is_dir() else None,
                    _validations_lock, case_validations
                )
            )
        
        # é€šçŸ¥å®Œæˆ
        if on_progress:
            on_progress(case.id, case.name, result)
        return index, result, filepath

    async def _run_validation_async(
        filepath: Path,
        result_dict: dict[str, Any],
        assertions: dict[str, Any],
        expected: dict[str, Any],
        workfile_dir: Path | None,
        lock: asyncio.Lock,
        case_validations: list,
    ):
        """åå°å¼‚æ­¥æ‰§è¡ŒéªŒè¯ï¼Œä¸é˜»å¡ä¸»æµç¨‹ã€‚"""
        try:
            validation = validate_case(
                result_dict,
                assertions,
                expected=expected if expected.get("golden_file") or expected.get("answer_position") else None,
                workfile_dir=workfile_dir,
            )
            # æ›´æ–°æ–‡ä»¶ï¼ˆè¿½åŠ éªŒè¯ç»“æœï¼‰
            result_dict["validation"] = validation.to_dict()
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(result_dict, f, ensure_ascii=False, indent=2)
            
            async with lock:
                case_validations.append((result_dict.get("case_id", "unknown"), validation))
            
            if validation.failed > 0:
                logger.warning(
                    "  âš  ç”¨ä¾‹ %s æ–­è¨€æ ¡éªŒ: %d/%d é€šè¿‡ (%d å¤±è´¥)",
                    result_dict.get("case_id", "unknown"),
                    validation.passed, validation.total, validation.failed,
                )
            else:
                logger.info(
                    "  âœ“ ç”¨ä¾‹ %s æ–­è¨€æ ¡éªŒ: %d/%d å…¨éƒ¨é€šè¿‡",
                    result_dict.get("case_id", "unknown"),
                    validation.passed, validation.total,
                )
        except Exception as exc:
            logger.error("ç”¨ä¾‹ %s éªŒè¯å¤±è´¥: %s", result_dict.get("case_id", "unknown"), exc, exc_info=True)

    results: list[BenchResult | None] = [None] * len(cases)
    case_log_files: list[Path | None] = [None] * len(cases)

    if concurrency == 1:
        for i, case in enumerate(cases, 1):
            logger.info("â”€â”€ ç”¨ä¾‹ %d/%d â”€â”€", i, len(cases))
            index, result, filepath = await _execute_case(
                i - 1,
                case,
                render_enabled=True,
            )
            results[index] = result
            case_log_files[index] = filepath
    else:
        logger.info("å¹¶å‘æ¨¡å¼å·²å¯ç”¨ï¼šå…³é—­é€äº‹ä»¶ç»ˆç«¯æ¸²æŸ“ï¼Œé¿å…è¾“å‡ºäº¤é”™ã€‚")
        semaphore = asyncio.Semaphore(concurrency)

        async def _worker(index: int, case: BenchCase) -> tuple[int, BenchResult, Path]:
            async with semaphore:
                logger.info("â”€â”€ ç”¨ä¾‹ %d/%d (å¹¶å‘) â”€â”€", index + 1, len(cases))
                return await _execute_case(
                    index,
                    case,
                    render_enabled=False,
                )

        tasks = [
            asyncio.create_task(_worker(index, case))
            for index, case in enumerate(cases)
        ]
        for index, result, filepath in await asyncio.gather(*tasks):
            results[index] = result
            case_log_files[index] = filepath

    # ç†è®ºä¸Š results ä¸ä¼šä¸º Noneï¼Œæ­¤å¤„åŠ å…œåº•ä¿è¯ç±»å‹ç¨³å®š
    normalized_results: list[BenchResult] = []
    normalized_case_files: list[Path] = []
    for index, case in enumerate(cases):
        current = results[index]
        if current is None:  # pragma: no cover - é˜²å¾¡æ€§é€»è¾‘
            current = BenchResult(
                case_id=case.id,
                case_name=case.name,
                message=case.message,
                timestamp=datetime.now(timezone.utc).isoformat(),
                duration_seconds=0.0,
                iterations=0,
                route_mode="error",
                skills_used=[],
                tool_calls=[],
                thinking_log=[],
                reply="[CRASH] case result missing",
                prompt_tokens=0,
                completion_tokens=0,
                total_tokens=0,
                status="error",
                error={
                    "type": "InternalError",
                    "message": "missing case result",
                },
            )
        normalized_results.append(current)
        if case_log_files[index] is not None:
            normalized_case_files.append(case_log_files[index])

    # ä¿å­˜å¥—ä»¶æ±‡æ€»
    summary_path = _save_suite_summary(
        suite_name,
        suite_path,
        normalized_results,
        output_dir,
        concurrency=concurrency,
        case_log_files=normalized_case_files,
    )
    logger.info("â•" * 50)
    logger.info("å¥—ä»¶æ‰§è¡Œå®Œæ¯•: %s", suite_name)
    logger.info("  æ±‡æ€»æ—¥å¿—: %s", summary_path)

    # æ‰“å°ç®€è¦ç»Ÿè®¡
    total_tokens = sum(r.total_tokens for r in normalized_results)
    total_duration = sum(r.duration_seconds for r in normalized_results)
    total_failures = sum(
        sum(1 for tc in r.tool_calls if not tc.success) for r in normalized_results
    )
    case_errors = sum(
        1 for r in normalized_results if r.status != "ok"
    )
    logger.info(
        "  ç»Ÿè®¡: %d ç”¨ä¾‹ â”‚ æ€» %d tokens â”‚ æ€» %.1fs â”‚ å·¥å…·å¤±è´¥ %d æ¬¡ â”‚ ç”¨ä¾‹å¤±è´¥ %d",
        len(normalized_results),
        total_tokens,
        total_duration,
        total_failures,
        case_errors,
    )

    # â”€â”€ æ–­è¨€æ ¡éªŒæ±‡æ€» + è‡ªåŠ¨æŠ¥å‘Š â”€â”€
    suite_validation = None
    if case_validations:
        suite_validation = aggregate_suite_validation(case_validations)
        logger.info(
            "  æ–­è¨€æ ¡éªŒ: %d/%d é€šè¿‡ (%.1f%%) â”‚ å¤±è´¥æ¡ˆä¾‹: %s",
            suite_validation.passed,
            suite_validation.total_assertions,
            suite_validation.pass_rate,
            ", ".join(suite_validation.failed_cases) or "æ— ",
        )

    # è‡ªåŠ¨ç”Ÿæˆ Markdown æŠ¥å‘Š
    try:
        # è¯»å–åˆšä¿å­˜çš„ suite summary JSON ç”¨äºç”ŸæˆæŠ¥å‘Š
        with open(summary_path, encoding="utf-8") as f:
            suite_summary_dict = json.load(f)
        # å°† validation ä¿¡æ¯æ³¨å…¥åˆ° suite summary çš„å„ case ä¸­
        if case_validations:
            validation_map = dict(case_validations)
            for case_dict in suite_summary_dict.get("artifacts", {}).get("cases", []):
                cid = case_dict.get("meta", {}).get("case_id")
                if cid and cid in validation_map:
                    case_dict["validation"] = validation_map[cid].to_dict()
            suite_summary_dict["validation"] = suite_validation.to_dict()
        report_path = save_suite_report(
            suite_summary_dict,
            output_dir,
            suite_validation=suite_validation,
        )
        logger.info("  ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: %s", report_path)
    except Exception as exc:
        logger.warning("  æŠ¥å‘Šç”Ÿæˆå¤±è´¥: %s", exc)

    logger.info("â•" * 50)
    return normalized_results


# â”€â”€ å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


async def run_single(
    message: str,
    config: ExcelManusConfig,
    output_dir: Path,
    *,
    trace_enabled: bool = True,
) -> BenchResult:
    """ç›´æ¥è¿è¡Œä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½œä¸ºæµ‹è¯•ç”¨ä¾‹ã€‚"""
    case = BenchCase(
        id="adhoc",
        name="ä¸´æ—¶ç”¨ä¾‹",
        message=message,
        messages=[message],
    )
    result = await run_case(
        case, config,
        render_enabled=True,
        trace_enabled=trace_enabled,
        output_dir=output_dir,
    )
    filepath, _ = _save_result(result, output_dir)
    logger.info("æ—¥å¿—å·²ä¿å­˜: %s", filepath)
    return result


@dataclass
class _RunPlan:
    """bench CLI è§£æåçš„æ‰§è¡Œè®¡åˆ’ã€‚"""

    mode: str
    suite_paths: list[Path] = field(default_factory=list)
    message: str = ""


def _positive_int(raw: str) -> int:
    """argparse ä½¿ç”¨çš„æ­£æ•´æ•°è§£æå™¨ã€‚"""
    try:
        value = int(raw)
    except ValueError as exc:
        raise argparse.ArgumentTypeError("å¿…é¡»æ˜¯æ•´æ•°") from exc
    if value < 1:
        raise argparse.ArgumentTypeError("å¿…é¡»æ˜¯ >= 1 çš„æ•´æ•°")
    return value


def _build_parser() -> argparse.ArgumentParser:
    """æ„å»º bench CLI å‚æ•°è§£æå™¨ã€‚"""
    parser = argparse.ArgumentParser(
        prog="python -m excelmanus.bench",
        description="Bench æµ‹è¯•è¿è¡Œå™¨",
    )
    parser.add_argument(
        "targets",
        nargs="*",
        help="ä½ç½®å‚æ•°ï¼šæ™ºèƒ½è¯†åˆ«ä¸º suite è·¯å¾„ï¼ˆ*.jsonï¼‰æˆ– message æ–‡æœ¬",
    )
    group = parser.add_mutually_exclusive_group()
    group.add_argument(
        "--suite",
        nargs="+",
        metavar="PATH",
        help="æ˜¾å¼æŒ‡å®šä¸€ä¸ªæˆ–å¤šä¸ª suite JSON æ–‡ä»¶",
    )
    group.add_argument(
        "--all",
        action="store_true",
        help="è¿è¡Œ bench/cases/ ä¸‹æ‰€æœ‰ suite",
    )
    group.add_argument(
        "--message",
        help="æ˜¾å¼æŒ‡å®šå•æ¡æ¶ˆæ¯ä½œä¸ºç”¨ä¾‹",
    )
    parser.add_argument(
        "--concurrency",
        type=_positive_int,
        default=1,
        help="å•ä¸ª suite å†…ç”¨ä¾‹å¹¶å‘åº¦ï¼ˆé»˜è®¤ 1ï¼‰",
    )
    parser.add_argument(
        "--suite-concurrency",
        type=_positive_int,
        default=1,
        help="suite é—´å¹¶å‘åº¦ï¼ˆé»˜è®¤ 1ï¼Œä»…å¤š suite æ—¶ç”Ÿæ•ˆï¼‰",
    )
    parser.add_argument(
        "--output-dir",
        default="outputs/bench",
        help="æ—¥å¿—è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ outputs/benchï¼‰",
    )
    trace_group = parser.add_mutually_exclusive_group()
    trace_group.add_argument(
        "--trace",
        action="store_true",
        dest="trace",
        default=True,
        help="å¯ç”¨ engine å†…éƒ¨äº¤äº’è½¨è¿¹è®°å½•ï¼ˆé»˜è®¤è¡Œä¸ºï¼Œå¯æ˜¾å¼æŒ‡å®šï¼‰",
    )
    trace_group.add_argument(
        "--no-trace",
        action="store_false",
        dest="trace",
        help="ç¦ç”¨ engine å†…éƒ¨äº¤äº’è½¨è¿¹è®°å½•ã€‚"
        "ä¹Ÿå¯é€šè¿‡ EXCELMANUS_BENCH_TRACE=0 ç¯å¢ƒå˜é‡ç¦ç”¨ã€‚",
    )
    return parser


def _is_json_like_path(raw: str) -> bool:
    """åˆ¤æ–­å‚æ•°æ˜¯å¦æ»¡è¶³ suite æ–‡ä»¶è·¯å¾„è¯­ä¹‰ã€‚"""
    return raw.lower().endswith(".json")


def _resolve_run_mode(args: argparse.Namespace) -> _RunPlan:
    """å°† argparse ç»“æœæ˜ å°„ä¸ºæ‰§è¡Œè®¡åˆ’ã€‚"""
    targets = list(args.targets or [])

    if args.suite is not None:
        if targets:
            raise ValueError("ä½¿ç”¨ --suite æ—¶ä¸åº”å†ä¼ å…¥ä½ç½®å‚æ•°ã€‚")
        return _RunPlan(
            mode="suite",
            suite_paths=[Path(p) for p in args.suite],
        )

    if args.all:
        if targets:
            raise ValueError("ä½¿ç”¨ --all æ—¶ä¸åº”å†ä¼ å…¥ä½ç½®å‚æ•°ã€‚")
        return _RunPlan(mode="all")

    if args.message is not None:
        if targets:
            raise ValueError("ä½¿ç”¨ --message æ—¶ä¸åº”å†ä¼ å…¥ä½ç½®å‚æ•°ã€‚")
        return _RunPlan(mode="message", message=args.message)

    if not targets:
        return _RunPlan(mode="help")

    # æ™ºèƒ½è¯†åˆ«ï¼šå…¨éƒ¨çœ‹èµ·æ¥åƒ *.json æ—¶ï¼Œè§†ä¸º suite æ¨¡å¼ï¼›å¦åˆ™æŒ‰ message æ¨¡å¼ã€‚
    if all(_is_json_like_path(item) for item in targets):
        return _RunPlan(
            mode="suite",
            suite_paths=[Path(item) for item in targets],
        )

    return _RunPlan(mode="message", message=" ".join(targets))


async def _run_suites(
    suite_paths: list[Path],
    config: ExcelManusConfig,
    output_dir: Path,
    *,
    concurrency: int = 1,
    suite_concurrency: int = 1,
    trace_enabled: bool = True,
) -> int:
    """å¹¶å‘è¿è¡Œå¤šä¸ª suiteï¼Œå¸¦ Rich Live è¿›åº¦é¢æ¿å’Œå…¨å±€æ±‡æ€»ã€‚

    è¿”å› shell é€€å‡ºç ï¼ˆ0 = å…¨éƒ¨æˆåŠŸï¼Œ1 = å­˜åœ¨å¤±è´¥ï¼‰ã€‚
    """
    total_suites = len(suite_paths)
    global_start = time.monotonic()

    # æ¯ä¸ª suite çš„è¿›åº¦è¿½è¸ª
    progress_map: dict[str, _SuiteProgress] = {}
    suite_results: list[tuple[str, list[BenchResult]]] = []
    results_lock = asyncio.Lock()

    def _short_name(p: Path) -> str:
        return p.stem

    # é¢„åŠ è½½ suite è·å– case æ•°é‡
    for p in suite_paths:
        name = _short_name(p)
        try:
            _, cases, _ = _load_suite(p)
            total = len(cases)
        except Exception:
            total = 0
        progress_map[name] = _SuiteProgress(suite_name=name, total_cases=total)

    def _build_progress_table() -> Table:
        """æ„å»ºå®æ—¶è¿›åº¦è¡¨æ ¼ã€‚"""
        elapsed = time.monotonic() - global_start
        elapsed_str = f"{elapsed:.0f}s" if elapsed < 60 else f"{elapsed / 60:.1f}m"
        # å…¨å±€ç»Ÿè®¡
        g_done = sum(p.done_cases for p in progress_map.values())
        g_total = sum(p.total_cases for p in progress_map.values())
        g_ok = sum(p.ok_cases for p in progress_map.values())
        g_fail = sum(p.fail_cases for p in progress_map.values())
        g_tok = sum(p.total_tokens for p in progress_map.values())

        title = (
            f"Bench å¹¶å‘æ‰§è¡Œé¢æ¿  â± {elapsed_str}"
            f"  â”‚  {g_done}/{g_total} cases"
            f"  {g_ok}âœ… {g_fail}âŒ"
            f"  â”‚  {g_tok:,} tok"
        )
        table = Table(title=title, show_lines=True, expand=True)
        table.add_column("Suite", style="cyan", min_width=20, max_width=35)
        table.add_column("è¿›åº¦", min_width=12, max_width=18)
        table.add_column("çŠ¶æ€", min_width=30)
        table.add_column("è€—æ—¶", justify="right", min_width=6, max_width=8)
        table.add_column("Tokens", justify="right", min_width=8, max_width=12)

        for prog in progress_map.values():
            # è¿›åº¦åˆ—
            if prog.done_cases > 0 or prog.status.startswith("ğŸ”„"):
                progress_col = prog.progress_bar()
            else:
                progress_col = ""

            # çŠ¶æ€åˆ—
            if prog.status.startswith("ğŸ”„"):
                # æ‰§è¡Œä¸­ï¼šæ˜¾ç¤ºå½“å‰ case å’Œé€šè¿‡/å¤±è´¥
                parts = []
                if prog.ok_cases or prog.fail_cases:
                    parts.append(f"{prog.ok_cases}âœ…")
                    if prog.fail_cases:
                        parts.append(f"{prog.fail_cases}âŒ")
                if prog.current_case:
                    case_display = prog.current_case
                    if len(case_display) > 20:
                        case_display = case_display[:18] + "â€¦"
                    parts.append(f"â–¸ {case_display}")
                status_col = "ğŸ”„ " + "  ".join(parts) if parts else "ğŸ”„ æ‰§è¡Œä¸­"
            else:
                status_col = prog.status

            # è€—æ—¶åˆ—
            if prog.status.startswith("â³"):
                time_col = ""
            else:
                time_col = prog.elapsed_str()

            # Token åˆ—
            tok_col = f"{prog.total_tokens:,}" if prog.total_tokens else ""

            table.add_row(prog.suite_name, progress_col, status_col, time_col, tok_col)

        return table

    def _make_progress_cb(name: str) -> ProgressCallback:
        """ä¸ºæŒ‡å®š suite åˆ›å»ºè¿›åº¦å›è°ƒã€‚"""
        def _cb(case_id: str, case_name: str, result: BenchResult | None) -> None:
            prog = progress_map[name]
            if result is None:
                # case å¼€å§‹æ‰§è¡Œ
                prog.current_case = case_name or case_id
            else:
                # case å®Œæˆ
                prog.done_cases += 1
                prog.total_tokens += result.total_tokens
                if result.status == "ok":
                    prog.ok_cases += 1
                else:
                    prog.fail_cases += 1
                prog.current_case = ""
        return _cb

    async def _suite_worker(
        suite_path: Path,
        sem: asyncio.Semaphore,
    ) -> None:
        name = _short_name(suite_path)
        async with sem:
            prog = progress_map[name]
            prog.status = "ğŸ”„ æ‰§è¡Œä¸­"
            prog.start_time = time.monotonic()
            try:
                try:
                    results = await run_suite(
                        suite_path,
                        config,
                        output_dir,
                        concurrency=concurrency,
                        trace_enabled=trace_enabled,
                        on_progress=_make_progress_cb(name),
                    )
                except TypeError as exc:
                    if "on_progress" not in str(exc):
                        raise
                    # å…¼å®¹æ—§æµ‹è¯•æ¡© / è‡ªå®šä¹‰ wrapperï¼šä¸æ”¯æŒ on_progress æ—¶é€€åŒ–è°ƒç”¨ã€‚
                    results = await run_suite(
                        suite_path,
                        config,
                        output_dir,
                        concurrency=concurrency,
                        trace_enabled=trace_enabled,
                    )
                ok_count = sum(1 for r in results if r.status == "ok")
                fail_count = len(results) - ok_count
                if fail_count:
                    prog.status = f"âš ï¸  å®Œæˆ ({ok_count}âœ… {fail_count}âŒ)"
                else:
                    prog.status = f"âœ… å®Œæˆ ({ok_count} ç”¨ä¾‹)"
                async with results_lock:
                    suite_results.append((name, results))
            except Exception as exc:
                prog.status = f"ğŸ’¥ å´©æºƒ: {exc}"
                async with results_lock:
                    suite_results.append((name, []))

    sem = asyncio.Semaphore(suite_concurrency)
    is_parallel = suite_concurrency > 1 and total_suites > 1

    if is_parallel:
        logger.info(
            "å¯åŠ¨å¹¶å‘æ¨¡å¼ï¼š%d ä¸ª suiteï¼Œsuite å¹¶å‘=%dï¼Œcase å¹¶å‘=%d",
            total_suites,
            suite_concurrency,
            concurrency,
        )
        tasks = [
            asyncio.create_task(_suite_worker(p, sem))
            for p in suite_paths
        ]

        console = Console()
        with Live(
            _build_progress_table(),
            console=console,
            refresh_per_second=2,
        ) as live:
            while not all(t.done() for t in tasks):
                live.update(_build_progress_table())
                await asyncio.sleep(0.5)
            live.update(_build_progress_table())

        for t in tasks:
            if t.exception():  # pragma: no cover
                logger.error("suite ä»»åŠ¡å¼‚å¸¸: %s", t.exception())
    else:
        # ä¸²è¡Œæ¨¡å¼ï¼šé€ä¸ªæ‰§è¡Œï¼Œä¿æŒåŸæœ‰æ—¥å¿—è¾“å‡º
        for suite_path in suite_paths:
            await _suite_worker(suite_path, sem)

    # â”€â”€ å…¨å±€æ±‡æ€»æŠ¥å‘Š â”€â”€
    all_results = [r for _, results in suite_results for r in results]
    if all_results:
        total_cases = len(all_results)
        total_ok = sum(1 for r in all_results if r.status == "ok")
        total_fail = total_cases - total_ok
        total_tokens = sum(r.total_tokens for r in all_results)
        total_duration = sum(r.duration_seconds for r in all_results)
        total_tool_failures = sum(
            sum(1 for tc in r.tool_calls if not tc.success) for r in all_results
        )

        # ä¿å­˜å…¨å±€æ±‡æ€» JSON
        ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%S")
        short_id = uuid.uuid4().hex[:6]
        global_summary = {
            "schema_version": 3,
            "kind": "global_summary",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "execution": {
                "suite_count": total_suites,
                "suite_concurrency": suite_concurrency,
                "case_concurrency": concurrency,
            },
            "stats": {
                "total_cases": total_cases,
                "passed": total_ok,
                "failed": total_fail,
                "total_tokens": total_tokens,
                "total_duration_seconds": round(total_duration, 2),
                "tool_failures": total_tool_failures,
            },
            "suites": [
                {
                    "name": name,
                    "case_count": len(results),
                    "passed": sum(1 for r in results if r.status == "ok"),
                    "failed": sum(1 for r in results if r.status != "ok"),
                }
                for name, results in suite_results
            ],
        }
        output_dir.mkdir(parents=True, exist_ok=True)
        global_path = output_dir / f"global_{ts}_{short_id}.json"
        with open(global_path, "w", encoding="utf-8") as f:
            json.dump(global_summary, f, ensure_ascii=False, indent=2)

        logger.info("â•" * 60)
        logger.info("å…¨å±€æ±‡æ€»")
        logger.info("â•" * 60)
        logger.info(
            "  %d ä¸ª suite â”‚ %d ç”¨ä¾‹ â”‚ %d é€šè¿‡ â”‚ %d å¤±è´¥",
            total_suites,
            total_cases,
            total_ok,
            total_fail,
        )
        logger.info(
            "  æ€» %d tokens â”‚ æ€» %.1fs â”‚ å·¥å…·å¤±è´¥ %d æ¬¡",
            total_tokens,
            total_duration,
            total_tool_failures,
        )
        logger.info("  å…¨å±€æ±‡æ€»: %s", global_path)
        logger.info("â•" * 60)

    return 0 if all(r.status == "ok" for r in all_results) else 1


async def _main(argv: list[str] | None = None) -> int:
    """è„šæœ¬å…¥å£ï¼Œè¿”å› shell é€€å‡ºç ã€‚"""
    parser = _build_parser()
    args = parser.parse_args(argv)

    try:
        plan = _resolve_run_mode(args)
    except ValueError as exc:
        logger.error("å‚æ•°é”™è¯¯ï¼š%s", exc)
        return 1

    if plan.mode == "help":
        parser.print_help()
        return 0

    # trace æ¨¡å¼ï¼šé»˜è®¤å¼€å¯ï¼Œå¯é€šè¿‡ --no-trace æˆ– EXCELMANUS_BENCH_TRACE=0 ç¦ç”¨
    trace_enabled = args.trace and os.environ.get("EXCELMANUS_BENCH_TRACE", "1") != "0"

    if plan.mode == "message":
        config = load_config()
        setup_logging(config.log_level)
        output_dir = Path(args.output_dir)
        await run_single(plan.message, config, output_dir, trace_enabled=trace_enabled)
        return 0

    if plan.mode == "suite":
        missing_paths = [p for p in plan.suite_paths if not p.is_file()]
        if missing_paths:
            for p in missing_paths:
                logger.error("æœªæ‰¾åˆ° suite æ–‡ä»¶: %s", p)
            return 1
        config = load_config()
        setup_logging(config.log_level)
        output_dir = Path(args.output_dir)
        return await _run_suites(
            plan.suite_paths,
            config,
            output_dir,
            concurrency=args.concurrency,
            suite_concurrency=args.suite_concurrency,
            trace_enabled=trace_enabled,
        )

    # all æ¨¡å¼
    cases_dir = Path("bench/cases")
    if not cases_dir.is_dir():
        logger.error("æœªæ‰¾åˆ°æµ‹è¯•ç”¨ä¾‹ç›®å½•: %s", cases_dir)
        return 1

    suite_paths = sorted(cases_dir.glob("*.json"))
    if not suite_paths:
        logger.error("ç›®å½• %s ä¸‹æ—  JSON ç”¨ä¾‹æ–‡ä»¶", cases_dir)
        return 1

    config = load_config()
    setup_logging(config.log_level)
    output_dir = Path(args.output_dir)
    return await _run_suites(
        suite_paths,
        config,
        output_dir,
        concurrency=args.concurrency,
        suite_concurrency=args.suite_concurrency,
        trace_enabled=trace_enabled,
    )


if __name__ == "__main__":
    raise SystemExit(asyncio.run(_main()))
